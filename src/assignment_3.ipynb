{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cab3a8a",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf4ce5",
   "metadata": {},
   "source": [
    "This project compares three feedforward neural network training algorithms: Stochastic Gradient Descent (SGD), Scaled Conjugate Gradient (SCG), and LeapFrog. Using six datasets—three for classification and three for regression—the study evaluates convergence speed, stability, and predictive accuracy. Each network has a single hidden layer, with experiments across different hidden layer sizes and hyperparameters. Performance is measured using accuracy and F1-score for classification, and MSE, RMSE, and R² for regression, alongside training time and convergence behavior. The results highlight the strengths and weaknesses of each optimizer across problems of varying complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e30bd8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d9f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ddc923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.datasets import load_iris, fetch_california_housing, fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score, confusion_matrix, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import List, Tuple, Optional, Callable\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import json\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "import time\n",
    "import math\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994246ee",
   "metadata": {},
   "source": [
    "## Data and Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ca2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preprocessing_pipeline(X, classification=True):\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Combine transformers\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5bc1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mnist(X, y, test_size=0.2, random_state=42):\n",
    "    X_norm = X / 255.0\n",
    "\n",
    "    if len(X_norm.shape) > 2:\n",
    "        X_flat = X_norm.reshape(X_norm.shape[0], -1)\n",
    "    else:\n",
    "        X_flat = X_norm\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_flat, y_encoded, test_size=test_size, random_state=random_state, stratify=y_encoded\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2af212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_california_housing(X, y, skewed_features=['MedInc'], log_target=True, test_size=0.2, random_state=42):\n",
    "\n",
    "    X_processed = X.copy()\n",
    "    for col in skewed_features:\n",
    "        X_processed[col] = np.log1p(X_processed[col])\n",
    "\n",
    "\n",
    "    if log_target:\n",
    "        y_processed = np.log1p(y)\n",
    "    else:\n",
    "        y_processed = y.copy()\n",
    "    if X_processed.isnull().any().any():\n",
    "        print(\"Warning: NaNs found in features after log transformation.\")\n",
    "    if pd.isnull(y_processed).any():\n",
    "        print(\"Warning: NaNs found in target after log transformation.\")\n",
    "\n",
    "    preprocessor = build_preprocessing_pipeline(X_processed, classification=False)\n",
    "    X_scaled = preprocessor.fit_transform(X_processed)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_processed, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b68706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train, val, test, scale=True, impute=False, classification=True):\n",
    "    X_train, y_train = train\n",
    "    X_val, y_val = val\n",
    "    X_test, y_test = test\n",
    "\n",
    "    preprocessor = build_preprocessing_pipeline(X_train, classification=classification)\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_val_processed = preprocessor.transform(X_val)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    if classification:\n",
    "        le = LabelEncoder()\n",
    "        y_train = le.fit_transform(y_train)\n",
    "        y_val = le.transform(y_val)\n",
    "        y_test = le.transform(y_test)\n",
    "\n",
    "    return X_train_processed, X_val_processed, X_test_processed, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f363bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val_test(X, y, test_size=0.15, val_fraction_of_total=0.15,\n",
    "                         random_state=42, stratify=None):\n",
    "\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=stratify\n",
    "    )\n",
    "\n",
    "    val_size = val_fraction_of_total / (1 - test_size)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=val_size,\n",
    "        random_state=random_state, stratify=y_train_val if stratify is not None else None\n",
    "    )\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8979ee4b",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52652ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris Dataset\n",
    "def load_iris_data():\n",
    "    iris = sns.load_dataset(\"iris\")\n",
    "    print(iris.head())\n",
    "\n",
    "    X_iris = iris.drop(\"species\", axis=1)\n",
    "    y_iris = iris[\"species\"]\n",
    "    print(X_iris.head())\n",
    "    print(y_iris.head())\n",
    "    return X_iris, y_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e022a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stroke Prediction Dataset\n",
    "def load_stroke_data():\n",
    "    stroke_data = pd.read_csv(\"../data/healthcare-dataset-stroke-data.csv\")\n",
    "    print(stroke_data.shape)\n",
    "    X_stroke = stroke_data.drop(\"stroke\", axis=1)\n",
    "    y_stroke = stroke_data[\"stroke\"]\n",
    "    print(X_stroke.head())\n",
    "    print(y_stroke.head())\n",
    "    return X_stroke, y_stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61848e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset\n",
    "def load_mnist_subset(n_samples=10000, random_state=42):\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    X = pd.DataFrame(mnist.data)\n",
    "    y = pd.Series(mnist.target, name=\"digit\")\n",
    "    # Sample 10k rows\n",
    "    X_mnist = X.sample(n=n_samples, random_state=random_state)\n",
    "    y_mnist = y.loc[X_mnist.index]\n",
    "    print(X_mnist.shape)\n",
    "    print(X_mnist.head())\n",
    "    print(y_mnist.head())\n",
    "    return X_mnist, y_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0d742d",
   "metadata": {},
   "source": [
    "### Function approx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1a7d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Sine Wave Dataset\n",
    "def generate_sine_wave_data(num_samples: int = 1000, noise_level: float = 0.1, random_state: int = 42) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    X = np.linspace(0, 2 * np.pi, num_samples)\n",
    "    y = np.sin(X) + noise_level * rng.standard_normal(num_samples)\n",
    "    X_sine = pd.DataFrame(X, columns=[\"x\"])\n",
    "    y_sine = pd.Series(y, name=\"y\")\n",
    "    return X_sine, y_sine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ee2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wine quality dataset from UCI ML Repository\n",
    "def load_wine_quality_data():\n",
    "    wine_quality = fetch_ucirepo(id=186) \n",
    "    \n",
    "    # data (as pandas dataframes) \n",
    "    X_wine = wine_quality.data.features \n",
    "    y_wine = wine_quality.data.targets \n",
    "    print(wine_quality.metadata)\n",
    "    print(wine_quality.variables)\n",
    "    print(X_wine.head())\n",
    "    print(y_wine.head())\n",
    "    print (X_wine.shape, y_wine.shape)\n",
    "    return X_wine, y_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce16cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# California Housing Dataset\n",
    "def load_california_housing_data(n_samples=10000, random_state=42):\n",
    "    california = fetch_california_housing()\n",
    "    X = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "    y = pd.Series(california.target, name=\"MedHouseVal\")\n",
    "    # Sample 10k rows\n",
    "    X_california = X.sample(n=n_samples, random_state=random_state)\n",
    "    y_california = y.loc[X_california.index]\n",
    "    print(X_california.shape)\n",
    "    print(X_california.head())\n",
    "    print(y_california.head())\n",
    "    return X_california, y_california"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11210285",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a2018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, activation_fn=nn.ReLU, dropout_p=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
    "        self.activation = activation_fn()\n",
    "        self.dropout = nn.Dropout(dropout_p) if dropout_p > 0 else nn.Identity()\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd77059e",
   "metadata": {},
   "source": [
    "### Training Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd22c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sgd(model, X_train, y_train, X_val, y_val, epochs=100, lr=0.01, batch_size=32,\n",
    "              classification=True, momentum=0.0, weight_decay=0.0,\n",
    "              early_stopping_patience=20, min_delta=1e-4, record_times=False):\n",
    "    criterion = nn.CrossEntropyLoss() if classification else nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "    X_tr = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
    "    if classification:\n",
    "        y_tr = torch.tensor(y_train, dtype=torch.long)\n",
    "        y_val_t = torch.tensor(y_val, dtype=torch.long)\n",
    "    else:\n",
    "        y_tr = torch.tensor(y_train, dtype=torch.float32)\n",
    "        y_val_t = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "    ds = torch.utils.data.TensorDataset(X_tr, y_tr)\n",
    "    loader = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    train_losses, val_losses, time_stamps = [], [], []\n",
    "    best_val = float('inf')\n",
    "    patience = 0\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "        for xb, yb in loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            total += loss.item() * xb.size(0)\n",
    "        train_loss = total / len(loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(X_val_t)\n",
    "            val_loss = criterion(val_out, y_val_t).item()\n",
    "        val_losses.append(val_loss)\n",
    "        if record_times:\n",
    "            time_stamps.append(time.time() - start)\n",
    "\n",
    "        if val_loss + min_delta < best_val:\n",
    "            best_val = val_loss\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= early_stopping_patience:\n",
    "                break\n",
    "\n",
    "    if record_times:\n",
    "        return train_losses, val_losses, time_stamps\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b7052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage of SGD - function approximation\n",
    "# X_sine, y_sine = generate_sine_wave_data()\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(X_sine, y_sine, classification=False)\n",
    "# model = FeedforwardNN(input_dim=1, hidden_dim=16, output_dim=1)\n",
    "# train_losses, test_losses = train_sgd(\n",
    "#     model,\n",
    "#     X_train_scaled,\n",
    "#     np.array(y_train).reshape(-1, 1).astype(np.float32),\n",
    "#     X_test_scaled,\n",
    "#     np.array(y_test).reshape(-1, 1).astype(np.float32),\n",
    "#     classification=False\n",
    "# )\n",
    "# print(\"Train Losses:\", train_losses)\n",
    "# print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ded0309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage of SGD - Classification\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(X_iris, y_iris, classification=True)\n",
    "# model = FeedforwardNN(input_dim=4, hidden_dim=16, output_dim=3)\n",
    "# train_losses, test_losses = train_sgd(model, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "# print(\"Train Losses:\", train_losses)\n",
    "# print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aa3547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_scg(\n",
    "    model: nn.Module,\n",
    "    X_train: torch.Tensor,\n",
    "    y_train: torch.Tensor,\n",
    "    X_test: torch.Tensor,\n",
    "    y_test: torch.Tensor,\n",
    "    max_epochs: int = 1000,\n",
    "    tolerance: float = 1e-6,\n",
    "    sigma: float = 5e-5,\n",
    "    lambda_init: float = 5e-7,\n",
    "    verbose: bool = True,\n",
    "    eval_freq: int = 10,\n",
    "    grad_clip: float = 5.0,\n",
    "    max_alpha: float = 1.0,\n",
    "    record_times: bool = False\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    device = next(model.parameters()).device\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "    X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "    # Task detection (robust)\n",
    "    if y_train.dtype in (torch.long, torch.int64) and y_train.ndim == 1:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        task_type = 'classification'\n",
    "    else:\n",
    "        criterion = nn.MSELoss()\n",
    "        task_type = 'regression'\n",
    "\n",
    "    def get_weights():\n",
    "        return torch.cat([p.view(-1) for p in model.parameters()])\n",
    "\n",
    "    def set_weights(w):\n",
    "        idx = 0\n",
    "        for p in model.parameters():\n",
    "            n = p.numel()\n",
    "            p.data = w[idx:idx+n].view(p.shape)\n",
    "            idx += n\n",
    "\n",
    "    def loss_and_grad(w):\n",
    "        set_weights(w)\n",
    "        model.zero_grad()\n",
    "        out = model(X_train)\n",
    "        loss = criterion(out, y_train)\n",
    "        loss.backward()\n",
    "        g = torch.cat([p.grad.view(-1) for p in model.parameters()])\n",
    "        return loss.item(), g\n",
    "\n",
    "    def eval_split():\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tr = criterion(model(X_train), y_train).item()\n",
    "            te = criterion(model(X_test), y_test).item()\n",
    "        model.train()\n",
    "        return tr, te\n",
    "\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    w_k = get_weights()\n",
    "    f_k, g_k = loss_and_grad(w_k)\n",
    "    r_k = g_k.clone()\n",
    "    r_k_prev = None\n",
    "    p_k = -r_k.clone()\n",
    "    lambda_k = lambda_init\n",
    "    success = True\n",
    "    k_iter = 0\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    time_stamps = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    tr0, te0 = eval_split()\n",
    "    train_losses.append(tr0); test_losses.append(te0)\n",
    "    if verbose:\n",
    "        print(f\"Initial - Train {tr0:.6f} Test {te0:.6f}\")\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        if success:\n",
    "            denom = torch.dot(p_k, p_k)\n",
    "            if denom.abs() < 1e-18:\n",
    "                if verbose: print(\"Direction norm too small; stopping.\")\n",
    "                break\n",
    "            sigma_k = sigma / torch.sqrt(denom)\n",
    "\n",
    "        # Finite difference Hessian-vector approx\n",
    "        w_temp = w_k + sigma_k * p_k\n",
    "        _, g_temp = loss_and_grad(w_temp)\n",
    "        s_k = (g_temp - g_k) / sigma_k\n",
    "\n",
    "        delta_k = torch.dot(p_k, s_k)\n",
    "        if delta_k <= 0:\n",
    "            s_k = s_k + (lambda_k - delta_k) * p_k\n",
    "            delta_k = lambda_k * torch.dot(p_k, p_k)\n",
    "            lambda_k *= 2\n",
    "\n",
    "        mu_k = torch.dot(p_k, r_k)\n",
    "        alpha_k = -mu_k / (delta_k + 1e-12)\n",
    "\n",
    "        if abs(alpha_k) > max_alpha:\n",
    "            alpha_k = torch.clamp(alpha_k, -max_alpha, max_alpha)\n",
    "\n",
    "        Delta_k = -(alpha_k * mu_k + 0.5 * alpha_k * alpha_k * delta_k)\n",
    "\n",
    "        w_new = w_k + alpha_k * p_k\n",
    "        f_new, g_new = loss_and_grad(w_new)\n",
    "\n",
    "        # NaN / Inf guard\n",
    "        if not torch.isfinite(torch.tensor(f_new)):\n",
    "            if verbose: print(f\"Non-finite loss at epoch {epoch}; aborting this run.\")\n",
    "            break\n",
    "\n",
    "        if Delta_k <= 0:\n",
    "            r_ratio = -float('inf')\n",
    "        else:\n",
    "            r_ratio = (f_k - f_new) / Delta_k\n",
    "\n",
    "        if r_ratio > 0:\n",
    "            success = True\n",
    "            w_k = w_new\n",
    "            f_k = f_new\n",
    "            r_k_prev = r_k.clone()\n",
    "            g_k = g_new\n",
    "            r_k = g_k.clone()\n",
    "\n",
    "            if grad_clip is not None:\n",
    "                g_norm = torch.norm(r_k)\n",
    "                if g_norm > grad_clip:\n",
    "                    r_k.mul_(grad_clip / (g_norm + 1e-12))\n",
    "                    g_k = r_k  \n",
    "\n",
    "            if r_ratio > 0.75:\n",
    "                lambda_k /= 2\n",
    "            elif r_ratio < 0.25:\n",
    "                lambda_k *= 2\n",
    "        else:\n",
    "            success = False\n",
    "            lambda_k *= 2  \n",
    "\n",
    "        if success:\n",
    "            if k_iter % n_params == 0 or r_k_prev is None:\n",
    "                p_k = -r_k.clone()\n",
    "            else:\n",
    "                beta_k = torch.dot(r_k, r_k - r_k_prev) / (torch.dot(r_k_prev, r_k_prev) + 1e-12)\n",
    "                p_k = -r_k + beta_k * p_k\n",
    "            k_iter += 1\n",
    "\n",
    "            grad_norm = torch.norm(r_k).item()\n",
    "            if grad_norm < tolerance:\n",
    "                if verbose: print(f\"Converged at epoch {epoch} ||g||={grad_norm:.2e}\")\n",
    "                break\n",
    "\n",
    "        if epoch % eval_freq == 0 or epoch == max_epochs - 1:\n",
    "            tr, te = eval_split()\n",
    "            train_losses.append(tr)\n",
    "            test_losses.append(te)\n",
    "            if record_times:\n",
    "                time_stamps.append(time.time() - start_time)\n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch:4d} f={tr:.6f} test={te:.6f} ||g||={torch.norm(r_k):.2e} r={r_ratio:.3f} λ={lambda_k:.2e} α={float(alpha_k):.3e}\")\n",
    "\n",
    "    if verbose and len(train_losses):\n",
    "        print(f\"Final - Train {train_losses[-1]:.6f} Test {test_losses[-1]:.6f}\")\n",
    "    if record_times:\n",
    "        return train_losses, test_losses, time_stamps\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a675d",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "*For classification tasks, the output layer is linear and we use `CrossEntropyLoss`, which applies the required softmax internally. For regression tasks, the output layer is also linear, and we use `MSELoss`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de23cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage of SCG - classification\n",
    "# model = FeedforwardNN(input_dim=4, hidden_dim=16, output_dim=3)\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(X, y, classification=True)\n",
    "\n",
    "# X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "# X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# train_losses, test_losses = train_scg(\n",
    "#     model=model,\n",
    "#     X_train=X_train_tensor,\n",
    "#     y_train=y_train_tensor,\n",
    "#     X_test=X_test_tensor,\n",
    "#     y_test=y_test_tensor,\n",
    "#     max_epochs=500,\n",
    "#     tolerance=1e-5,\n",
    "#     verbose=True,\n",
    "#     eval_freq=1\n",
    "# )\n",
    "# print(\"Train Losses:\", train_losses)\n",
    "# print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02672718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage of SCG - function approximation\n",
    "# model = FeedforwardNN(input_dim=1, hidden_dim=16, output_dim=1)\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(X_sine, y_sine, classification=False)\n",
    "# X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "# X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "# train_losses, test_losses = train_scg(\n",
    "#     model=model,\n",
    "#     X_train=X_train_tensor,\n",
    "#     y_train=y_train_tensor,\n",
    "#     X_test=X_test_tensor,\n",
    "#     y_test=y_test_tensor,\n",
    "#     max_epochs=500,\n",
    "#     tolerance=1e-5,\n",
    "#     verbose=True,\n",
    "#     eval_freq=1\n",
    "# )\n",
    "# print(\"Train Losses:\", train_losses)\n",
    "# print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14362933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lfrog(\n",
    "    model: nn.Module,\n",
    "    X_train: torch.Tensor,\n",
    "    y_train: torch.Tensor,\n",
    "    X_test: torch.Tensor,\n",
    "    y_test: torch.Tensor,\n",
    "    loss_fn: Optional[Callable] = None,\n",
    "    epochs: int = 1000,\n",
    "    dt: float = 0.5,\n",
    "    max_step: float = 1.0,\n",
    "    convergence_tol: float = 1e-5,\n",
    "    max_consecutive_decreases: int = 2,\n",
    "    time_step_reduction_threshold: int = 3,\n",
    "    time_step_increase_factor: float = 0.001,\n",
    "    batch_size: Optional[int] = None,\n",
    "    device: str = 'cpu',\n",
    "    print_every: int = 100,\n",
    "    early_stopping_patience: int = 50,\n",
    "    min_improvement: float = 1e-6,\n",
    "    record_times: bool = False\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    model = model.to(device)\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "    X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "    \n",
    "    if loss_fn is None:\n",
    "        if len(y_train.shape) == 1 or y_train.shape[1] == 1:\n",
    "            if torch.all((y_train == 0) | (y_train == 1)):\n",
    "                loss_fn = nn.BCEWithLogitsLoss()\n",
    "            else:\n",
    "                loss_fn = nn.MSELoss()\n",
    "        else:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    time_stamps = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_test_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    params = []\n",
    "    for p in model.parameters():\n",
    "        params.append(p.view(-1))\n",
    "    x_k = torch.cat(params)\n",
    "    n_params = len(x_k)\n",
    "    \n",
    "    # Compute initial gradient and velocity\n",
    "    def compute_loss_and_grad():\n",
    "        model.zero_grad()\n",
    "        if batch_size is None:\n",
    "            outputs = model(X_train)\n",
    "            loss = loss_fn(outputs, y_train)\n",
    "        else:\n",
    "            # Mini-batch gradient\n",
    "            idx = torch.randperm(len(X_train))[:batch_size]\n",
    "            outputs = model(X_train[idx])\n",
    "            loss = loss_fn(outputs, y_train[idx])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        grads = []\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                grads.append(p.grad.view(-1))\n",
    "            else:\n",
    "                grads.append(torch.zeros_like(p.view(-1)))\n",
    "        grad = torch.cat(grads)\n",
    "        \n",
    "        return loss.item(), grad\n",
    "    \n",
    "    def update_model_params(x):\n",
    "        idx = 0\n",
    "        for p in model.parameters():\n",
    "            param_size = p.numel()\n",
    "            p.data = x[idx:idx + param_size].view(p.shape)\n",
    "            idx += param_size\n",
    "    \n",
    "    def evaluate_test():\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test)\n",
    "            test_loss = loss_fn(test_outputs, y_test)\n",
    "        model.train()\n",
    "        return test_loss.item()\n",
    "    \n",
    "    train_loss, grad_k = compute_loss_and_grad()\n",
    "    v_k = -0.5 * grad_k * dt  \n",
    "    \n",
    "    consecutive_decreases = 0\n",
    "    consecutive_negative_dot_products = 0\n",
    "    successful_steps = 0\n",
    "    current_dt = dt\n",
    "    \n",
    "    print(f\"Initial loss: {train_loss:.6f}, Gradient norm: {torch.norm(grad_k):.6f}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        x_k_old = x_k.clone()\n",
    "        v_k_old = v_k.clone()\n",
    "        grad_k_old = grad_k.clone()\n",
    "        v_k_norm_old = torch.norm(v_k)\n",
    "        \n",
    "        # Step A: Compute step size and limit if necessary\n",
    "        step_size = torch.norm(v_k) * current_dt\n",
    "        if step_size > max_step:\n",
    "            v_k = max_step * v_k / step_size\n",
    "            step_size = max_step\n",
    "        \n",
    "        # Step B: Leap-frog integration\n",
    "        x_k = x_k + v_k * current_dt\n",
    "        update_model_params(x_k)\n",
    "        \n",
    "        train_loss, grad_k = compute_loss_and_grad()\n",
    "        a_k = -grad_k  \n",
    "        v_k = v_k + a_k * current_dt\n",
    "        \n",
    "        # Record losses\n",
    "        test_loss = evaluate_test()\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        if record_times:\n",
    "            time_stamps.append(time.time() - start_time)\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if test_loss < best_test_loss - min_improvement:\n",
    "            best_test_loss = test_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping at epoch {epoch}: No improvement in test loss for {early_stopping_patience} epochs\")\n",
    "                print(f\"Best test loss: {best_test_loss:.6f}\")\n",
    "                break\n",
    "        \n",
    "        # Step C: Check convergence\n",
    "        grad_norm = torch.norm(grad_k)\n",
    "        if grad_norm < convergence_tol:\n",
    "            print(f\"Converged at epoch {epoch}: gradient norm {grad_norm:.2e}\")\n",
    "            break\n",
    "        \n",
    "        if epoch > 0:\n",
    "            dot_product = torch.dot(grad_k, grad_k_old)\n",
    "            velocity_gradient_dot = torch.dot(v_k, grad_k)\n",
    "            \n",
    "            if dot_product <= 0:\n",
    "                consecutive_negative_dot_products += 1\n",
    "            else:\n",
    "                consecutive_negative_dot_products = 0\n",
    "                \n",
    "            if velocity_gradient_dot > 0:\n",
    "                consecutive_negative_dot_products += 1  \n",
    "        \n",
    "        # Time step reduction\n",
    "        if consecutive_negative_dot_products >= time_step_reduction_threshold:\n",
    "            current_dt = current_dt / 2\n",
    "            current_dt = max(current_dt, 1e-6)\n",
    "            x_k = (x_k + x_k_old) / 2\n",
    "            v_k = (v_k + v_k_old) / 4\n",
    "            update_model_params(x_k)\n",
    "            consecutive_negative_dot_products = 0\n",
    "            successful_steps = 0\n",
    "            if print_every > 0 and epoch % print_every == 0:\n",
    "                print(f\"Epoch {epoch}: Reduced time step to {current_dt:.6f}\")\n",
    "        \n",
    "        # Step D: Energy monitoring (kinetic energy check)\n",
    "        v_k_norm = torch.norm(v_k)\n",
    "        if v_k_norm > v_k_norm_old:\n",
    "            # Kinetic energy increased - successful step\n",
    "            consecutive_decreases = 0\n",
    "            if step_size < max_step:\n",
    "                successful_steps += 1\n",
    "                growth_factor = min(1.01, 1 + successful_steps * time_step_increase_factor)\n",
    "                current_dt = growth_factor * current_dt\n",
    "                current_dt = max(current_dt, 1e-6)\n",
    "\n",
    "        else:\n",
    "            consecutive_decreases += 1\n",
    "            successful_steps = 0\n",
    "            \n",
    "            x_k = (x_k + x_k_old) / 2\n",
    "            update_model_params(x_k)\n",
    "            \n",
    "            if consecutive_decreases <= max_consecutive_decreases:\n",
    "                v_k = (v_k + v_k_old) / 4\n",
    "            else:\n",
    "                v_k = torch.zeros_like(v_k)\n",
    "                consecutive_decreases = 0\n",
    "        \n",
    "        if print_every > 0 and epoch % print_every == 0:\n",
    "            print(f\"Epoch {epoch}: Train Loss = {train_loss:.6f}, \"\n",
    "                  f\"Test Loss = {test_loss:.6f}, Grad Norm = {grad_norm:.6f}, \"\n",
    "                  f\"dt = {current_dt:.6f}\")\n",
    "\n",
    "    if record_times:\n",
    "        return train_losses, test_losses, time_stamps\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34a1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage of LFROG - classification\n",
    "# model = FeedforwardNN(input_dim=4, hidden_dim=16, output_dim=3)\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(X, y, classification=True)\n",
    "\n",
    "# X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "# X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# train_losses, test_losses = train_lfrog(\n",
    "#     model=model,\n",
    "#     X_train=X_train_tensor,\n",
    "#     y_train=y_train_tensor,\n",
    "#     X_test=X_test_tensor,\n",
    "#     y_test=y_test_tensor,\n",
    "#     epochs=500,\n",
    "#     dt=0.005,\n",
    "#     max_step=0.05,\n",
    "#     early_stopping_patience=50,  # Stop if no improvement for 50 epochs\n",
    "#     min_improvement=1e-6,        # Minimum improvement threshold\n",
    "#     print_every=25,\n",
    "#     loss_fn=nn.CrossEntropyLoss()\n",
    "# )\n",
    "# print(\"Train Losses:\", train_losses)\n",
    "# print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e5fee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage of LFROG - function approximation\n",
    "# model = FeedforwardNN(input_dim=1, hidden_dim=16, output_dim=1)\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(X_sine, y_sine, classification=False)\n",
    "# X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "# X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)    \n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "# train_losses, test_losses = train_lfrog(\n",
    "#     model=model,\n",
    "#     X_train=X_train_tensor,\n",
    "#     y_train=y_train_tensor,\n",
    "#     X_test=X_test_tensor,\n",
    "#     y_test=y_test_tensor,   \n",
    "#     epochs=500,\n",
    "#     dt=0.005,\n",
    "#     max_step=0.05,\n",
    "#     early_stopping_patience=50,  # Stop if no improvement for 50 epochs\n",
    "#     min_improvement=1e-6,        # Minimum improvement threshold\n",
    "#     print_every=25,\n",
    "#     loss_fn=nn.MSELoss()\n",
    "# )\n",
    "# print(\"Train Losses:\", train_losses)\n",
    "# print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3075a1",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed14b18",
   "metadata": {},
   "source": [
    "### Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477cad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_seed = 123\n",
    "R = [42, 1337, 2025, 31415, 27182]\n",
    "datasets = {}\n",
    "\n",
    "# --- Iris (classification) ---\n",
    "X_iris, y_iris = load_iris_data()\n",
    "splits = split_train_val_test(X_iris, y_iris,\n",
    "                              test_size=0.15, val_fraction_of_total=0.15,\n",
    "                              random_state=split_seed, stratify=y_iris)\n",
    "X_train_iris, X_val_iris, X_test_iris, y_train_iris, y_val_iris, y_test_iris = preprocess_data(*splits, scale=True, impute=False, classification=True)\n",
    "datasets[\"iris\"] = {\n",
    "    \"train\": (X_train_iris, y_train_iris),\n",
    "    \"val\":   (X_val_iris, y_val_iris),\n",
    "    \"test\":  (X_test_iris, y_test_iris),\n",
    "}\n",
    "\n",
    "# --- Stroke Prediction (classification, needs imputation) ---\n",
    "X_stroke, y_stroke = load_stroke_data()\n",
    "splits = split_train_val_test(X_stroke, y_stroke,\n",
    "                              test_size=0.15, val_fraction_of_total=0.15,\n",
    "                              random_state=split_seed, stratify=y_stroke)\n",
    "X_train_stroke, X_val_stroke, X_test_stroke, y_train_stroke, y_val_stroke, y_test_stroke = preprocess_data(*splits, scale=True, impute=True, classification=True)\n",
    "datasets[\"stroke\"] = {\n",
    "    \"train\": (X_train_stroke, y_train_stroke),\n",
    "    \"val\":   (X_val_stroke, y_val_stroke),\n",
    "    \"test\":  (X_test_stroke, y_test_stroke),\n",
    "}\n",
    "\n",
    "# --- Sine Wave (regression, synthetic) ---\n",
    "X_sine, y_sine = generate_sine_wave_data(num_samples=1000, noise_level=0.25, random_state=split_seed)\n",
    "splits = split_train_val_test(X_sine, y_sine,\n",
    "                              test_size=0.15, val_fraction_of_total=0.15,\n",
    "                              random_state=split_seed, stratify=None)\n",
    "X_train_sine, X_val_sine, X_test_sine, y_train_sine, y_val_sine, y_test_sine = preprocess_data(*splits, scale=True, impute=False, classification=False)\n",
    "datasets[\"sine\"] = {\n",
    "    \"train\": (X_train_sine, y_train_sine),\n",
    "    \"val\":   (X_val_sine, y_val_sine),\n",
    "    \"test\":  (X_test_sine, y_test_sine),\n",
    "}\n",
    "\n",
    "# --- Wine Quality (regression) ---\n",
    "X_wine, y_wine = load_wine_quality_data()\n",
    "splits = split_train_val_test(X_wine, y_wine,\n",
    "                              test_size=0.15, val_fraction_of_total=0.15,\n",
    "                              random_state=split_seed, stratify=None)\n",
    "X_train_wine, X_val_wine, X_test_wine, y_train_wine, y_val_wine, y_test_wine = preprocess_data(*splits, scale=True, impute=False, classification=False)\n",
    "datasets[\"wine\"] = {\n",
    "    \"train\": (X_train_wine, y_train_wine),\n",
    "    \"val\":   (X_val_wine, y_val_wine),\n",
    "    \"test\":  (X_test_wine, y_test_wine),\n",
    "}\n",
    "\n",
    "# --- California Housing (regression) ---\n",
    "X_california, y_california = load_california_housing_data(n_samples=10000, random_state=split_seed)\n",
    "splits = split_train_val_test(X_california, y_california,\n",
    "                              test_size=0.15, val_fraction_of_total=0.15,\n",
    "                              random_state=split_seed, stratify=None)\n",
    "X_train_cal, X_val_cal, X_test_cal, y_train_cal, y_val_cal, y_test_cal = preprocess_data(*splits, scale=True, impute=False, classification=False)\n",
    "datasets[\"california\"] = {\n",
    "    \"train\": (X_train_cal, y_train_cal),\n",
    "    \"val\":   (X_val_cal, y_val_cal),\n",
    "    \"test\":  (X_test_cal, y_test_cal),\n",
    "}\n",
    "\n",
    "# --- MNIST (classification, pixel values 0–255 → scale to [0,1]) ---\n",
    "X_mnist, y_mnist = load_mnist_subset(n_samples=10000, random_state=split_seed)\n",
    "splits = split_train_val_test(X_mnist, y_mnist,\n",
    "                              test_size=0.15, val_fraction_of_total=0.15,\n",
    "                              random_state=split_seed, stratify=y_mnist)\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = splits\n",
    "X_train = np.array(X_train) / 255.0\n",
    "X_val   = np.array(X_val) / 255.0\n",
    "X_test  = np.array(X_test) / 255.0\n",
    "y_train = np.array(y_train).astype(int)\n",
    "y_val   = np.array(y_val).astype(int)\n",
    "y_test  = np.array(y_test).astype(int)\n",
    "datasets[\"mnist\"] = {\n",
    "    \"train\": (X_train, y_train),\n",
    "    \"val\":   (X_val, y_val),\n",
    "    \"test\":  (X_test, y_test),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1b96fe",
   "metadata": {},
   "source": [
    "### Baseline sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b4ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 8  \n",
    "\n",
    "# Sine dataset (regression)\n",
    "X_train_sine, y_train_sine = datasets[\"sine\"][\"train\"]\n",
    "X_test_sine, y_test_sine = datasets[\"sine\"][\"test\"]\n",
    "\n",
    "model = FeedforwardNN(input_dim=X_train_sine.shape[1], hidden_dim=hidden_dim, output_dim=1, dropout_p=0.3)\n",
    "torch.save(model.state_dict(), \"init_sine_baseline.pth\")\n",
    "\n",
    "# SGD\n",
    "train_losses, test_losses = train_sgd(\n",
    "    model,\n",
    "    X_train_sine,\n",
    "    np.array(y_train_sine).reshape(-1, 1).astype(np.float32),\n",
    "    X_test_sine,\n",
    "    np.array(y_test_sine).reshape(-1, 1).astype(np.float32),\n",
    "    epochs=100,\n",
    "    lr=0.01,\n",
    "    batch_size=32,\n",
    "    classification=False,\n",
    "    early_stopping_patience=15\n",
    ")\n",
    "print(f\"Sine SGD baseline: Final train loss={train_losses[-1]:.4f}, test loss={test_losses[-1]:.4f}\")\n",
    "\n",
    "# SCG\n",
    "model.load_state_dict(torch.load(\"init_sine_baseline.pth\"))\n",
    "X_train_tensor = torch.tensor(X_train_sine, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(np.array(y_train_sine).reshape(-1, 1).astype(np.float32))\n",
    "X_test_tensor = torch.tensor(X_test_sine, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(np.array(y_test_sine).reshape(-1, 1).astype(np.float32))\n",
    "train_losses, test_losses = train_scg(\n",
    "    model,\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    X_test_tensor,\n",
    "    y_test_tensor,\n",
    "    max_epochs=100,\n",
    "    tolerance=1e-5,\n",
    "    verbose=False,\n",
    "    eval_freq=1\n",
    ")\n",
    "print(f\"Sine SCG baseline: Final train loss={train_losses[-1]:.4f}, test loss={test_losses[-1]:.4f}\")\n",
    "\n",
    "# LeapFrog\n",
    "model.load_state_dict(torch.load(\"init_sine_baseline.pth\"))\n",
    "train_losses, test_losses = train_lfrog(\n",
    "    model=model,\n",
    "    X_train=X_train_tensor,\n",
    "    y_train=y_train_tensor,\n",
    "    X_test=X_test_tensor,\n",
    "    y_test=y_test_tensor,   \n",
    "    epochs=100,\n",
    "    dt=0.005,\n",
    "    max_step=0.05,\n",
    "    early_stopping_patience=50,  \n",
    "    min_improvement=1e-6,      \n",
    "    print_every=0,\n",
    "    loss_fn=nn.MSELoss()\n",
    ")\n",
    "print(f\"Sine LeapFrog baseline: Final train loss={train_losses[-1]:.4f}, test loss={test_losses[-1]:.4f}\")\n",
    "\n",
    "# Iris dataset (classification)\n",
    "X_train_iris, y_train_iris = datasets[\"iris\"][\"train\"]\n",
    "X_test_iris, y_test_iris = datasets[\"iris\"][\"test\"]\n",
    "output_dim_iris = len(np.unique(y_train_iris))\n",
    "\n",
    "model = FeedforwardNN(input_dim=X_train_iris.shape[1], hidden_dim=hidden_dim, output_dim=output_dim_iris)\n",
    "torch.save(model.state_dict(), \"init_iris_baseline.pth\")\n",
    "\n",
    "# SGD\n",
    "train_losses, test_losses = train_sgd(\n",
    "    model,\n",
    "    X_train_iris,\n",
    "    y_train_iris,\n",
    "    X_test_iris,\n",
    "    y_test_iris,\n",
    "    epochs=100,\n",
    "    lr=0.01,\n",
    "    batch_size=32,\n",
    "    classification=True\n",
    ")\n",
    "print(f\"Iris SGD baseline: Final train loss={train_losses[-1]:.4f}, test loss={test_losses[-1]:.4f}\")\n",
    "\n",
    "# SCG\n",
    "model.load_state_dict(torch.load(\"init_iris_baseline.pth\"))\n",
    "X_train_tensor = torch.tensor(X_train_iris, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_iris, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_iris, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_iris, dtype=torch.long)\n",
    "train_losses, test_losses = train_scg(\n",
    "    model,\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    X_test_tensor,\n",
    "    y_test_tensor,\n",
    "    max_epochs=100,\n",
    "    tolerance=1e-5,\n",
    "    verbose=False,\n",
    "    eval_freq=1\n",
    ")\n",
    "print(f\"Iris SCG baseline: Final train loss={train_losses[-1]:.4f}, test loss={test_losses[-1]:.4f}\")\n",
    "\n",
    "# LeapFrog\n",
    "model.load_state_dict(torch.load(\"init_iris_baseline.pth\"))\n",
    "train_losses, test_losses = train_lfrog(\n",
    "    model=model,\n",
    "    X_train=X_train_tensor,\n",
    "    y_train=y_train_tensor,\n",
    "    X_test=X_test_tensor,\n",
    "    y_test=y_test_tensor,\n",
    "    epochs=100,\n",
    "    dt=0.005,\n",
    "    max_step=0.05,\n",
    "    early_stopping_patience=50,  \n",
    "    min_improvement=1e-6,        \n",
    "    print_every=0,\n",
    "    loss_fn=nn.CrossEntropyLoss()\n",
    ")\n",
    "print(f\"Iris LeapFrog baseline: Final train loss={train_losses[-1]:.4f}, test loss={test_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152ff200",
   "metadata": {},
   "source": [
    "### Optimal Hidden units search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c410659",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "n_runs = 3  \n",
    "\n",
    "def hidden_unit_search(dataset_name, classification=True):\n",
    "    X_train, y_train = datasets[dataset_name][\"train\"]\n",
    "    X_val, y_val = datasets[dataset_name][\"val\"]\n",
    "    output_dim = len(np.unique(y_train)) if classification else 1\n",
    "\n",
    "    results = []\n",
    "    for h in hidden_sizes:\n",
    "        val_metrics = []\n",
    "        accs = []\n",
    "        f1s = []\n",
    "        r2s = []\n",
    "        mses = []\n",
    "        for seed in R[:n_runs]:\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            model = FeedforwardNN(\n",
    "                input_dim=X_train.shape[1],\n",
    "                hidden_dim=h,\n",
    "                output_dim=output_dim\n",
    "            )\n",
    "            # For regression, ensure targets are float32 and shaped correctly\n",
    "            if classification:\n",
    "                y_train_run = y_train\n",
    "                y_val_run = y_val\n",
    "            else:\n",
    "                y_train_run = np.array(y_train).reshape(-1, 1).astype(np.float32)\n",
    "                y_val_run = np.array(y_val).reshape(-1, 1).astype(np.float32)\n",
    "            _, val_losses = train_sgd(\n",
    "                model,\n",
    "                X_train,\n",
    "                y_train_run,\n",
    "                X_val,\n",
    "                y_val_run,\n",
    "                epochs=100,\n",
    "                lr=0.01,\n",
    "                batch_size=32,\n",
    "                classification=classification\n",
    "            )\n",
    "            val_metrics.append(val_losses[-1])\n",
    "\n",
    "            model.eval()\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(X_val_tensor)\n",
    "\n",
    "                if classification:\n",
    "                    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "                    accs.append(accuracy_score(y_val, preds))\n",
    "                    f1s.append(f1_score(y_val, preds, average=\"weighted\"))\n",
    "                else:\n",
    "                    preds = outputs.cpu().numpy().flatten()\n",
    "                    mses.append(mean_squared_error(y_val_run.flatten(), preds))\n",
    "                    r2s.append(r2_score(y_val_run.flatten(), preds))\n",
    "\n",
    "        result = {\n",
    "            \"hidden_dim\": h,\n",
    "            \"mean_val\": float(np.mean(val_metrics)),\n",
    "            \"std_val\": float(np.std(val_metrics)),\n",
    "            \"all_val\": [float(x) for x in val_metrics],\n",
    "        }\n",
    "\n",
    "        if classification:\n",
    "            result.update({\n",
    "                \"mean_acc\": float(np.mean(accs)),\n",
    "                \"std_acc\": float(np.std(accs)),\n",
    "                \"all_acc\": accs,\n",
    "                \"mean_f1\": float(np.mean(f1s)),\n",
    "                \"std_f1\": float(np.std(f1s)),\n",
    "                \"all_f1\": f1s,\n",
    "            })\n",
    "        else:\n",
    "            result.update({\n",
    "                \"mean_mse\": float(np.mean(mses)),\n",
    "                \"std_mse\": float(np.std(mses)),\n",
    "                \"all_mse\": mses,\n",
    "                \"mean_r2\": float(np.mean(r2s)),\n",
    "                \"std_r2\": float(np.std(r2s)),\n",
    "                \"all_r2\": r2s,\n",
    "            })\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "def select_best_hidden(results, task=\"classification\", metric=None, n_runs=3, relative_rule=False, relative_pct=0.01):\n",
    "\n",
    "    if metric is None:\n",
    "        metric = \"val\" if task == \"classification\" else \"mse\"\n",
    "\n",
    "    key = f\"mean_{metric}\"\n",
    "\n",
    "    if relative_rule:\n",
    "        if metric in [\"val\", \"mse\"]:  \n",
    "            best = min(results, key=lambda x: x[key])\n",
    "            threshold = best[key] * (1 + relative_pct)\n",
    "            candidates = [r for r in results if r[key] <= threshold]\n",
    "        else: \n",
    "            best = max(results, key=lambda x: x[key])\n",
    "            threshold = best[key] * (1 - relative_pct)\n",
    "            candidates = [r for r in results if r[key] >= threshold]\n",
    "        se_best = None  \n",
    "    else:\n",
    "        if metric in [\"val\", \"mse\"]:  \n",
    "            best = min(results, key=lambda x: x[key])\n",
    "            se_best = best[f\"std_{metric}\"] / np.sqrt(n_runs)\n",
    "            threshold = best[key] + se_best\n",
    "            candidates = [r for r in results if r[key] <= threshold]\n",
    "        else:  \n",
    "            best = max(results, key=lambda x: x[key])\n",
    "            se_best = best[f\"std_{metric}\"] / np.sqrt(n_runs)\n",
    "            threshold = best[key] - se_best\n",
    "            candidates = [r for r in results if r[key] >= threshold]\n",
    "\n",
    "    parsimonious = min(candidates, key=lambda x: x[\"hidden_dim\"])\n",
    "    return parsimonious, best, se_best, threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1437bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = {}\n",
    "\n",
    "# Sine (regression)\n",
    "sine_results = hidden_unit_search(\"sine\", classification=False)\n",
    "parsimonious, best, se_best, threshold = select_best_hidden(sine_results, task=\"regression\", metric=\"mse\", n_runs=n_runs, relative_rule=True)\n",
    "search_results[\"sine\"] = {\n",
    "    \"results\": sine_results,\n",
    "    \"selected\": parsimonious,\n",
    "    \"best\": best,\n",
    "    \"se_best\": se_best,\n",
    "    \"threshold\": threshold\n",
    "}\n",
    "print(f\"Sine: Selected hidden units = {parsimonious['hidden_dim']} (mean val loss: {parsimonious['mean_val']:.4f})\")\n",
    "\n",
    "# Iris (classification)\n",
    "iris_results = hidden_unit_search(\"iris\", classification=True)\n",
    "parsimonious, best, se_best, threshold = select_best_hidden(iris_results, task=\"classification\", metric=\"val\", n_runs=n_runs, relative_rule=True)\n",
    "search_results[\"iris\"] = {\n",
    "    \"results\": iris_results,\n",
    "    \"selected\": parsimonious,\n",
    "    \"best\": best,\n",
    "    \"se_best\": se_best,\n",
    "    \"threshold\": threshold\n",
    "}\n",
    "print(f\"Iris: Selected hidden units = {parsimonious['hidden_dim']} (mean val loss: {parsimonious['mean_val']:.4f})\")\n",
    "\n",
    "# Stroke (classification)\n",
    "stroke_results = hidden_unit_search(\"stroke\", classification=True)\n",
    "parsimonious, best, se_best, threshold = select_best_hidden(stroke_results, task=\"classification\", metric=\"val\", n_runs=n_runs, relative_rule=True)\n",
    "search_results[\"stroke\"] = {\n",
    "    \"results\": stroke_results,\n",
    "    \"selected\": parsimonious,\n",
    "    \"best\": best,\n",
    "    \"se_best\": se_best,\n",
    "    \"threshold\": threshold\n",
    "}\n",
    "print(f\"Stroke: Selected hidden units = {parsimonious['hidden_dim']} (mean val loss: {parsimonious['mean_val']:.4f})\")\n",
    "\n",
    "# Wine (regression)\n",
    "wine_results = hidden_unit_search(\"wine\", classification=False)\n",
    "parsimonious, best, se_best, threshold = select_best_hidden(wine_results, task=\"regression\", metric=\"mse\", n_runs=n_runs, relative_rule=True)\n",
    "search_results[\"wine\"] = {\n",
    "    \"results\": wine_results,\n",
    "    \"selected\": parsimonious,\n",
    "    \"best\": best,\n",
    "    \"se_best\": se_best,\n",
    "    \"threshold\": threshold\n",
    "}\n",
    "print(f\"Wine: Selected hidden units = {parsimonious['hidden_dim']} (mean val loss: {parsimonious['mean_val']:.4f})\")\n",
    "\n",
    "# California Housing (regression)\n",
    "california_results = hidden_unit_search(\"california\", classification=False)\n",
    "parsimonious, best, se_best, threshold = select_best_hidden(california_results, task=\"regression\", metric=\"mse\", n_runs=n_runs, relative_rule=True)\n",
    "search_results[\"california\"] = {\n",
    "    \"results\": california_results,\n",
    "    \"selected\": parsimonious,\n",
    "    \"best\": best,\n",
    "    \"se_best\": se_best,\n",
    "    \"threshold\": threshold\n",
    "}\n",
    "print(f\"California: Selected hidden units = {parsimonious['hidden_dim']} (mean val loss: {parsimonious['mean_val']:.4f})\")\n",
    "\n",
    "# MNIST (classification)\n",
    "mnist_results = hidden_unit_search(\"mnist\", classification=True)\n",
    "parsimonious, best, se_best, threshold = select_best_hidden(mnist_results, task=\"classification\", metric=\"val\", n_runs=n_runs, relative_rule=True)\n",
    "search_results[\"mnist\"] = {\n",
    "    \"results\": mnist_results,\n",
    "    \"selected\": parsimonious,\n",
    "    \"best\": best,\n",
    "    \"se_best\": se_best,\n",
    "    \"threshold\": threshold\n",
    "}\n",
    "print(f\"MNIST: Selected hidden units = {parsimonious['hidden_dim']} (mean val loss: {parsimonious['mean_val']:.4f})\")\n",
    "\n",
    "# --- Save results to file ---\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "run_id = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"results/hidden_unit_search_results_{run_id}.json\"\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump(search_results, f, indent=2)\n",
    "\n",
    "print(f\"All hidden unit search results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc4bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "with open(\"results/hidden_unit_search_results_20250930_173007.json\", \"r\") as f:\n",
    "    search_results = json.load(f)\n",
    "\n",
    "ordered_datasets = [\"sine\", \"iris\", \"stroke\", \"wine\", \"california\", \"mnist\"]\n",
    "dataset_names = [d for d in ordered_datasets if d in search_results]\n",
    "\n",
    "rows, cols = 2, 3\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 3.2 * rows), squeeze=False)\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for i, ds in enumerate(dataset_names):\n",
    "    ax = axes_flat[i]\n",
    "    results = search_results[ds][\"results\"]\n",
    "    hidden_sizes = [r[\"hidden_dim\"] for r in results]\n",
    "    mean_vals = [r[\"mean_val\"] for r in results]\n",
    "    std_vals = [r[\"std_val\"] for r in results]\n",
    "    ax.errorbar(hidden_sizes, mean_vals, yerr=std_vals, fmt='-o', capsize=4)\n",
    "    ax.set_title(ds.capitalize())\n",
    "    ax.set_xlabel(\"Hidden units\")\n",
    "    ax.set_ylabel(\"Mean val loss\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "# Hide any unused subplot slots\n",
    "for j in range(len(dataset_names), len(axes_flat)):\n",
    "    axes_flat[j].set_visible(False)\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2872aeda",
   "metadata": {},
   "source": [
    "### Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b12d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_tuning(\n",
    "    dataset_name,\n",
    "    param_grid,\n",
    "    hidden_dim,\n",
    "    optimizer_name=\"sgd\", \n",
    "    classification=True,\n",
    "    n_runs=3,\n",
    "    epochs=50,\n",
    "    batch_size=32\n",
    "):\n",
    "    X_train, y_train = datasets[dataset_name][\"train\"]\n",
    "    X_val, y_val = datasets[dataset_name][\"val\"]\n",
    "    output_dim = len(np.unique(y_train)) if classification else 1\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Select the training function\n",
    "    if optimizer_name == \"sgd\":\n",
    "        train_fn = train_sgd\n",
    "    elif optimizer_name == \"scg\":\n",
    "        train_fn = train_scg\n",
    "    elif optimizer_name == \"lfrog\":\n",
    "        train_fn = train_lfrog\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "\n",
    "    # Use R seed list for reproducibility\n",
    "    seeds = R[:n_runs] if 'R' in globals() else list(range(n_runs))\n",
    "\n",
    "\n",
    "    for params in param_grid:\n",
    "        val_losses = []\n",
    "        accs, f1s, mses, r2s = [], [], [], []\n",
    "        times = []\n",
    "\n",
    "        for seed in seeds:\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "            model = FeedforwardNN(\n",
    "                input_dim=input_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                output_dim=output_dim\n",
    "            )\n",
    "\n",
    "            if classification:\n",
    "                y_train_run, y_val_run = y_train, y_val\n",
    "            else:\n",
    "                y_train_run = np.array(y_train).reshape(-1, 1).astype(np.float32)\n",
    "                y_val_run = np.array(y_val).reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "            extra_kwargs = {k: v for k, v in params.items() if k != \"hidden_dim\"}\n",
    "\n",
    "            start_time = time.time()\n",
    "            if optimizer_name == \"sgd\":\n",
    "                _, val_loss_curve = train_fn(\n",
    "                    model,\n",
    "                    X_train,\n",
    "                    y_train_run,\n",
    "                    X_val,\n",
    "                    y_val_run,\n",
    "                    epochs=epochs,\n",
    "                    lr=extra_kwargs.get(\"lr\", 0.01),\n",
    "                    batch_size=batch_size,\n",
    "                    classification=classification,\n",
    "                    momentum=extra_kwargs.get(\"momentum\", 0.0)\n",
    "                )\n",
    "            elif optimizer_name == \"scg\":\n",
    "                # Convert data to torch tensors\n",
    "                X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "                y_train_tensor = torch.tensor(y_train_run, dtype=torch.long if classification else torch.float32)\n",
    "                X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "                y_val_tensor = torch.tensor(y_val_run, dtype=torch.long if classification else torch.float32)\n",
    "                _, val_loss_curve = train_fn(\n",
    "                    model,\n",
    "                    X_train_tensor,\n",
    "                    y_train_tensor,\n",
    "                    X_val_tensor,\n",
    "                    y_val_tensor,\n",
    "                    max_epochs=epochs,\n",
    "                    **extra_kwargs\n",
    "                )\n",
    "            elif optimizer_name == \"lfrog\":\n",
    "                # Convert data to torch tensors\n",
    "                X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "                y_train_tensor = torch.tensor(y_train_run, dtype=torch.long if classification else torch.float32)\n",
    "                X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "                y_val_tensor = torch.tensor(y_val_run, dtype=torch.long if classification else torch.float32)\n",
    "                # Choose loss_fn\n",
    "                loss_fn = nn.CrossEntropyLoss() if classification else nn.MSELoss()\n",
    "                _, val_loss_curve = train_fn(\n",
    "                    model,\n",
    "                    X_train_tensor,\n",
    "                    y_train_tensor,\n",
    "                    X_val_tensor,\n",
    "                    y_val_tensor,\n",
    "                    epochs=epochs,\n",
    "                    loss_fn=loss_fn,\n",
    "                    **extra_kwargs\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed = end_time - start_time\n",
    "            times.append(elapsed)\n",
    "            val_losses.append(val_loss_curve[-1])\n",
    "\n",
    "            model.eval()\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(X_val_tensor)\n",
    "                if classification:\n",
    "                    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "                    accs.append(accuracy_score(y_val, preds))\n",
    "                    f1s.append(f1_score(y_val, preds, average=\"weighted\"))\n",
    "                else:\n",
    "                    preds = outputs.cpu().numpy().flatten()\n",
    "                    # Check for NaNs before appending metrics\n",
    "                    if (\n",
    "                        np.isnan(y_val_run.flatten()).any()\n",
    "                        or np.isnan(preds).any()\n",
    "                    ):\n",
    "                        print(f\"Warning: NaNs detected in validation targets or predictions for {dataset_name} (seed={seed}). Skipping metrics for this run.\")\n",
    "                        print(\"y_val_run:\", y_val_run.flatten())\n",
    "                        print(\"preds:\", preds)\n",
    "                    else:\n",
    "                        mses.append(mean_squared_error(y_val_run.flatten(), preds))\n",
    "                        r2s.append(r2_score(y_val_run.flatten(), preds))\n",
    "\n",
    "        results.append({\n",
    "            \"optimizer\": optimizer_name,\n",
    "            \"params\": params,\n",
    "            \"mean_val_loss\": float(np.mean(val_losses)),\n",
    "            \"std_val_loss\": float(np.std(val_losses)),\n",
    "            \"mean_acc\": float(np.mean(accs)) if accs else None,\n",
    "            \"std_acc\": float(np.std(accs)) if accs else None,\n",
    "            \"mean_f1\": float(np.mean(f1s)) if f1s else None,\n",
    "            \"std_f1\": float(np.std(f1s)) if f1s else None,\n",
    "            \"mean_mse\": float(np.mean(mses)) if mses else None,\n",
    "            \"std_mse\": float(np.std(mses)) if mses else None,\n",
    "            \"mean_r2\": float(np.mean(r2s)) if r2s else None,\n",
    "            \"std_r2\": float(np.std(r2s)) if r2s else None,\n",
    "            \"mean_time\": float(np.mean(times)),\n",
    "            \"std_time\": float(np.std(times)),\n",
    "            \"all_times\": [float(t) for t in times]\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c92ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hidden = {\n",
    "    \"sine\": 8,\n",
    "    \"iris\": 4,\n",
    "    \"stroke\": 8,\n",
    "    \"wine\": 8,\n",
    "    \"california\": 64,\n",
    "    \"mnist\": 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e13534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_hidden = {ds: search_results[ds][\"selected\"][\"hidden_dim\"] for ds in search_results}\n",
    "\n",
    "\n",
    "\n",
    "param_grid_sgd = [\n",
    "    {\"lr\": lr, \"momentum\": m}\n",
    "    for lr, m in product(\n",
    "        [0.1, 0.05, 0.01, 0.005, 0.001, 0.0005],\n",
    "        [0.0, 0.5, 0.9]\n",
    "    )\n",
    "]\n",
    "\n",
    "param_grid_scg = [\n",
    "    {\"sigma\": sigma, \"lambda_init\": lam}\n",
    "    for sigma, lam in product(\n",
    "        [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "        [1e-4, 1e-5, 1e-6, 1e-7]\n",
    "    )\n",
    "]\n",
    "\n",
    "param_grid_lfrog = [\n",
    "    {\"dt\": dt, \"max_step\": ms, \"convergence_tol\": tol}\n",
    "    for dt, ms, tol in product(\n",
    "        [0.5, 0.2, 0.1, 0.05, 0.01],\n",
    "        [1.0, 0.5, 0.1],\n",
    "        [1e-4, 1e-5, 1e-6]\n",
    "    )\n",
    "]\n",
    "\n",
    "# --- Run tuning ---\n",
    "all_results = {}\n",
    "\n",
    "# # Iris (classification)\n",
    "# all_results[\"iris\"] = {\n",
    "#     \"sgd\": parameter_tuning(\"iris\", param_grid_sgd, hidden_dim=best_hidden[\"iris\"], optimizer_name=\"sgd\", classification=True),\n",
    "#     \"scg\": parameter_tuning(\"iris\", param_grid_scg, hidden_dim=best_hidden[\"iris\"], optimizer_name=\"scg\", classification=True),\n",
    "#     \"lfrog\": parameter_tuning(\"iris\", param_grid_lfrog, hidden_dim=best_hidden[\"iris\"], optimizer_name=\"lfrog\", classification=True)\n",
    "# }\n",
    "\n",
    "# Sine (regression)\n",
    "all_results[\"sine\"] = {\n",
    "    \"sgd\": parameter_tuning(\"sine\", param_grid_sgd, hidden_dim=best_hidden[\"sine\"], optimizer_name=\"sgd\", classification=False),\n",
    "    \"scg\": parameter_tuning(\"sine\", param_grid_scg, hidden_dim=best_hidden[\"sine\"], optimizer_name=\"scg\", classification=False),\n",
    "    \"lfrog\": parameter_tuning(\"sine\", param_grid_lfrog, hidden_dim=best_hidden[\"sine\"], optimizer_name=\"lfrog\", classification=False)\n",
    "}\n",
    "\n",
    "# Wine (regression)\n",
    "all_results[\"wine\"] = {\n",
    "    \"sgd\": parameter_tuning(\"wine\", param_grid_sgd, hidden_dim=best_hidden[\"wine\"], optimizer_name=\"sgd\", classification=False),\n",
    "    \"scg\": parameter_tuning(\"wine\", param_grid_scg, hidden_dim=best_hidden[\"wine\"], optimizer_name=\"scg\", classification=False),\n",
    "    \"lfrog\": parameter_tuning(\"wine\", param_grid_lfrog, hidden_dim=best_hidden[\"wine\"], optimizer_name=\"lfrog\", classification=False)\n",
    "}\n",
    "\n",
    "# Stroke (classification)\n",
    "all_results[\"stroke\"] = {\n",
    "    \"sgd\": parameter_tuning(\"stroke\", param_grid_sgd, hidden_dim=best_hidden[\"stroke\"], optimizer_name=\"sgd\", classification=True),\n",
    "    \"scg\": parameter_tuning(\"stroke\", param_grid_scg, hidden_dim=best_hidden[\"stroke\"], optimizer_name=\"scg\", classification=True),\n",
    "    \"lfrog\": parameter_tuning(\"stroke\", param_grid_lfrog, hidden_dim=best_hidden[\"stroke\"], optimizer_name=\"lfrog\", classification=True)\n",
    "}\n",
    "\n",
    "# --- Save results ---\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_path = f\"results/param_tuning_results_{timestamp}.json\"\n",
    "\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(f\"All parameter tuning results saved to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbeddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_one_se(results, minimization=True):\n",
    "    if minimization:\n",
    "        # 1. Find best (lowest mean)\n",
    "        best = min(results, key=lambda x: x[\"mean_val_loss\"])\n",
    "        se_best = best[\"std_val_loss\"]  # already averaged per seed → std across runs\n",
    "        threshold = best[\"mean_val_loss\"] + se_best\n",
    "\n",
    "        # 2. All configs within one-SE\n",
    "        candidates = [r for r in results if r[\"mean_val_loss\"] <= threshold]\n",
    "\n",
    "        # 3. Parsimony: choose simplest candidate (here = lowest learning rate if SGD, lowest dt if LFROG)\n",
    "        # Fallback: pick the one with lowest mean_time if multiple remain\n",
    "        selected = min(candidates, key=lambda x: (x[\"params\"].get(\"lr\", x[\"mean_time\"])))\n",
    "    else:\n",
    "        # Same logic but for maximization\n",
    "        best = max(results, key=lambda x: x[\"mean_val_loss\"])\n",
    "        se_best = best[\"std_val_loss\"]\n",
    "        threshold = best[\"mean_val_loss\"] - se_best\n",
    "        candidates = [r for r in results if r[\"mean_val_loss\"] >= threshold]\n",
    "        selected = min(candidates, key=lambda x: (x[\"params\"].get(\"lr\", x[\"mean_time\"])))\n",
    "\n",
    "    return {\n",
    "        \"best_config\": best,\n",
    "        \"selected_config\": selected,\n",
    "        \"threshold\": threshold,\n",
    "        \"candidates\": candidates\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad95d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parsimony_key(result):\n",
    "    p = result[\"params\"]\n",
    "    if \"lr\" in p:\n",
    "        return (p[\"lr\"], result[\"mean_time\"])\n",
    "    if \"dt\" in p:\n",
    "        return (p[\"dt\"], result[\"mean_time\"])\n",
    "    if \"sigma\" in p:\n",
    "        return (p[\"sigma\"], result[\"mean_time\"])\n",
    "    return (result[\"mean_time\"],)\n",
    "\n",
    "def select_best_one_se_generic(results, n_runs, minimization=True):\n",
    "    if minimization:\n",
    "        best = min(results, key=lambda x: x[\"mean_val_loss\"])\n",
    "        se_best = best[\"std_val_loss\"] / np.sqrt(n_runs)\n",
    "        threshold = best[\"mean_val_loss\"] + se_best\n",
    "        candidates = [r for r in results if r[\"mean_val_loss\"] <= threshold]\n",
    "    else:\n",
    "        best = max(results, key=lambda x: x[\"mean_val_loss\"])\n",
    "        se_best = best[\"std_val_loss\"] / np.sqrt(n_runs)\n",
    "        threshold = best[\"mean_val_loss\"] - se_best\n",
    "        candidates = [r for r in results if r[\"mean_val_loss\"] >= threshold]\n",
    "    selected = min(candidates, key=_parsimony_key)\n",
    "    return {\"best_config\": best,\n",
    "            \"selected_config\": selected,\n",
    "            \"threshold\": threshold,\n",
    "            \"candidates\": candidates,\n",
    "            \"se_best\": se_best}\n",
    "\n",
    "def build_selection_summary(all_results, n_runs, minimization=True):\n",
    "    rows = []\n",
    "    for dataset, optim_dict in all_results.items():\n",
    "        for optimizer, results in optim_dict.items():\n",
    "            if not results:\n",
    "                continue\n",
    "            sel = select_best_one_se_generic(results, n_runs=n_runs, minimization=minimization)\n",
    "            best = sel[\"best_config\"]; chosen = sel[\"selected_config\"]\n",
    "            rows.append({\n",
    "                \"dataset\": dataset,\n",
    "                \"optimizer\": optimizer,\n",
    "                \"best_params\": json.dumps(best[\"params\"], sort_keys=True),\n",
    "                \"best_mean_val_loss\": best[\"mean_val_loss\"],\n",
    "                \"best_std_val_loss\": best[\"std_val_loss\"],\n",
    "                \"best_mean_time\": best.get(\"mean_time\"),\n",
    "                \"best_std_time\": best.get(\"std_time\"),\n",
    "                \"selected_params\": json.dumps(chosen[\"params\"], sort_keys=True),\n",
    "                \"selected_mean_val_loss\": chosen[\"mean_val_loss\"],\n",
    "                \"selected_std_val_loss\": chosen[\"std_val_loss\"],\n",
    "                \"selected_mean_time\": chosen.get(\"mean_time\"),\n",
    "                \"selected_std_time\": chosen.get(\"std_time\"),\n",
    "                \"time_ratio_selected_over_best\": (\n",
    "                    (chosen.get(\"mean_time\") / best.get(\"mean_time\"))\n",
    "                    if best.get(\"mean_time\") and chosen.get(\"mean_time\") else None\n",
    "                ),\n",
    "                \"threshold\": sel[\"threshold\"],\n",
    "                \"se_best\": sel[\"se_best\"],\n",
    "                \"n_candidates\": len(sel[\"candidates\"])\n",
    "            })\n",
    "    return pd.DataFrame(rows).sort_values([\"dataset\",\"optimizer\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c1866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"results/param_tuning_results_20251001_185306.json\", \"r\") as f:\n",
    "    all_results = json.load(f)\n",
    "\n",
    "\n",
    "sine_sgd_results = all_results[\"sine\"][\"sgd\"]\n",
    "\n",
    "selection = select_best_one_se(sine_sgd_results, minimization=True)\n",
    "\n",
    "# Ensure results JSON loaded (reuse all_results if already in scope)\n",
    "# If not loaded yet, uncomment:\n",
    "# with open(\"results/param_tuning_results_20251001_173314.json\", \"r\") as f:\n",
    "#     all_results = json.load(f)\n",
    "\n",
    "summary_df = build_selection_summary(all_results, n_runs=3, minimization=True)\n",
    "print(summary_df)\n",
    "\n",
    "# Save\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_path = f\"results/param_tuning_selection_summary_{timestamp}.csv\"\n",
    "json_path = f\"results/param_tuning_selection_summary_{timestamp}.json\"\n",
    "summary_df.to_csv(csv_path, index=False)\n",
    "summary_df.to_json(json_path, orient=\"records\", indent=2)\n",
    "print(f\"Saved summary to:\\n  {csv_path}\\n  {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e197d7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_global_best(all_results, optimizers=(\"sgd\",\"scg\",\"lfrog\"), require_full_coverage=False):\n",
    "\n",
    "    per_optimizer = {}\n",
    "    for opt in optimizers:\n",
    "        # Collect runs per dataset for this optimizer\n",
    "        per_dataset = {\n",
    "            ds: opt_dict[opt]\n",
    "            for ds, opt_dict in all_results.items()\n",
    "            if opt in opt_dict and opt_dict[opt]\n",
    "        }\n",
    "        if not per_dataset:\n",
    "            continue\n",
    "\n",
    "        # Best loss per dataset (scale-free normalization)\n",
    "        best_loss = {\n",
    "            ds: min(r[\"mean_val_loss\"] for r in runs)\n",
    "            for ds, runs in per_dataset.items()\n",
    "        }\n",
    "\n",
    "        metrics = defaultdict(lambda: {\"losses\":{}, \"rel\":{}, \"ranks\":{}, \"times\":{}})\n",
    "\n",
    "        # Accumulate stats\n",
    "        for ds, runs in per_dataset.items():\n",
    "            sorted_runs = sorted(runs, key=lambda r: r[\"mean_val_loss\"])\n",
    "            for rank, r in enumerate(sorted_runs, start=1):\n",
    "                k = tuple(sorted(r[\"params\"].items()))\n",
    "                loss = r[\"mean_val_loss\"]\n",
    "                rel_gap = (loss - best_loss[ds]) / (best_loss[ds] + 1e-12)\n",
    "                m = metrics[k]\n",
    "                m[\"losses\"][ds] = loss\n",
    "                m[\"rel\"][ds] = rel_gap\n",
    "                m[\"ranks\"][ds] = rank\n",
    "                m[\"times\"][ds] = r.get(\"mean_time\", math.nan)\n",
    "\n",
    "        rows = []\n",
    "        full_cov = len(per_dataset)\n",
    "        for k, d in metrics.items():\n",
    "            cov = len(d[\"losses\"])\n",
    "            if require_full_coverage and cov < full_cov:\n",
    "                continue\n",
    "            rels = list(d[\"rel\"].values())\n",
    "            ranks = list(d[\"ranks\"].values())\n",
    "            times = list(d[\"times\"].values())\n",
    "            rows.append({\n",
    "                \"optimizer\": opt,\n",
    "                \"params\": dict(k),\n",
    "                \"datasets_covered\": cov,\n",
    "                \"mean_relative_gap\": float(np.mean(rels)),\n",
    "                \"worst_relative_gap\": float(np.max(rels)),\n",
    "                \"avg_rank\": float(np.mean(ranks)),\n",
    "                \"mean_time_avg\": float(np.nanmean(times)),\n",
    "            })\n",
    "\n",
    "        if not rows:\n",
    "            continue\n",
    "\n",
    "        df = pd.DataFrame(rows).sort_values(\n",
    "            [\"mean_relative_gap\",\"worst_relative_gap\",\"avg_rank\",\"mean_time_avg\"]\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        per_optimizer[opt] = {\n",
    "            \"candidates\": df.to_dict(orient=\"records\"),\n",
    "            \"selected\": df.iloc[0].to_dict()\n",
    "        }\n",
    "\n",
    "    return per_optimizer\n",
    "\n",
    "global_hparam_choices = aggregate_global_best(all_results, require_full_coverage=False)\n",
    "\n",
    "print(\"Global hyperparameter choices:\")\n",
    "for opt, info in global_hparam_choices.items():\n",
    "    sel = info[\"selected\"]\n",
    "    print(f\"{opt}: {sel['params']} \"\n",
    "          f\"mean_rel_gap={sel['mean_relative_gap']:.3e} \"\n",
    "          f\"worst_rel_gap={sel['worst_relative_gap']:.3e} \"\n",
    "          f\"avg_rank={sel['avg_rank']:.2f}\")\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "with open(\"results/global_hparam_choices.json\",\"w\") as f:\n",
    "    json.dump(global_hparam_choices, f, indent=2)\n",
    "\n",
    "# Optional: flat CSV of selected\n",
    "selected_rows = [\n",
    "    {\n",
    "        \"optimizer\": opt,\n",
    "        **info[\"selected\"][\"params\"],\n",
    "        **{k: v for k, v in info[\"selected\"].items() if k not in (\"params\",\"optimizer\")}\n",
    "    }\n",
    "    for opt, info in global_hparam_choices.items()\n",
    "]\n",
    "pd.DataFrame(selected_rows).to_csv(\"results/global_hparam_choices_selected.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c7241",
   "metadata": {},
   "source": [
    "### Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b270316",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = {'lr': 0.01, 'momentum': 0.9} \n",
    "scg = {'lambda_init': 1e-07, 'sigma': 0.0005} \n",
    "lfrog = {'convergence_tol': 0.0001, 'dt': 0.2, 'max_step': 0.5} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af2061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_best_param_sanity_checks(\n",
    "    global_hparam_choices,\n",
    "    best_hidden,\n",
    "    datasets_dict,\n",
    "    max_epochs_sgd=100,\n",
    "    max_epochs_scg=100,\n",
    "    max_epochs_lfrog=100\n",
    "):\n",
    "\n",
    "    selected_params = {\n",
    "        opt: info[\"selected\"][\"params\"] if \"selected\" in info else info[\"params\"]\n",
    "        for opt, info in global_hparam_choices.items()\n",
    "    }\n",
    "\n",
    "    classification_flags = {\n",
    "        \"iris\": True,\n",
    "        \"stroke\": True,\n",
    "        \"mnist\": True,\n",
    "        \"sine\": False,\n",
    "        \"wine\": False,\n",
    "        \"california\": False\n",
    "    }\n",
    "\n",
    "    def prepare_targets(y, classification):\n",
    "        if classification:\n",
    "            return y  \n",
    "        return np.array(y).reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "    lines = []\n",
    "    print(\"\\n===== SANITY CHECKS: Best Global Hyperparameters =====\")\n",
    "    for ds, parts in datasets_dict.items():\n",
    "        if ds not in classification_flags:\n",
    "            continue\n",
    "        is_clf = classification_flags[ds]\n",
    "        if ds not in best_hidden:\n",
    "            print(f\"[WARN] Missing hidden_dim for dataset {ds}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        (X_train, y_train) = parts[\"train\"]\n",
    "        (X_val, y_val)     = parts[\"val\"]\n",
    "        (X_test, y_test)   = parts[\"test\"]\n",
    "\n",
    "        X_train_full = np.vstack([X_train, X_val])\n",
    "        y_train_full = np.concatenate([y_train, y_val])\n",
    "\n",
    "        y_train_prepared = prepare_targets(y_train_full, is_clf)\n",
    "        y_test_prepared  = prepare_targets(y_test, is_clf)\n",
    "\n",
    "        input_dim  = X_train_full.shape[1]\n",
    "        hidden_dim = best_hidden[ds]\n",
    "        output_dim = (len(np.unique(y_train_full)) if is_clf else 1)\n",
    "\n",
    "        print(f\"\\n--- Dataset: {ds} (classification={is_clf}) | hidden_dim={hidden_dim} ---\")\n",
    "\n",
    "        for opt in (\"sgd\", \"scg\", \"lfrog\"):\n",
    "            if opt not in selected_params:\n",
    "                continue\n",
    "            params = selected_params[opt]\n",
    "            print(f\"\\nOptimizer: {opt} | params={params}\")\n",
    "\n",
    "            model = FeedforwardNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "\n",
    "            start = time.time()\n",
    "            if opt == \"sgd\":\n",
    "                lr = params.get(\"lr\", 0.01)\n",
    "                momentum = params.get(\"momentum\", 0.0)\n",
    "                train_losses, test_losses = train_sgd(\n",
    "                    model,\n",
    "                    X_train_full,\n",
    "                    y_train_prepared,\n",
    "                    X_test,\n",
    "                    y_test_prepared,\n",
    "                    epochs=max_epochs_sgd,\n",
    "                    lr=lr,\n",
    "                    batch_size=32,\n",
    "                    classification=is_clf,\n",
    "                    momentum=momentum\n",
    "                )\n",
    "            elif opt == \"scg\":\n",
    "                sigma = params.get(\"sigma\", 5e-5)\n",
    "                lam   = params.get(\"lambda_init\", 5e-7)\n",
    "                X_train_t = torch.tensor(X_train_full, dtype=torch.float32)\n",
    "                X_test_t  = torch.tensor(X_test, dtype=torch.float32)\n",
    "                if is_clf:\n",
    "                    y_train_t = torch.tensor(y_train_prepared, dtype=torch.long)\n",
    "                    y_test_t  = torch.tensor(y_test_prepared, dtype=torch.long)\n",
    "                else:\n",
    "                    y_train_t = torch.tensor(y_train_prepared, dtype=torch.float32)\n",
    "                    y_test_t  = torch.tensor(y_test_prepared, dtype=torch.float32)\n",
    "                train_losses, test_losses = train_scg(\n",
    "                    model,\n",
    "                    X_train_t,\n",
    "                    y_train_t,\n",
    "                    X_test_t,\n",
    "                    y_test_t,\n",
    "                    max_epochs=max_epochs_scg,\n",
    "                    sigma=sigma,\n",
    "                    lambda_init=lam,\n",
    "                    tolerance=1e-6,\n",
    "                    verbose=False,\n",
    "                    eval_freq=max(1, max_epochs_scg // 30)\n",
    "                )\n",
    "            else:  # lfrog\n",
    "                dt = params.get(\"dt\", 0.05)\n",
    "                max_step = params.get(\"max_step\", 0.1)\n",
    "                conv_tol = params.get(\"convergence_tol\", 1e-5)\n",
    "                X_train_t = torch.tensor(X_train_full, dtype=torch.float32)\n",
    "                X_test_t  = torch.tensor(X_test, dtype=torch.float32)\n",
    "                if is_clf:\n",
    "                    y_train_t = torch.tensor(y_train_prepared, dtype=torch.long)\n",
    "                    y_test_t  = torch.tensor(y_test_prepared, dtype=torch.long)\n",
    "                    loss_fn = nn.CrossEntropyLoss()\n",
    "                else:\n",
    "                    y_train_t = torch.tensor(y_train_prepared, dtype=torch.float32)\n",
    "                    y_test_t  = torch.tensor(y_test_prepared, dtype=torch.float32)\n",
    "                    loss_fn = nn.MSELoss()\n",
    "                train_losses, test_losses = train_lfrog(\n",
    "                    model=model,\n",
    "                    X_train=X_train_t,\n",
    "                    y_train=y_train_t,\n",
    "                    X_test=X_test_t,\n",
    "                    y_test=y_test_t,\n",
    "                    loss_fn=loss_fn,\n",
    "                    epochs=max_epochs_lfrog,\n",
    "                    dt=dt,\n",
    "                    max_step=max_step,\n",
    "                    convergence_tol=conv_tol,\n",
    "                    print_every=0,\n",
    "                    early_stopping_patience=50,\n",
    "                    min_improvement=1e-6\n",
    "                )\n",
    "            elapsed = time.time() - start\n",
    "\n",
    "            model.eval()\n",
    "            X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(X_test_tensor)\n",
    "\n",
    "            if is_clf:\n",
    "                preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "                acc = accuracy_score(y_test, preds)\n",
    "                f1  = f1_score(y_test, preds, average=\"weighted\")\n",
    "                print(f\"Final Train Loss: {train_losses[-1]:.4f} | Test Loss: {test_losses[-1]:.4f} | Acc: {acc:.4f} | F1(w): {f1:.4f} | Time: {elapsed:.2f}s\")\n",
    "                lines.append({\n",
    "                    \"dataset\": ds,\n",
    "                    \"optimizer\": opt,\n",
    "                    \"params\": params,\n",
    "                    \"train_loss\": train_losses[-1],\n",
    "                    \"test_loss\": test_losses[-1],\n",
    "                    \"accuracy\": acc,\n",
    "                    \"f1_weighted\": f1,\n",
    "                    \"time_sec\": elapsed\n",
    "                })\n",
    "            else:\n",
    "                preds = outputs.cpu().numpy().flatten()\n",
    "                mse = mean_squared_error(y_test_prepared.flatten(), preds)\n",
    "                rmse = mse ** 0.5\n",
    "                r2 = r2_score(y_test_prepared.flatten(), preds)\n",
    "                print(f\"Final Train Loss: {train_losses[-1]:.4f} | Test Loss: {test_losses[-1]:.4f} | MSE: {mse:.4f} | RMSE: {rmse:.4f} | R2: {r2:.4f} | Time: {elapsed:.2f}s\")\n",
    "                lines.append({\n",
    "                    \"dataset\": ds,\n",
    "                    \"optimizer\": opt,\n",
    "                    \"params\": params,\n",
    "                    \"train_loss\": train_losses[-1],\n",
    "                    \"test_loss\": test_losses[-1],\n",
    "                    \"mse\": mse,\n",
    "                    \"rmse\": rmse,\n",
    "                    \"r2\": r2,\n",
    "                    \"time_sec\": elapsed\n",
    "                })\n",
    "\n",
    "    print(\"\\n===== SUMMARY (compact) =====\")\n",
    "    for rec in lines:\n",
    "        if \"accuracy\" in rec:\n",
    "            print(f\"{rec['dataset']:<11} {rec['optimizer']:<6} loss(train/test)= {rec['train_loss']:.3f}/{rec['test_loss']:.3f} acc={rec['accuracy']:.3f} f1={rec['f1_weighted']:.3f}\")\n",
    "        else:\n",
    "            print(f\"{rec['dataset']:<11} {rec['optimizer']:<6} loss(train/test)= {rec['train_loss']:.3f}/{rec['test_loss']:.3f} mse={rec['mse']:.3f} r2={rec['r2']:.3f}\")\n",
    "\n",
    "if 'global_hparam_choices' not in globals():\n",
    "    try:\n",
    "        with open(\"results/global_hparam_choices.json\",\"r\") as f:\n",
    "            global_hparam_choices = json.load(f)\n",
    "        print(\"Loaded global_hparam_choices from file.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"global_hparam_choices not found. Please run the aggregation cell first.\")\n",
    "else:\n",
    "    print(\"Using in-memory global_hparam_choices.\")\n",
    "\n",
    "run_best_param_sanity_checks(global_hparam_choices, best_hidden, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2860a7c1",
   "metadata": {},
   "source": [
    "### Final Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e31de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_final_experiments(\n",
    "    datasets_dict,\n",
    "    best_hidden,\n",
    "    global_hparam_choices,\n",
    "    n_runs=3,\n",
    "    epochs_map=None,\n",
    "    batch_size=32,\n",
    "    out_prefix=\"results/final_experiments\",\n",
    "    classification_flags=None,\n",
    "    save_models=True,\n",
    "    model_dir=\"results/models\",\n",
    "    save_state_dict=True,\n",
    "    save_full_model=False,\n",
    "    target_delta=0.02,\n",
    "    compute_time_to_target=True\n",
    "):\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    if save_models:\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    if classification_flags is None:\n",
    "        classification_flags = {\n",
    "            \"iris\": True,\n",
    "            \"stroke\": True,\n",
    "            \"mnist\": True,\n",
    "            \"sine\": False,\n",
    "            \"wine\": False,\n",
    "            \"california\": False\n",
    "        }\n",
    "\n",
    "    selected_params = {\n",
    "        opt: info[\"selected\"][\"params\"] if \"selected\" in info else info[\"params\"]\n",
    "        for opt, info in global_hparam_choices.items()\n",
    "    }\n",
    "\n",
    "    if epochs_map is None:\n",
    "        epochs_map = {\"sgd\": 100, \"scg\": 100, \"lfrog\": 100}\n",
    "\n",
    "    seeds = R[:n_runs] if 'R' in globals() else list(range(n_runs))\n",
    "\n",
    "    all_run_records = []\n",
    "    summary_rows = []\n",
    "    model_manifest = []\n",
    "\n",
    "    def _first_time_to_target(loss_curve, time_curve, target):\n",
    "        for l, t in zip(loss_curve, time_curve):\n",
    "            if l <= target:\n",
    "                return t\n",
    "        return None\n",
    "\n",
    "    def _prep_targets(y, classification):\n",
    "        return y if classification else np.array(y).reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "    def _metrics(y_true, y_pred, classification):\n",
    "        if classification:\n",
    "            return {\n",
    "                \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "                \"f1_weighted\": float(f1_score(y_true, y_pred, average=\"weighted\")),\n",
    "                \"confusion_matrix\": confusion_matrix(y_true, y_pred).tolist()\n",
    "            }\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        return {\n",
    "            \"mse\": float(mse),\n",
    "            \"rmse\": float(np.sqrt(mse)),\n",
    "            \"r2\": float(r2_score(y_true, y_pred)),\n",
    "            \"mae\": float(mae)\n",
    "        }\n",
    "\n",
    "    def _downsample_curve(curve, max_points=300, keep_ends=True):\n",
    "        if max_points is None or len(curve) <= max_points:\n",
    "            return [float(x) for x in curve]\n",
    "        n = len(curve)\n",
    "        idx = np.linspace(0, n - 1, max_points, dtype=int)\n",
    "        if keep_ends:\n",
    "            idx[0], idx[-1] = 0, n - 1\n",
    "        return [float(curve[i]) for i in idx]\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    for ds, parts in datasets_dict.items():\n",
    "        if ds not in classification_flags or ds not in best_hidden:\n",
    "            continue\n",
    "        is_clf = classification_flags[ds]\n",
    "\n",
    "        (X_train, y_train) = parts[\"train\"]\n",
    "        (X_val, y_val) = parts[\"val\"]\n",
    "        (X_test, y_test) = parts[\"test\"]\n",
    "\n",
    "        # Merge train + val\n",
    "        X_train_full = np.vstack([X_train, X_val])\n",
    "        y_train_full = np.concatenate([y_train, y_val])\n",
    "\n",
    "        input_dim = X_train_full.shape[1]\n",
    "        hidden_dim = best_hidden[ds]\n",
    "        output_dim = (len(np.unique(y_train_full)) if is_clf else 1)\n",
    "        dataset_run_records = []\n",
    "\n",
    "        for opt in (\"sgd\", \"scg\", \"lfrog\"):\n",
    "            if opt not in selected_params:\n",
    "                continue\n",
    "            hparams = selected_params[opt]\n",
    "            epoch_budget = epochs_map.get(opt, 100)\n",
    "\n",
    "            for run_idx, seed in enumerate(seeds, start=1):\n",
    "                np.random.seed(seed)\n",
    "                torch.manual_seed(seed)\n",
    "\n",
    "                model = FeedforwardNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, dropout_p=0.3)\n",
    "\n",
    "                y_train_prep = _prep_targets(y_train_full, is_clf)\n",
    "                y_test_prep = _prep_targets(y_test, is_clf)\n",
    "\n",
    "                start_time = time.time()\n",
    "                if opt == \"sgd\":\n",
    "                    train_curve, test_curve, test_times = train_sgd(\n",
    "                        model,\n",
    "                        X_train_full,\n",
    "                        y_train_prep,\n",
    "                        X_test,\n",
    "                        y_test_prep,\n",
    "                        epochs=epoch_budget,\n",
    "                        lr=hparams.get(\"lr\", 0.01),\n",
    "                        batch_size=batch_size,\n",
    "                        classification=is_clf,\n",
    "                        momentum=hparams.get(\"momentum\", 0.0),\n",
    "                        record_times=True,\n",
    "                        early_stopping_patience=15\n",
    "                    )\n",
    "                elif opt == \"scg\":\n",
    "                    X_tr_t = torch.tensor(X_train_full, dtype=torch.float32)\n",
    "                    X_te_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "                    if is_clf:\n",
    "                        y_tr_t = torch.tensor(y_train_prep, dtype=torch.long)\n",
    "                        y_te_t = torch.tensor(y_test_prep, dtype=torch.long)\n",
    "                    else:\n",
    "                        y_tr_t = torch.tensor(y_train_prep, dtype=torch.float32)\n",
    "                        y_te_t = torch.tensor(y_test_prep, dtype=torch.float32)\n",
    "                    train_curve, test_curve, test_times = train_scg(\n",
    "                        model,\n",
    "                        X_tr_t,\n",
    "                        y_tr_t,\n",
    "                        X_te_t,\n",
    "                        y_te_t,\n",
    "                        max_epochs=epoch_budget,\n",
    "                        sigma=hparams.get(\"sigma\", 5e-5),\n",
    "                        lambda_init=hparams.get(\"lambda_init\", 5e-7),\n",
    "                        tolerance=1e-6,\n",
    "                        verbose=False,\n",
    "                        eval_freq=max(1, epoch_budget // 30),\n",
    "                        record_times=True\n",
    "                    )\n",
    "                else:  # lfrog\n",
    "                    X_tr_t = torch.tensor(X_train_full, dtype=torch.float32)\n",
    "                    X_te_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "                    if is_clf:\n",
    "                        y_tr_t = torch.tensor(y_train_prep, dtype=torch.long)\n",
    "                        y_te_t = torch.tensor(y_test_prep, dtype=torch.long)\n",
    "                        loss_fn = nn.CrossEntropyLoss()\n",
    "                    else:\n",
    "                        y_tr_t = torch.tensor(y_train_prep, dtype=torch.float32)\n",
    "                        y_te_t = torch.tensor(y_test_prep, dtype=torch.float32)\n",
    "                        loss_fn = nn.MSELoss()\n",
    "                    train_curve, test_curve, test_times = train_lfrog(\n",
    "                        model=model,\n",
    "                        X_train=X_tr_t,\n",
    "                        y_train=y_tr_t,\n",
    "                        X_test=X_te_t,\n",
    "                        y_test=y_te_t,\n",
    "                        loss_fn=loss_fn,\n",
    "                        epochs=epoch_budget,\n",
    "                        dt=hparams.get(\"dt\", 0.05),\n",
    "                        max_step=hparams.get(\"max_step\", 0.1),\n",
    "                        convergence_tol=hparams.get(\"convergence_tol\", 1e-5),\n",
    "                        early_stopping_patience=50,\n",
    "                        min_improvement=1e-6,\n",
    "                        print_every=0,\n",
    "                        record_times=True\n",
    "                    )\n",
    "                elapsed = time.time() - start_time\n",
    "\n",
    "                # Evaluate\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "                    outputs = model(X_test_tensor)\n",
    "                    if is_clf:\n",
    "                        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "                        metrics = _metrics(y_test, preds, True)\n",
    "                    else:\n",
    "                        preds = outputs.cpu().numpy().flatten()\n",
    "                        metrics = _metrics(y_test_prep.flatten(), preds, False)\n",
    "\n",
    "                # Save model artifacts\n",
    "                model_path = None\n",
    "                if save_models:\n",
    "                    base_name = f\"{ds}_{opt}_run{run_idx}_seed{seed}_{timestamp}\"\n",
    "                    state_path = os.path.join(model_dir, base_name + \"_state.pt\")\n",
    "                    payload = {\n",
    "                        \"timestamp\": timestamp,\n",
    "                        \"dataset\": ds,\n",
    "                        \"optimizer\": opt,\n",
    "                        \"run\": run_idx,\n",
    "                        \"seed\": seed,\n",
    "                        \"input_dim\": input_dim,\n",
    "                        \"hidden_dim\": hidden_dim,\n",
    "                        \"output_dim\": output_dim,\n",
    "                        \"epoch_budget\": epoch_budget,\n",
    "                        \"hyperparams\": hparams,\n",
    "                        \"train_loss_curve\": train_curve,\n",
    "                        \"test_loss_curve\": test_curve,\n",
    "                        \"final_train_loss\": float(train_curve[-1]),\n",
    "                        \"final_test_loss\": float(test_curve[-1]),\n",
    "                        \"metrics\": metrics\n",
    "                    }\n",
    "                    if save_state_dict:\n",
    "                        payload[\"model_state_dict\"] = model.state_dict()\n",
    "                    torch.save(payload, state_path)\n",
    "                    model_path = state_path\n",
    "                    if save_full_model:\n",
    "                        full_path = os.path.join(model_dir, base_name + \"_full.pt\")\n",
    "                        torch.save(model, full_path)\n",
    "                        model_manifest.append({\n",
    "                            \"dataset\": ds,\n",
    "                            \"optimizer\": opt,\n",
    "                            \"run\": run_idx,\n",
    "                            \"seed\": seed,\n",
    "                            \"type\": \"full_model\",\n",
    "                            \"path\": full_path\n",
    "                        })\n",
    "                    model_manifest.append({\n",
    "                        \"dataset\": ds,\n",
    "                        \"optimizer\": opt,\n",
    "                        \"run\": run_idx,\n",
    "                        \"seed\": seed,\n",
    "                        \"type\": \"state_payload\",\n",
    "                        \"path\": state_path\n",
    "                    })\n",
    "\n",
    "                param_count = sum(p.numel() for p in model.parameters())\n",
    "                max_curve_points = 400\n",
    "                train_curve_ds = _downsample_curve(train_curve, max_points=max_curve_points)\n",
    "                test_curve_ds = _downsample_curve(test_curve, max_points=max_curve_points)\n",
    "\n",
    "                record = {\n",
    "                    \"dataset\": ds,\n",
    "                    \"optimizer\": opt,\n",
    "                    \"run\": run_idx,\n",
    "                    \"seed\": seed,\n",
    "                    \"hidden_dim\": hidden_dim,\n",
    "                    \"epoch_budget\": epoch_budget,\n",
    "                    \"params\": hparams,\n",
    "                    \"train_loss_curve\": train_curve_ds,\n",
    "                    \"test_loss_curve\": test_curve_ds,\n",
    "                    \"original_curve_len\": len(train_curve),\n",
    "                    \"stored_curve_len\": len(train_curve_ds),\n",
    "                    \"final_train_loss\": float(train_curve[-1]),\n",
    "                    \"final_test_loss\": float(test_curve[-1]),\n",
    "                    \"best_train_loss\": float(min(train_curve)),\n",
    "                    \"best_test_loss\": float(min(test_curve)),\n",
    "                    \"time_sec\": float(elapsed),\n",
    "                    \"n_curve_points\": len(train_curve_ds),\n",
    "                    \"model_path\": model_path,\n",
    "                    \"param_count\": int(param_count),\n",
    "                    \"test_time_curve\": test_times\n",
    "                }\n",
    "                if compute_time_to_target:\n",
    "                    record[\"full_test_loss_curve\"] = [float(x) for x in test_curve]\n",
    "\n",
    "                record.update(metrics)\n",
    "                all_run_records.append(record)\n",
    "                dataset_run_records.append(record)\n",
    "\n",
    "        if compute_time_to_target and dataset_run_records:\n",
    "            for opt in (\"sgd\", \"scg\", \"lfrog\"):\n",
    "                opt_runs = [r for r in dataset_run_records if r[\"optimizer\"] == opt]\n",
    "                if not opt_runs:\n",
    "                    continue\n",
    "                L_star_opt = min(r[\"best_test_loss\"] for r in opt_runs)\n",
    "                target_loss_opt = L_star_opt * (1 + target_delta)\n",
    "                for r in opt_runs:\n",
    "                    loss_curve = r.get(\"full_test_loss_curve\", r[\"test_loss_curve\"])\n",
    "                    time_curve = r[\"test_time_curve\"]\n",
    "                    t_hit = _first_time_to_target(loss_curve, time_curve, target_loss_opt)\n",
    "                    r[\"opt_target_loss\"] = target_loss_opt\n",
    "                    r[\"opt_time_to_target_sec\"] = t_hit\n",
    "                    r[\"opt_reached_target\"] = t_hit is not None\n",
    "\n",
    "        for opt in (\"sgd\", \"scg\", \"lfrog\"):\n",
    "            if opt not in selected_params:\n",
    "                continue\n",
    "            ds_opt_runs = [r for r in dataset_run_records if r[\"optimizer\"] == opt]\n",
    "            if not ds_opt_runs:\n",
    "                continue\n",
    "            final_tests = [r[\"final_test_loss\"] for r in ds_opt_runs]\n",
    "            summary = {\n",
    "                \"dataset\": ds,\n",
    "                \"optimizer\": opt,\n",
    "                \"hidden_dim\": hidden_dim,\n",
    "                \"params\": selected_params[opt],\n",
    "                \"n_runs\": len(ds_opt_runs),\n",
    "                \"mean_final_test_loss\": float(np.mean(final_tests)),\n",
    "                \"std_final_test_loss\": float(np.std(final_tests)),\n",
    "                \"mean_time_sec\": float(np.mean([r[\"time_sec\"] for r in ds_opt_runs]))\n",
    "            }\n",
    "            if is_clf:\n",
    "                summary.update({\n",
    "                    \"mean_accuracy\": float(np.mean([r[\"accuracy\"] for r in ds_opt_runs])),\n",
    "                    \"std_accuracy\": float(np.std([r[\"accuracy\"] for r in ds_opt_runs])),\n",
    "                    \"mean_f1_weighted\": float(np.mean([r[\"f1_weighted\"] for r in ds_opt_runs])),\n",
    "                    \"std_f1_weighted\": float(np.std([r[\"f1_weighted\"] for r in ds_opt_runs]))\n",
    "                })\n",
    "            else:\n",
    "                summary.update({\n",
    "                    \"mean_mse\": float(np.mean([r[\"mse\"] for r in ds_opt_runs])),\n",
    "                    \"std_mse\": float(np.std([r[\"mse\"] for r in ds_opt_runs])),\n",
    "                    \"mean_r2\": float(np.mean([r[\"r2\"] for r in ds_opt_runs])),\n",
    "                    \"std_r2\": float(np.std([r[\"r2\"] for r in ds_opt_runs]))\n",
    "                })\n",
    "\n",
    "            if compute_time_to_target and any(\"opt_time_to_target_sec\" in r for r in ds_opt_runs):\n",
    "                hits = [r[\"opt_time_to_target_sec\"] for r in ds_opt_runs if r.get(\"opt_reached_target\")]\n",
    "                summary.update({\n",
    "                    \"opt_target_loss\": ds_opt_runs[0].get(\"opt_target_loss\"),\n",
    "                    \"target_delta\": target_delta,\n",
    "                    \"mean_time_to_target_sec\": float(np.mean(hits)) if hits else None,\n",
    "                    \"std_time_to_target_sec\": float(np.std(hits)) if hits else None,\n",
    "                    \"n_reached_target\": int(sum(r.get(\"opt_reached_target\", False) for r in ds_opt_runs))\n",
    "                })\n",
    "            summary_rows.append(summary)\n",
    "\n",
    "    runs_json = f\"{out_prefix}_{timestamp}.json\"\n",
    "    summary_csv = f\"{out_prefix}_summary_{timestamp}.csv\"\n",
    "    with open(runs_json, \"w\") as f:\n",
    "        json.dump(all_run_records, f, indent=2)\n",
    "    pd.DataFrame(summary_rows).to_csv(summary_csv, index=False)\n",
    "\n",
    "    # Save model manifest\n",
    "    manifest_path = None\n",
    "    if save_models:\n",
    "        manifest_path = os.path.join(model_dir, f\"model_manifest_{timestamp}.json\")\n",
    "        with open(manifest_path, \"w\") as f:\n",
    "            json.dump(model_manifest, f, indent=2)\n",
    "\n",
    "    print(f\"Saved runs JSON   : {runs_json}\")\n",
    "    print(f\"Saved summary CSV : {summary_csv}\")\n",
    "    if manifest_path:\n",
    "        print(f\"Saved model manifest: {manifest_path}\")\n",
    "    return all_run_records, pd.DataFrame(summary_rows), manifest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c6097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_runs, final_summary, manifest = run_final_experiments(\n",
    "    datasets_dict=datasets,\n",
    "    best_hidden=best_hidden,\n",
    "    global_hparam_choices=global_hparam_choices,\n",
    "    n_runs=3,\n",
    "    epochs_map={\"sgd\":120,\"scg\":150,\"lfrog\":150},\n",
    "    save_models=True,\n",
    "    save_full_model=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef9d413",
   "metadata": {},
   "source": [
    "### Visualisation and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff540dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = pd.read_csv(\"results/final_experiments_summary_20251002_201217.csv\")  \n",
    "# Classification subset\n",
    "clf = final_summary[final_summary.dataset.isin([\"iris\",\"stroke\",\"mnist\"])]\n",
    "clf_table = (clf\n",
    "    .pivot(index=\"dataset\", columns=\"optimizer\",\n",
    "           values=[\"mean_accuracy\",\"mean_f1_weighted\",\"mean_time_sec\"])\n",
    "    .round(4))\n",
    "print(clf_table)\n",
    "\n",
    "# Regression subset\n",
    "reg = final_summary[~final_summary.dataset.isin([\"iris\",\"stroke\",\"mnist\"])]\n",
    "reg_table = (reg\n",
    "    .pivot(index=\"dataset\", columns=\"optimizer\",\n",
    "           values=[\"mean_mse\",\"mean_r2\",\"mean_time_sec\"])\n",
    "    .round(4))\n",
    "print(reg_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1bca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"results/final_experiments_20251002_113644.json\") as f:\n",
    "    runs = json.load(f)\n",
    "\n",
    "def plot_convergence(dataset, optimizer, curve_key=\"test_loss_curve\"):\n",
    "    ds_runs = [r for r in runs if r[\"dataset\"]==dataset and r[\"optimizer\"]==optimizer]\n",
    "    curves = [r[curve_key] for r in ds_runs]\n",
    "    max_len = max(len(c) for c in curves)\n",
    "    # Pad with NaN then compute mean ignoring NaN\n",
    "    arr = np.full((len(curves), max_len), np.nan)\n",
    "    for i,c in enumerate(curves):\n",
    "        arr[i,:len(c)] = c\n",
    "    mean = np.nanmean(arr, axis=0)\n",
    "    std  = np.nanstd(arr, axis=0)\n",
    "    x = np.arange(len(mean))\n",
    "    plt.fill_between(x, mean-std, mean+std, alpha=0.2)\n",
    "    plt.plot(x, mean, label=f\"{optimizer}\")\n",
    "    plt.xlabel(\"Evaluation step\")\n",
    "    plt.ylabel(\"Test loss\")\n",
    "    plt.title(f\"{dataset} convergence\")\n",
    "\n",
    "for ds in sorted(set(r[\"dataset\"] for r in runs)):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    for opt in [\"sgd\",\"scg\",\"lfrog\"]:\n",
    "        if any(r[\"dataset\"]==ds and r[\"optimizer\"]==opt for r in runs):\n",
    "            plot_convergence(ds,opt)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c4f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(runs)\n",
    "plt.figure(figsize=(7,4))\n",
    "sns.boxplot(data=df, x=\"dataset\", y=\"final_test_loss\", hue=\"optimizer\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Final test loss distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9520c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(runs)\n",
    "\n",
    "if \"final_train_loss\" not in df.columns:\n",
    "    raise ValueError(\"final_train_loss missing from runs records; ensure run_final_experiments saved it.\")\n",
    "\n",
    "df[\"generalization_gap\"] = df[\"final_test_loss\"] - df[\"final_train_loss\"]\n",
    "df[\"gap_ratio\"] = df[\"final_test_loss\"] / (df[\"final_train_loss\"] + 1e-12)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.boxplot(data=df, x=\"dataset\", y=\"final_test_loss\", hue=\"optimizer\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Final test loss distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.boxplot(data=df, x=\"dataset\", y=\"final_train_loss\", hue=\"optimizer\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Final train loss distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.boxplot(data=df, x=\"dataset\", y=\"generalization_gap\", hue=\"optimizer\")\n",
    "plt.axhline(0, color=\"k\", lw=1, ls=\"--\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Generalization gap (test - train)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.boxplot(data=df, x=\"dataset\", y=\"generalization_gap\", hue=\"optimizer\", dodge=True, showcaps=False, boxprops={'alpha':0.6})\n",
    "sns.stripplot(data=df, x=\"dataset\", y=\"generalization_gap\", hue=\"optimizer\", dodge=True, marker=\"o\",\n",
    "              alpha=0.6, linewidth=0.5, edgecolor=\"gray\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Generalization gap with individual runs\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summarize gaps\n",
    "gap_summary = (df.groupby([\"dataset\",\"optimizer\"])\n",
    "                 .agg(mean_gap=(\"generalization_gap\",\"mean\"),\n",
    "                      std_gap=(\"generalization_gap\",\"std\"),\n",
    "                      mean_test=(\"final_test_loss\",\"mean\"),\n",
    "                      mean_train=(\"final_train_loss\",\"mean\"))\n",
    "                 .reset_index()\n",
    "              )\n",
    "print(gap_summary.sort_values(\"mean_gap\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d45f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_convergence(\n",
    "    dataset,\n",
    "    optimizer,\n",
    "    curve_key=\"test_loss_curve\",\n",
    "    burn_in=1,\n",
    "    logy=False,\n",
    "    normalize=False,\n",
    "    smooth_window=0,\n",
    "    cap_percentile=None\n",
    "):\n",
    "    ds_runs = [r for r in runs if r[\"dataset\"]==dataset and r[\"optimizer\"]==optimizer]\n",
    "    curves = [r[curve_key] for r in ds_runs]\n",
    "\n",
    "    if burn_in > 0:\n",
    "        curves = [c[burn_in:] for c in curves if len(c) > burn_in]\n",
    "\n",
    "    # Normalize by first value\n",
    "    if normalize:\n",
    "        norm_curves = []\n",
    "        for c in curves:\n",
    "            if c and c[0] != 0:\n",
    "                norm_curves.append([x / c[0] for x in c])\n",
    "        curves = norm_curves if norm_curves else curves\n",
    "\n",
    "\n",
    "    if smooth_window and smooth_window > 1:\n",
    "        def smooth(c, w):\n",
    "            if len(c) < w:\n",
    "                return c\n",
    "            return [\n",
    "                np.mean(c[max(0,i-w+1):i+1])\n",
    "                for i in range(len(c))\n",
    "            ]\n",
    "        curves = [smooth(c, smooth_window) for c in curves]\n",
    "\n",
    "\n",
    "    if not curves:\n",
    "        return\n",
    "    max_len = max(len(c) for c in curves)\n",
    "    arr = np.full((len(curves), max_len), np.nan)\n",
    "    for i,c in enumerate(curves):\n",
    "        arr[i,:len(c)] = c\n",
    "    mean = np.nanmean(arr, axis=0)\n",
    "    std  = np.nanstd(arr, axis=0)\n",
    "\n",
    "\n",
    "    if cap_percentile:\n",
    "        cap_val = np.nanpercentile(mean, cap_percentile)\n",
    "        mean = np.minimum(mean, cap_val)\n",
    "        std = np.minimum(std, cap_val)\n",
    "\n",
    "    x = np.arange(len(mean))\n",
    "    plt.fill_between(x, mean-std, mean+std, alpha=0.2)\n",
    "    plt.plot(x, mean, label=f\"{optimizer}\")\n",
    "    plt.xlabel(\"Evaluation step\")\n",
    "    plt.ylabel(\"Normalized loss\" if normalize else \"Test loss\")\n",
    "    if logy:\n",
    "        plt.yscale(\"log\")\n",
    "    plt.title(f\"{dataset} convergence\")\n",
    "\n",
    "# Example usage with better readability\n",
    "for ds in sorted(set(r[\"dataset\"] for r in runs)):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    for opt in [\"sgd\",\"scg\",\"lfrog\"]:\n",
    "        if any(r[\"dataset\"]==ds and r[\"optimizer\"]==opt for r in runs):\n",
    "            plot_convergence(\n",
    "                ds, opt,\n",
    "                burn_in=1,     \n",
    "                logy=True,        \n",
    "                normalize=False,  \n",
    "                smooth_window=3,  \n",
    "                cap_percentile=99 \n",
    "            )\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffe0b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "runs_df = pd.DataFrame(runs)\n",
    "\n",
    "\n",
    "time_summary = (runs_df\n",
    "    .groupby([\"dataset\",\"optimizer\"])\n",
    "    .agg(mean_time=(\"time_sec\",\"mean\"),\n",
    "         std_time=(\"time_sec\",\"std\"),\n",
    "         runs=(\"time_sec\",\"count\"))\n",
    "    .reset_index())\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "sns.barplot(data=time_summary, x=\"dataset\", y=\"mean_time\", hue=\"optimizer\", capsize=0.15)\n",
    "for i,row in time_summary.iterrows():\n",
    "    plt.text(i//3 + (i%3)*0.0, 0, \"\", alpha=0)  # keep structure robust\n",
    "plt.ylabel(\"Mean training time (s)\")\n",
    "plt.title(\"Wall-clock training time (mean ± variability)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Relative time vs fastest optimizer per dataset\n",
    "fastest = time_summary.groupby(\"dataset\")[\"mean_time\"].min().rename(\"fastest_time\")\n",
    "rel = time_summary.merge(fastest, on=\"dataset\")\n",
    "rel[\"time_ratio\"] = rel[\"mean_time\"] / rel[\"fastest_time\"]\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "sns.barplot(data=rel, x=\"dataset\", y=\"time_ratio\", hue=\"optimizer\")\n",
    "plt.ylabel(\"Time / Fastest\")\n",
    "plt.title(\"Relative training time (lower is faster)\")\n",
    "plt.axhline(1.0, color=\"k\", ls=\"--\", lw=1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Loss vs wall-clock time curves (aggregated) for each dataset\n",
    "def plot_loss_vs_time(dataset, curve_key=\"test_loss_curve\", time_key=\"test_time_curve\", logy=False):\n",
    "    ds = [r for r in runs if r[\"dataset\"] == dataset]\n",
    "    if not ds:\n",
    "        return\n",
    "    plt.figure(figsize=(6,4))\n",
    "    for opt in [\"sgd\",\"scg\",\"lfrog\"]:\n",
    "        opt_runs = [r for r in ds if r[\"optimizer\"] == opt]\n",
    "        if not opt_runs:\n",
    "            continue\n",
    "        # Interpolate each run onto a common time grid\n",
    "        # Build union grid capped to min(max_time_across_runs, percentile)\n",
    "        max_end = np.median([r[time_key][-1] for r in opt_runs if len(r[time_key])])\n",
    "        grid = np.linspace(0, max_end, 200)\n",
    "        interp_curves = []\n",
    "        for r in opt_runs:\n",
    "            t = np.array(r[time_key])\n",
    "            L = np.array(r[curve_key])\n",
    "            if len(t) < 2:\n",
    "                continue\n",
    "            interp = np.interp(grid, t, L)\n",
    "            interp_curves.append(interp)\n",
    "        if not interp_curves:\n",
    "            continue\n",
    "        arr = np.vstack(interp_curves)\n",
    "        mean = arr.mean(0)\n",
    "        std  = arr.std(0)\n",
    "        plt.fill_between(grid, mean-std, mean+std, alpha=0.15)\n",
    "        plt.plot(grid, mean, label=opt)\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Test loss\")\n",
    "    if logy:\n",
    "        plt.yscale(\"log\")\n",
    "    plt.title(f\"{dataset} loss vs time\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for ds in sorted(runs_df.dataset.unique()):\n",
    "    plot_loss_vs_time(ds, logy=True)\n",
    "\n",
    "# 4. Speed-to-quality metrics\n",
    "def compute_speed_metrics(r, percentages=(0.5,0.9), curve_key=\"test_loss_curve\", time_key=\"test_time_curve\"):\n",
    "    L = np.array(r[curve_key])\n",
    "    T = np.array(r[time_key])\n",
    "    if len(L) == 0 or len(T) == 0:\n",
    "        return {}\n",
    "    L0 = L[0]\n",
    "    Lmin = L.min()\n",
    "    out = {}\n",
    "    span = L0 - Lmin\n",
    "    for p in percentages:\n",
    "        target = L0 - p * span\n",
    "\n",
    "        idx = np.where(L <= target)[0]\n",
    "        out[f\"time_to_{int(p*100)}pct_reduction\"] = float(T[idx[0]]) if len(idx) else None\n",
    "    return out\n",
    "\n",
    "speed_rows = []\n",
    "for r in runs:\n",
    "    metrics = compute_speed_metrics(r)\n",
    "    speed_rows.append({\n",
    "        \"dataset\": r[\"dataset\"],\n",
    "        \"optimizer\": r[\"optimizer\"],\n",
    "        \"run\": r[\"run\"],\n",
    "        **metrics\n",
    "    })\n",
    "speed_df = pd.DataFrame(speed_rows)\n",
    "\n",
    "speed_summary = (speed_df\n",
    "    .groupby([\"dataset\",\"optimizer\"])\n",
    "    .agg(**{\n",
    "        \"mean_t50\": (\"time_to_50pct_reduction\",\"mean\"),\n",
    "        \"std_t50\": (\"time_to_50pct_reduction\",\"std\"),\n",
    "        \"mean_t90\": (\"time_to_90pct_reduction\",\"mean\"),\n",
    "        \"std_t90\": (\"time_to_90pct_reduction\",\"std\"),\n",
    "        \"n\": (\"run\",\"count\")\n",
    "    })\n",
    "    .reset_index())\n",
    "\n",
    "print(\"Speed summary (seconds):\")\n",
    "print(speed_summary)\n",
    "\n",
    "# 5. Plot time-to-90% reduction (where available)\n",
    "plt.figure(figsize=(9,4))\n",
    "sns.barplot(data=speed_summary, x=\"dataset\", y=\"mean_t90\", hue=\"optimizer\")\n",
    "plt.ylabel(\"Time to 90% loss reduction (s)\")\n",
    "plt.title(\"Convergence speed (lower is better)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Merge into final summary export (optional)\n",
    "merged_summary = final_summary.merge(\n",
    "    time_summary[[\"dataset\",\"optimizer\",\"mean_time\",\"std_time\"]],\n",
    "    on=[\"dataset\",\"optimizer\"],\n",
    "    how=\"left\"\n",
    ").merge(\n",
    "    speed_summary[[\"dataset\",\"optimizer\",\"mean_t50\",\"mean_t90\"]],\n",
    "    on=[\"dataset\",\"optimizer\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "merged_summary.to_csv(\"results/final_with_time_metrics.csv\", index=False)\n",
    "print(\"Saved extended summary: results/final_with_time_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b3eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in sorted(df.dataset.unique()):\n",
    "    subset = df[df.dataset == ds]\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.boxplot(data=subset, x=\"optimizer\", y=\"final_test_loss\")\n",
    "    sns.stripplot(data=subset, x=\"optimizer\", y=\"final_test_loss\", color=\"k\", alpha=0.5, dodge=True)\n",
    "    plt.title(f\"{ds} - Final test loss distribution\")\n",
    "    plt.ylabel(\"Final test loss\")\n",
    "    plt.xlabel(\"Optimizer\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0de038",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[\"generalization_gap\"] = df[\"final_test_loss\"] - df[\"final_train_loss\"]\n",
    "df[\"gap_ratio\"] = df[\"final_test_loss\"] / (df[\"final_train_loss\"] + 1e-12)\n",
    "\n",
    "for ds in sorted(df.dataset.unique()):\n",
    "    subset = df[df.dataset == ds]\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.boxplot(data=subset, x=\"optimizer\", y=\"generalization_gap\")\n",
    "    sns.stripplot(data=subset, x=\"optimizer\", y=\"generalization_gap\", color=\"k\", alpha=0.5, dodge=True)\n",
    "    plt.axhline(0, color=\"k\", lw=1, ls=\"--\")\n",
    "    plt.title(f\"{ds} - Generalization gap (test - train)\")\n",
    "    plt.ylabel(\"Generalization gap\")\n",
    "    plt.xlabel(\"Optimizer\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b14fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'final_summary' not in globals():\n",
    "    final_summary = pd.read_csv(sorted(\n",
    "        [p for p in os.listdir(\"results\") if p.startswith(\"final_experiments_summary_\")],\n",
    "        reverse=True\n",
    "    )[0].join(\"results/\"))\n",
    "\n",
    "classification_datasets = [\"iris\", \"stroke\", \"mnist\"]\n",
    "regression_datasets = [d for d in final_summary.dataset.unique() if d not in classification_datasets]\n",
    "\n",
    "for ds in classification_datasets:\n",
    "    sub = final_summary[final_summary.dataset == ds]\n",
    "    if sub.empty: \n",
    "        continue\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 3.2), sharey=False)\n",
    "    order = sorted(sub.optimizer.unique())\n",
    "\n",
    "    # Accuracy\n",
    "    sns.barplot(data=sub, x=\"optimizer\", y=\"mean_accuracy\", order=order, ax=axes[0], palette=\"tab10\")\n",
    "    axes[0].set_title(f\"{ds} - Accuracy\")\n",
    "    axes[0].set_xlabel(\"Optimizer\")\n",
    "    axes[0].set_ylabel(\"Mean Accuracy\")\n",
    "    for p in axes[0].patches:\n",
    "        v = p.get_height()\n",
    "        axes[0].annotate(f\"{v:.3f}\", (p.get_x()+p.get_width()/2, v), ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "    # F1\n",
    "    sns.barplot(data=sub, x=\"optimizer\", y=\"mean_f1_weighted\", order=order, ax=axes[1], palette=\"tab10\")\n",
    "    axes[1].set_title(f\"{ds} - F1 (weighted)\")\n",
    "    axes[1].set_xlabel(\"Optimizer\")\n",
    "    axes[1].set_ylabel(\"Mean F1\")\n",
    "    for p in axes[1].patches:\n",
    "        v = p.get_height()\n",
    "        axes[1].annotate(f\"{v:.3f}\", (p.get_x()+p.get_width()/2, v), ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "    fig.suptitle(f\"{ds} (classification)\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for ds in regression_datasets:\n",
    "    sub = final_summary[final_summary.dataset == ds]\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 3.2), sharey=False)\n",
    "    order = sorted(sub.optimizer.unique())\n",
    "\n",
    "    # MSE\n",
    "    sns.barplot(data=sub, x=\"optimizer\", y=\"mean_mse\", order=order, ax=axes[0], palette=\"tab10\")\n",
    "    axes[0].set_title(f\"{ds} - MSE\")\n",
    "    axes[0].set_xlabel(\"Optimizer\")\n",
    "    axes[0].set_ylabel(\"Mean MSE\")\n",
    "    for p in axes[0].patches:\n",
    "        v = p.get_height()\n",
    "        axes[0].annotate(f\"{v:.3g}\", (p.get_x()+p.get_width()/2, v), ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "    # R2\n",
    "    sns.barplot(data=sub, x=\"optimizer\", y=\"mean_r2\", order=order, ax=axes[1], palette=\"tab10\")\n",
    "    axes[1].set_title(f\"{ds} - R²\")\n",
    "    axes[1].set_xlabel(\"Optimizer\")\n",
    "    axes[1].set_ylabel(\"Mean R²\")\n",
    "    for p in axes[1].patches:\n",
    "        v = p.get_height()\n",
    "        axes[1].annotate(f\"{v:.3f}\", (p.get_x()+p.get_width()/2, v), ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# OPTIONAL: Combined overview (multi-panel)\n",
    "all_class_sub = final_summary[final_summary.dataset.isin(classification_datasets)]\n",
    "all_reg_sub = final_summary[final_summary.dataset.isin(regression_datasets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'final_summary' not in globals():\n",
    "\n",
    "    latest = sorted(\n",
    "        [p for p in os.listdir(\"results\") if p.startswith(\"final_experiments_summary_\")],\n",
    "        reverse=True\n",
    "    )[0]\n",
    "    final_summary = pd.read_csv(os.path.join(\"results\", latest))\n",
    "\n",
    "classification_datasets = [\"iris\", \"stroke\", \"mnist\"]\n",
    "clf_df = final_summary[final_summary.dataset.isin(classification_datasets)].copy()\n",
    "\n",
    "\n",
    "pivot_table = (clf_df\n",
    "               .pivot_table(index=\"dataset\",\n",
    "                            columns=\"optimizer\",\n",
    "                            values=[\"mean_accuracy\",\"mean_f1_weighted\"])\n",
    "               .round(4)\n",
    "              )\n",
    "\n",
    "print(\"Pivot table (mean metrics):\")\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25241856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence_grid(\n",
    "    runs,\n",
    "    datasets_order=None,\n",
    "    optimizers=(\"sgd\",\"scg\",\"lfrog\"),\n",
    "    curve_key=\"test_loss_curve\",\n",
    "    burn_in=1,\n",
    "    smooth_window=3,\n",
    "    normalize=False,\n",
    "    logy=True,\n",
    "    cap_percentile=99,\n",
    "    ncols=3,\n",
    "    figsize_per_panel=(4.0,3.2),\n",
    "    sharey=False,\n",
    "    annotate=True,\n",
    "    palette=None\n",
    "):\n",
    "    if datasets_order is None:\n",
    "        datasets_order = sorted({r[\"dataset\"] for r in runs})\n",
    "    if palette is None:\n",
    "        palette = {\"sgd\":\"#1f77b4\",\"scg\":\"#2ca02c\",\"lfrog\":\"#d62728\"}\n",
    "\n",
    "    agg = {}\n",
    "    for ds in datasets_order:\n",
    "        for opt in optimizers:\n",
    "            ds_runs = [r for r in runs if r[\"dataset\"]==ds and r[\"optimizer\"]==opt and curve_key in r]\n",
    "            if not ds_runs:\n",
    "                continue\n",
    "            curves = []\n",
    "            for r in ds_runs:\n",
    "                c = r[curve_key]\n",
    "                if burn_in > 0 and len(c) > burn_in:\n",
    "                    c = c[burn_in:]\n",
    "                if normalize and c and c[0] != 0:\n",
    "                    c = [x / c[0] for x in c]\n",
    "                if smooth_window and smooth_window > 1 and len(c) >= smooth_window:\n",
    "                    c = [np.mean(c[max(0,i-smooth_window+1):i+1]) for i in range(len(c))]\n",
    "                curves.append(c)\n",
    "            if not curves:\n",
    "                continue\n",
    "            max_len = max(len(c) for c in curves)\n",
    "            arr = np.full((len(curves), max_len), np.nan)\n",
    "            for i,c in enumerate(curves):\n",
    "                arr[i,:len(c)] = c\n",
    "            mean = np.nanmean(arr, axis=0)\n",
    "            std  = np.nanstd(arr, axis=0)\n",
    "            if cap_percentile:\n",
    "                cap_val = np.nanpercentile(mean, cap_percentile)\n",
    "                mean = np.minimum(mean, cap_val)\n",
    "                std  = np.minimum(std, cap_val)\n",
    "            agg[(ds,opt)] = (mean, std)\n",
    "\n",
    "    n = len(datasets_order)\n",
    "    nrows = int(np.ceil(n / ncols))\n",
    "    fig_w = ncols * figsize_per_panel[0]\n",
    "    fig_h = nrows * figsize_per_panel[1]\n",
    "    fig, axes = plt.subplots(nrows, ncols,\n",
    "                             figsize=(fig_w, fig_h),\n",
    "                             sharey=sharey,\n",
    "                             squeeze=False)\n",
    "    axes_flat = axes.flatten()\n",
    "\n",
    "    handles = {}\n",
    "    for idx, ds in enumerate(datasets_order):\n",
    "        ax = axes_flat[idx]\n",
    "        for opt in optimizers:\n",
    "            key = (ds,opt)\n",
    "            if key not in agg:\n",
    "                continue\n",
    "            mean, std = agg[key]\n",
    "            x = np.arange(len(mean))\n",
    "            h = ax.plot(x, mean, label=opt, color=palette.get(opt, None), linewidth=1.6)[0]\n",
    "            ax.fill_between(x, mean-std, mean+std, color=h.get_color(), alpha=0.18, linewidth=0)\n",
    "            handles[opt] = h\n",
    "            if annotate:\n",
    "                ax.text(0.98, 0.02,\n",
    "                        f\"{opt}: {mean[-1]:.3g}\",\n",
    "                        transform=ax.transAxes,\n",
    "                        ha=\"right\", va=\"bottom\",\n",
    "                        fontsize=8,\n",
    "                        color=h.get_color())\n",
    "        ax.set_title(ds)\n",
    "        ax.set_xlabel(\"Step\")\n",
    "        if idx % ncols == 0:\n",
    "            ax.set_ylabel(\"Normalized Loss\" if normalize else \"Test Loss\")\n",
    "        if logy:\n",
    "            ax.set_yscale(\"log\")\n",
    "        ax.grid(alpha=0.25, linewidth=0.5)\n",
    "\n",
    "    # Hide unused axes\n",
    "    for j in range(n, len(axes_flat)):\n",
    "        axes_flat[j].set_visible(False)\n",
    "\n",
    "    # Single legend\n",
    "    fig.legend([handles[o] for o in optimizers if o in handles],\n",
    "               [o.upper() for o in optimizers if o in handles],\n",
    "               loc=\"upper center\", ncol=len(handles),\n",
    "               frameon=False, fontsize=10, bbox_to_anchor=(0.5, 1.02))\n",
    "\n",
    "    fig.tight_layout(rect=[0,0,1,0.95])\n",
    "    suptitle = \"Convergence Curves (Mean ± Std)\"\n",
    "    if normalize: suptitle += \" - Normalized\"\n",
    "    fig.suptitle(suptitle, y=0.995, fontsize=13)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "plot_convergence_grid(\n",
    "    runs,\n",
    "    datasets_order=[\"iris\",\"stroke\",\"mnist\",\"sine\",\"wine\",\"california\"],  \n",
    "    burn_in=1,\n",
    "    smooth_window=3,\n",
    "    normalize=False,   \n",
    "    logy=True,\n",
    "    cap_percentile=99,\n",
    "    ncols=3,\n",
    "    sharey=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee47e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_datasets = [d for d in final_summary.dataset.unique() if d not in [\"iris\",\"stroke\",\"mnist\"]]\n",
    "\n",
    "reg = final_summary[final_summary.dataset.isin(regression_datasets)].copy()\n",
    "\n",
    "reg_table = (reg\n",
    "    .pivot(index=\"dataset\", columns=\"optimizer\", values=[\"mean_mse\",\"mean_r2\"])\n",
    "    .round(4)\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "display(reg_table)\n",
    "\n",
    "# Export to LaTeX\n",
    "latex_reg = reg_table.to_latex(\n",
    "    multicolumn=True,\n",
    "    multirow=False,\n",
    "    caption=\"Regression performance (mean MSE and R² over runs).\",\n",
    "    label=\"tab:reg_perf\"\n",
    ")\n",
    "print(latex_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5418251b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
