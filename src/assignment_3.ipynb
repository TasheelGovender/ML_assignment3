{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cab3a8a",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf4ce5",
   "metadata": {},
   "source": [
    "This project compares three feedforward neural network training algorithms: Stochastic Gradient Descent (SGD), Scaled Conjugate Gradient (SCG), and LeapFrog. Using six datasets—three for classification and three for regression—the study evaluates convergence speed, stability, and predictive accuracy. Each network has a single hidden layer, with experiments across different hidden layer sizes and hyperparameters. Performance is measured using accuracy and F1-score for classification, and MSE, RMSE, and R² for regression, alongside training time and convergence behavior. The results highlight the strengths and weaknesses of each optimizer across problems of varying complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e30bd8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d9f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "72ddc923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.datasets import load_iris, fetch_california_housing, fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import List, Tuple, Optional, Callable\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import json\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "import time\n",
    "import math\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994246ee",
   "metadata": {},
   "source": [
    "## Data and Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a93ca2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preprocessing_pipeline(X, classification=True):\n",
    "    \"\"\"\n",
    "    Preprocessing pipeline for tabular datasets (Iris, Stroke, Wine, Sine).\n",
    "    Skips MNIST and California housing (use custom pipelines for them).\n",
    "    \"\"\"\n",
    "    # Identify feature types\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    # Numeric: impute missing values, then scale\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # Categorical: impute, then one-hot encode (drop='first' avoids dummy trap)\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Combine transformers\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5bc1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mnist(X, y, test_size=0.2, random_state=42):\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X_norm = X / 255.0\n",
    "\n",
    "    # Flatten images (if not already flat)\n",
    "    if len(X_norm.shape) > 2:\n",
    "        X_flat = X_norm.reshape(X_norm.shape[0], -1)\n",
    "    else:\n",
    "        X_flat = X_norm\n",
    "\n",
    "    # Encode target labels\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    # Stratified split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_flat, y_encoded, test_size=test_size, random_state=random_state, stratify=y_encoded\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2af212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_california_housing(X, y, skewed_features=['MedInc'], log_target=True, test_size=0.2, random_state=42):\n",
    "    # Log-transform skewed features\n",
    "    X_processed = X.copy()\n",
    "    for col in skewed_features:\n",
    "        X_processed[col] = np.log1p(X_processed[col])\n",
    "\n",
    "    # Log-transform target if needed\n",
    "    if log_target:\n",
    "        y_processed = np.log1p(y)\n",
    "    else:\n",
    "        y_processed = y.copy()\n",
    "\n",
    "    # Check for NaNs after transformation\n",
    "    if X_processed.isnull().any().any():\n",
    "        print(\"Warning: NaNs found in features after log transformation.\")\n",
    "    if pd.isnull(y_processed).any():\n",
    "        print(\"Warning: NaNs found in target after log transformation.\")\n",
    "\n",
    "    # Build and fit pipeline\n",
    "    preprocessor = build_preprocessing_pipeline(X_processed, classification=False)\n",
    "    X_scaled = preprocessor.fit_transform(X_processed)\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_processed, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "9b68706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train, val, test, scale=True, impute=False, classification=True):\n",
    "    \"\"\"\n",
    "    Fit preprocessing on train, apply to val/test.\n",
    "    scale: whether to standardize numeric features\n",
    "    impute: whether to impute missing values\n",
    "    classification: whether the task is classification (affects pipeline)\n",
    "    \"\"\"\n",
    "    X_train, y_train = train\n",
    "    X_val, y_val = val\n",
    "    X_test, y_test = test\n",
    "\n",
    "    # Build and fit pipeline on training data\n",
    "    preprocessor = build_preprocessing_pipeline(X_train, classification=classification)\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_val_processed = preprocessor.transform(X_val)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "    if classification:\n",
    "        le = LabelEncoder()\n",
    "        y_train = le.fit_transform(y_train)\n",
    "        y_val = le.transform(y_val)\n",
    "        y_test = le.transform(y_test)\n",
    "\n",
    "    return X_train_processed, X_val_processed, X_test_processed, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "b5f363bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val_test(X, y, test_size=0.15, val_fraction_of_total=0.15,\n",
    "                         random_state=42, stratify=None):\n",
    "    \"\"\"\n",
    "    Split dataset into train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    # First split: train_val vs test\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=stratify\n",
    "    )\n",
    "\n",
    "    # Compute val_size relative to train_val\n",
    "    val_size = val_fraction_of_total / (1 - test_size)\n",
    "\n",
    "    # Second split: train vs val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=val_size,\n",
    "        random_state=random_state, stratify=y_train_val if stratify is not None else None\n",
    "    )\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8979ee4b",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52652ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris Dataset\n",
    "def load_iris_data():\n",
    "    iris = sns.load_dataset(\"iris\")\n",
    "    print(iris.head())\n",
    "\n",
    "    X_iris = iris.drop(\"species\", axis=1)\n",
    "    y_iris = iris[\"species\"]\n",
    "    print(X_iris.head())\n",
    "    print(y_iris.head())\n",
    "    return X_iris, y_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e022a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stroke Prediction Dataset\n",
    "def load_stroke_data():\n",
    "    stroke_data = pd.read_csv(\"../data/healthcare-dataset-stroke-data.csv\")\n",
    "    print(stroke_data.shape)\n",
    "    X_stroke = stroke_data.drop(\"stroke\", axis=1)\n",
    "    y_stroke = stroke_data[\"stroke\"]\n",
    "    print(X_stroke.head())\n",
    "    print(y_stroke.head())\n",
    "    return X_stroke, y_stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61848e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset\n",
    "def load_mnist_subset(n_samples=10000, random_state=42):\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    X = pd.DataFrame(mnist.data)\n",
    "    y = pd.Series(mnist.target, name=\"digit\")\n",
    "    # Sample 10k rows\n",
    "    X_mnist = X.sample(n=n_samples, random_state=random_state)\n",
    "    y_mnist = y.loc[X_mnist.index]\n",
    "    print(X_mnist.shape)\n",
    "    print(X_mnist.head())\n",
    "    print(y_mnist.head())\n",
    "    return X_mnist, y_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0d742d",
   "metadata": {},
   "source": [
    "### Function approx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1a7d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Sine Wave Dataset\n",
    "def generate_sine_wave_data(num_samples: int = 1000, noise_level: float = 0.1, random_state: int = 42) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    X = np.linspace(0, 2 * np.pi, num_samples)\n",
    "    y = np.sin(X) + noise_level * rng.standard_normal(num_samples)\n",
    "    X_sine = pd.DataFrame(X, columns=[\"x\"])\n",
    "    y_sine = pd.Series(y, name=\"y\")\n",
    "    return X_sine, y_sine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ee2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wine quality dataset from UCI ML Repository\n",
    "def load_wine_quality_data():\n",
    "    wine_quality = fetch_ucirepo(id=186) \n",
    "    \n",
    "    # data (as pandas dataframes) \n",
    "    X_wine = wine_quality.data.features \n",
    "    y_wine = wine_quality.data.targets \n",
    "    print(wine_quality.metadata)\n",
    "    print(wine_quality.variables)\n",
    "    print(X_wine.head())\n",
    "    print(y_wine.head())\n",
    "    print (X_wine.shape, y_wine.shape)\n",
    "    return X_wine, y_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce16cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# California Housing Dataset\n",
    "def load_california_housing_data(n_samples=10000, random_state=42):\n",
    "    california = fetch_california_housing()\n",
    "    X = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "    y = pd.Series(california.target, name=\"MedHouseVal\")\n",
    "    # Sample 10k rows\n",
    "    X_california = X.sample(n=n_samples, random_state=random_state)\n",
    "    y_california = y.loc[X_california.index]\n",
    "    print(X_california.shape)\n",
    "    print(X_california.head())\n",
    "    print(y_california.head())\n",
    "    return X_california, y_california"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11210285",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a2018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, activation_fn=nn.ReLU):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
    "        self.activation = activation_fn()\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd77059e",
   "metadata": {},
   "source": [
    "### Training Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "ffd22c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sgd(model, X_train, y_train, X_test, y_test, epochs=50, lr=0.01, batch_size=32, classification=True, momentum=0.0):\n",
    "    criterion = nn.CrossEntropyLoss() if classification else nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    if classification:\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    else:\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "        train_loss = running_loss / len(loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_test_tensor)\n",
    "            loss = criterion(outputs, y_test_tensor)\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b7052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage of SGD - function approximation\n",
    "# X_sine, y_sine = generate_sine_wave_data()\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(X_sine, y_sine, classification=False)\n",
    "# model = FeedforwardNN(input_dim=1, hidden_dim=16, output_dim=1)\n",
    "# train_losses, test_losses = train_sgd(\n",
    "#     model,\n",
    "#     X_train_scaled,\n",
    "#     np.array(y_train).reshape(-1, 1).astype(np.float32),\n",
    "#     X_test_scaled,\n",
    "#     np.array(y_test).reshape(-1, 1).astype(np.float32),\n",
    "#     classification=False\n",
    "# )\n",
    "# print(\"Train Losses:\", train_losses)\n",
    "# print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ded0309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage of SGD - Classification\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(X_iris, y_iris, classification=True)\n",
    "# model = FeedforwardNN(input_dim=4, hidden_dim=16, output_dim=3)\n",
    "# train_losses, test_losses = train_sgd(model, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "# print(\"Train Losses:\", train_losses)\n",
    "# print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "48aa3547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_scg(\n",
    "    model: nn.Module,\n",
    "    X_train: torch.Tensor,\n",
    "    y_train: torch.Tensor,\n",
    "    X_test: torch.Tensor,\n",
    "    y_test: torch.Tensor,\n",
    "    max_epochs: int = 1000,\n",
    "    tolerance: float = 1e-6,\n",
    "    sigma: float = 5e-5,\n",
    "    lambda_init: float = 5e-7,\n",
    "    verbose: bool = True,\n",
    "    eval_freq: int = 10,\n",
    "    grad_clip: float = 5.0,\n",
    "    max_alpha: float = 1.0\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    device = next(model.parameters()).device\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "    X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "    # Task detection (robust)\n",
    "    if y_train.dtype in (torch.long, torch.int64) and y_train.ndim == 1:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        task_type = 'classification'\n",
    "    else:\n",
    "        criterion = nn.MSELoss()\n",
    "        task_type = 'regression'\n",
    "\n",
    "    def get_weights():\n",
    "        return torch.cat([p.view(-1) for p in model.parameters()])\n",
    "\n",
    "    def set_weights(w):\n",
    "        idx = 0\n",
    "        for p in model.parameters():\n",
    "            n = p.numel()\n",
    "            p.data = w[idx:idx+n].view(p.shape)\n",
    "            idx += n\n",
    "\n",
    "    def loss_and_grad(w):\n",
    "        set_weights(w)\n",
    "        model.zero_grad()\n",
    "        out = model(X_train)\n",
    "        loss = criterion(out, y_train)\n",
    "        loss.backward()\n",
    "        g = torch.cat([p.grad.view(-1) for p in model.parameters()])\n",
    "        return loss.item(), g\n",
    "\n",
    "    def eval_split():\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tr = criterion(model(X_train), y_train).item()\n",
    "            te = criterion(model(X_test), y_test).item()\n",
    "        model.train()\n",
    "        return tr, te\n",
    "\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    w_k = get_weights()\n",
    "    f_k, g_k = loss_and_grad(w_k)\n",
    "    r_k = g_k.clone()\n",
    "    r_k_prev = None\n",
    "    p_k = -r_k.clone()\n",
    "    lambda_k = lambda_init\n",
    "    success = True\n",
    "    k_iter = 0\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    tr0, te0 = eval_split()\n",
    "    train_losses.append(tr0); test_losses.append(te0)\n",
    "    if verbose:\n",
    "        print(f\"Initial - Train {tr0:.6f} Test {te0:.6f}\")\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        if success:\n",
    "            denom = torch.dot(p_k, p_k)\n",
    "            if denom.abs() < 1e-18:\n",
    "                if verbose: print(\"Direction norm too small; stopping.\")\n",
    "                break\n",
    "            sigma_k = sigma / torch.sqrt(denom)\n",
    "\n",
    "        # Finite difference Hessian-vector approx\n",
    "        w_temp = w_k + sigma_k * p_k\n",
    "        _, g_temp = loss_and_grad(w_temp)\n",
    "        s_k = (g_temp - g_k) / sigma_k\n",
    "\n",
    "        delta_k = torch.dot(p_k, s_k)\n",
    "        if delta_k <= 0:\n",
    "            s_k = s_k + (lambda_k - delta_k) * p_k\n",
    "            delta_k = lambda_k * torch.dot(p_k, p_k)\n",
    "            lambda_k *= 2\n",
    "\n",
    "        mu_k = torch.dot(p_k, r_k)\n",
    "        alpha_k = -mu_k / (delta_k + 1e-12)\n",
    "\n",
    "        if abs(alpha_k) > max_alpha:\n",
    "            alpha_k = torch.clamp(alpha_k, -max_alpha, max_alpha)\n",
    "\n",
    "        Delta_k = -(alpha_k * mu_k + 0.5 * alpha_k * alpha_k * delta_k)\n",
    "\n",
    "        w_new = w_k + alpha_k * p_k\n",
    "        f_new, g_new = loss_and_grad(w_new)\n",
    "\n",
    "        # NaN / Inf guard\n",
    "        if not torch.isfinite(torch.tensor(f_new)):\n",
    "            if verbose: print(f\"Non-finite loss at epoch {epoch}; aborting this run.\")\n",
    "            break\n",
    "\n",
    "        if Delta_k <= 0:\n",
    "            r_ratio = -float('inf')\n",
    "        else:\n",
    "            r_ratio = (f_k - f_new) / Delta_k\n",
    "\n",
    "        if r_ratio > 0:\n",
    "            success = True\n",
    "            w_k = w_new\n",
    "            f_k = f_new\n",
    "            r_k_prev = r_k.clone()\n",
    "            g_k = g_new\n",
    "            r_k = g_k.clone()\n",
    "\n",
    "            if grad_clip is not None:\n",
    "                g_norm = torch.norm(r_k)\n",
    "                if g_norm > grad_clip:\n",
    "                    r_k.mul_(grad_clip / (g_norm + 1e-12))\n",
    "                    g_k = r_k  # keep consistent\n",
    "\n",
    "            if r_ratio > 0.75:\n",
    "                lambda_k /= 2\n",
    "            elif r_ratio < 0.25:\n",
    "                lambda_k *= 2\n",
    "        else:\n",
    "            success = False\n",
    "            lambda_k *= 2  # increase damping\n",
    "\n",
    "        if success:\n",
    "            if k_iter % n_params == 0 or r_k_prev is None:\n",
    "                p_k = -r_k.clone()\n",
    "            else:\n",
    "                beta_k = torch.dot(r_k, r_k - r_k_prev) / (torch.dot(r_k_prev, r_k_prev) + 1e-12)\n",
    "                p_k = -r_k + beta_k * p_k\n",
    "            k_iter += 1\n",
    "\n",
    "            grad_norm = torch.norm(r_k).item()\n",
    "            if grad_norm < tolerance:\n",
    "                if verbose: print(f\"Converged at epoch {epoch} ||g||={grad_norm:.2e}\")\n",
    "                break\n",
    "\n",
    "        if epoch % eval_freq == 0 or epoch == max_epochs - 1:\n",
    "            tr, te = eval_split()\n",
    "            train_losses.append(tr)\n",
    "            test_losses.append(te)\n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch:4d} f={tr:.6f} test={te:.6f} ||g||={torch.norm(r_k):.2e} r={r_ratio:.3f} λ={lambda_k:.2e} α={float(alpha_k):.3e}\")\n",
    "\n",
    "    if verbose and len(train_losses):\n",
    "        print(f\"Final - Train {train_losses[-1]:.6f} Test {test_losses[-1]:.6f}\")\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a675d",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "*For classification tasks, the output layer is linear and we use `CrossEntropyLoss`, which applies the required softmax internally. For regression tasks, the output layer is also linear, and we use `MSELoss`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de23cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage of SCG - classification\n",
    "# model = FeedforwardNN(input_dim=4, hidden_dim=16, output_dim=3)\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(X, y, classification=True)\n",
    "\n",
    "# X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "# X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# train_losses, test_losses = train_scg(\n",
    "#     model=model,\n",
    "#     X_train=X_train_tensor,\n",
    "#     y_train=y_train_tensor,\n",
    "#     X_test=X_test_tensor,\n",
    "#     y_test=y_test_tensor,\n",
    "#     max_epochs=500,\n",
    "#     tolerance=1e-5,\n",
    "#     verbose=True,\n",
    "#     eval_freq=1\n",
    "# )\n",
    "# print(\"Train Losses:\", train_losses)\n",
    "# print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02672718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage of SCG - function approximation\n",
    "# model = FeedforwardNN(input_dim=1, hidden_dim=16, output_dim=1)\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(X_sine, y_sine, classification=False)\n",
    "# X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "# X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "# train_losses, test_losses = train_scg(\n",
    "#     model=model,\n",
    "#     X_train=X_train_tensor,\n",
    "#     y_train=y_train_tensor,\n",
    "#     X_test=X_test_tensor,\n",
    "#     y_test=y_test_tensor,\n",
    "#     max_epochs=500,\n",
    "#     tolerance=1e-5,\n",
    "#     verbose=True,\n",
    "#     eval_freq=1\n",
    "# )\n",
    "# print(\"Train Losses:\", train_losses)\n",
    "# print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14362933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lfrog(\n",
    "    model: nn.Module,\n",
    "    X_train: torch.Tensor,\n",
    "    y_train: torch.Tensor,\n",
    "    X_test: torch.Tensor,\n",
    "    y_test: torch.Tensor,\n",
    "    loss_fn: Optional[Callable] = None,\n",
    "    epochs: int = 1000,\n",
    "    dt: float = 0.5,\n",
    "    max_step: float = 1.0,\n",
    "    convergence_tol: float = 1e-5,\n",
    "    max_consecutive_decreases: int = 2,\n",
    "    time_step_reduction_threshold: int = 3,\n",
    "    time_step_increase_factor: float = 0.001,\n",
    "    batch_size: Optional[int] = None,\n",
    "    device: str = 'cpu',\n",
    "    print_every: int = 100,\n",
    "    early_stopping_patience: int = 50,\n",
    "    min_improvement: float = 1e-6\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train a neural network using Snyman's LeapFrog dynamic optimization algorithm.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch neural network model\n",
    "        X_train, y_train: Training data and labels\n",
    "        X_test, y_test: Test data and labels\n",
    "        loss_fn: Loss function (default: MSE for regression, CrossEntropy for classification)\n",
    "        epochs: Maximum number of iterations\n",
    "        dt: Initial time step\n",
    "        max_step: Maximum allowable step size (δ in the paper)\n",
    "        convergence_tol: Convergence tolerance for gradient norm\n",
    "        max_consecutive_decreases: j parameter - max consecutive velocity decreases before reset\n",
    "        time_step_reduction_threshold: m parameter - consecutive steps before time step reduction\n",
    "        time_step_increase_factor: δ₁ parameter for time step increase\n",
    "        batch_size: Batch size (None for full batch)\n",
    "        device: Device to run on\n",
    "        print_every: Print progress every N steps\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_losses, test_losses) lists\n",
    "    \"\"\"\n",
    "    \n",
    "    # Move model and data to device\n",
    "    model = model.to(device)\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "    X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "    \n",
    "    # Auto-detect loss function if not provided\n",
    "    if loss_fn is None:\n",
    "        if len(y_train.shape) == 1 or y_train.shape[1] == 1:\n",
    "            if torch.all((y_train == 0) | (y_train == 1)):\n",
    "                loss_fn = nn.BCEWithLogitsLoss()\n",
    "            else:\n",
    "                loss_fn = nn.MSELoss()\n",
    "        else:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_test_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Get initial parameters as flat vector\n",
    "    params = []\n",
    "    for p in model.parameters():\n",
    "        params.append(p.view(-1))\n",
    "    x_k = torch.cat(params)\n",
    "    n_params = len(x_k)\n",
    "    \n",
    "    # Compute initial gradient and velocity\n",
    "    def compute_loss_and_grad():\n",
    "        model.zero_grad()\n",
    "        if batch_size is None:\n",
    "            outputs = model(X_train)\n",
    "            loss = loss_fn(outputs, y_train)\n",
    "        else:\n",
    "            # Mini-batch gradient\n",
    "            idx = torch.randperm(len(X_train))[:batch_size]\n",
    "            outputs = model(X_train[idx])\n",
    "            loss = loss_fn(outputs, y_train[idx])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Extract gradients as flat vector\n",
    "        grads = []\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                grads.append(p.grad.view(-1))\n",
    "            else:\n",
    "                grads.append(torch.zeros_like(p.view(-1)))\n",
    "        grad = torch.cat(grads)\n",
    "        \n",
    "        return loss.item(), grad\n",
    "    \n",
    "    def update_model_params(x):\n",
    "        \"\"\"Update model parameters from flat parameter vector\"\"\"\n",
    "        idx = 0\n",
    "        for p in model.parameters():\n",
    "            param_size = p.numel()\n",
    "            p.data = x[idx:idx + param_size].view(p.shape)\n",
    "            idx += param_size\n",
    "    \n",
    "    def evaluate_test():\n",
    "        \"\"\"Evaluate model on test set\"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test)\n",
    "            test_loss = loss_fn(test_outputs, y_test)\n",
    "        model.train()\n",
    "        return test_loss.item()\n",
    "    \n",
    "    # Initialize algorithm variables\n",
    "    train_loss, grad_k = compute_loss_and_grad()\n",
    "    v_k = -0.5 * grad_k * dt  # Initial velocity\n",
    "    \n",
    "    # Algorithm state variables\n",
    "    consecutive_decreases = 0\n",
    "    consecutive_negative_dot_products = 0\n",
    "    successful_steps = 0\n",
    "    current_dt = dt\n",
    "    \n",
    "    print(f\"Initial loss: {train_loss:.6f}, Gradient norm: {torch.norm(grad_k):.6f}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Store current state\n",
    "        x_k_old = x_k.clone()\n",
    "        v_k_old = v_k.clone()\n",
    "        grad_k_old = grad_k.clone()\n",
    "        v_k_norm_old = torch.norm(v_k)\n",
    "        \n",
    "        # Step A: Compute step size and limit if necessary\n",
    "        step_size = torch.norm(v_k) * current_dt\n",
    "        if step_size > max_step:\n",
    "            v_k = max_step * v_k / step_size\n",
    "            step_size = max_step\n",
    "        \n",
    "        # Step B: Leap-frog integration\n",
    "        # Update position\n",
    "        x_k = x_k + v_k * current_dt\n",
    "        update_model_params(x_k)\n",
    "        \n",
    "        # Compute new gradient and update velocity\n",
    "        train_loss, grad_k = compute_loss_and_grad()\n",
    "        a_k = -grad_k  # acceleration (negative gradient)\n",
    "        v_k = v_k + a_k * current_dt\n",
    "        \n",
    "        # Record losses\n",
    "        test_loss = evaluate_test()\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if test_loss < best_test_loss - min_improvement:\n",
    "            best_test_loss = test_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping at epoch {epoch}: No improvement in test loss for {early_stopping_patience} epochs\")\n",
    "                print(f\"Best test loss: {best_test_loss:.6f}\")\n",
    "                break\n",
    "        \n",
    "        # Step C: Check convergence\n",
    "        grad_norm = torch.norm(grad_k)\n",
    "        if grad_norm < convergence_tol:\n",
    "            print(f\"Converged at epoch {epoch}: gradient norm {grad_norm:.2e}\")\n",
    "            break\n",
    "        \n",
    "        # Time step control - check for gradient direction changes AND velocity-gradient alignment\n",
    "        if epoch > 0:\n",
    "            dot_product = torch.dot(grad_k, grad_k_old)\n",
    "            velocity_gradient_dot = torch.dot(v_k, grad_k)\n",
    "            \n",
    "            if dot_product <= 0:\n",
    "                consecutive_negative_dot_products += 1\n",
    "            else:\n",
    "                consecutive_negative_dot_products = 0\n",
    "                \n",
    "            # Additional safeguard: if velocity is pointing uphill, this is concerning\n",
    "            if velocity_gradient_dot > 0:\n",
    "                consecutive_negative_dot_products += 1  # Treat as problematic\n",
    "        \n",
    "        # Time step reduction\n",
    "        if consecutive_negative_dot_products >= time_step_reduction_threshold:\n",
    "            current_dt = current_dt / 2\n",
    "            current_dt = max(current_dt, 1e-6)\n",
    "            x_k = (x_k + x_k_old) / 2\n",
    "            v_k = (v_k + v_k_old) / 4\n",
    "            update_model_params(x_k)\n",
    "            consecutive_negative_dot_products = 0\n",
    "            successful_steps = 0\n",
    "            if print_every > 0 and epoch % print_every == 0:\n",
    "                print(f\"Epoch {epoch}: Reduced time step to {current_dt:.6f}\")\n",
    "        \n",
    "        # Step D: Energy monitoring (kinetic energy check)\n",
    "        v_k_norm = torch.norm(v_k)\n",
    "        if v_k_norm > v_k_norm_old:\n",
    "            # Kinetic energy increased - successful step\n",
    "            consecutive_decreases = 0\n",
    "            if step_size < max_step:\n",
    "                successful_steps += 1\n",
    "                # Time step increase - cap growth for stability\n",
    "                growth_factor = min(1.01, 1 + successful_steps * time_step_increase_factor)\n",
    "                current_dt = growth_factor * current_dt\n",
    "                current_dt = max(current_dt, 1e-6)\n",
    "\n",
    "        else:\n",
    "            # Kinetic energy decreased - intervene\n",
    "            consecutive_decreases += 1\n",
    "            successful_steps = 0\n",
    "            \n",
    "            # Restart from midpoint\n",
    "            x_k = (x_k + x_k_old) / 2\n",
    "            update_model_params(x_k)\n",
    "            \n",
    "            if consecutive_decreases <= max_consecutive_decreases:\n",
    "                # Reduce velocity\n",
    "                v_k = (v_k + v_k_old) / 4\n",
    "            else:\n",
    "                # Reset velocity to zero\n",
    "                v_k = torch.zeros_like(v_k)\n",
    "                consecutive_decreases = 0\n",
    "        \n",
    "        # Print progress\n",
    "        if print_every > 0 and epoch % print_every == 0:\n",
    "            print(f\"Epoch {epoch}: Train Loss = {train_loss:.6f}, \"\n",
    "                  f\"Test Loss = {test_loss:.6f}, Grad Norm = {grad_norm:.6f}, \"\n",
    "                  f\"dt = {current_dt:.6f}\")\n",
    "    \n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34a1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage of LFROG - classification\n",
    "# model = FeedforwardNN(input_dim=4, hidden_dim=16, output_dim=3)\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(X, y, classification=True)\n",
    "\n",
    "# X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "# X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# train_losses, test_losses = train_lfrog(\n",
    "#     model=model,\n",
    "#     X_train=X_train_tensor,\n",
    "#     y_train=y_train_tensor,\n",
    "#     X_test=X_test_tensor,\n",
    "#     y_test=y_test_tensor,\n",
    "#     epochs=500,\n",
    "#     dt=0.005,\n",
    "#     max_step=0.05,\n",
    "#     early_stopping_patience=50,  # Stop if no improvement for 50 epochs\n",
    "#     min_improvement=1e-6,        # Minimum improvement threshold\n",
    "#     print_every=25,\n",
    "#     loss_fn=nn.CrossEntropyLoss()\n",
    "# )\n",
    "# print(\"Train Losses:\", train_losses)\n",
    "# print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e5fee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage of LFROG - function approximation\n",
    "# model = FeedforwardNN(input_dim=1, hidden_dim=16, output_dim=1)\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(X_sine, y_sine, classification=False)\n",
    "# X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "# X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)    \n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "# train_losses, test_losses = train_lfrog(\n",
    "#     model=model,\n",
    "#     X_train=X_train_tensor,\n",
    "#     y_train=y_train_tensor,\n",
    "#     X_test=X_test_tensor,\n",
    "#     y_test=y_test_tensor,   \n",
    "#     epochs=500,\n",
    "#     dt=0.005,\n",
    "#     max_step=0.05,\n",
    "#     early_stopping_patience=50,  # Stop if no improvement for 50 epochs\n",
    "#     min_improvement=1e-6,        # Minimum improvement threshold\n",
    "#     print_every=25,\n",
    "#     loss_fn=nn.MSELoss()\n",
    "# )\n",
    "# print(\"Train Losses:\", train_losses)\n",
    "# print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3075a1",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d83c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Iris classification with SGD\n",
    "\n",
    "# 1. Load data\n",
    "X_iris, y_iris = load_iris_data()\n",
    "\n",
    "# 2. Preprocess\n",
    "X_train, X_test, y_train, y_test = preprocess_data(X_iris, y_iris, classification=True)\n",
    "\n",
    "# 3. Initialize model\n",
    "model = FeedforwardNN(input_dim=X_train.shape[1], hidden_dim=16, output_dim=len(np.unique(y_train)))\n",
    "\n",
    "# 4. Train\n",
    "train_losses, test_losses = train_sgd(model, X_train, y_train, X_test, y_test, epochs=50, lr=0.01, batch_size=32, classification=True)\n",
    "\n",
    "# 5. Results\n",
    "print(\"Train Losses:\", train_losses)\n",
    "print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2735feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sine, y_sine = generate_sine_wave_data(num_samples=1000, noise_level=0.1)\n",
    "\n",
    "# Convert to DataFrame/Series for compatibility\n",
    "X_sine_df = pd.DataFrame(X_sine, columns=[\"x\"])\n",
    "y_sine_sr = pd.Series(y_sine.flatten(), name=\"y\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess_data(X_sine_df, y_sine_sr, classification=False)\n",
    "\n",
    "model = FeedforwardNN(input_dim=1, hidden_dim=16, output_dim=1)\n",
    "\n",
    "# Convert targets to numpy arrays for regression\n",
    "train_losses, test_losses = train_sgd(\n",
    "    model,\n",
    "    X_train,\n",
    "    np.array(y_train).reshape(-1, 1).astype(np.float32),\n",
    "    X_test,\n",
    "    np.array(y_test).reshape(-1, 1).astype(np.float32),\n",
    "    epochs=50,\n",
    "    lr=0.01,\n",
    "    batch_size=32,\n",
    "    classification=False\n",
    ")\n",
    "\n",
    "print(\"Train Losses:\", train_losses)\n",
    "print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed14b18",
   "metadata": {},
   "source": [
    "### Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "477cad1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width species\n",
      "0           5.1          3.5           1.4          0.2  setosa\n",
      "1           4.9          3.0           1.4          0.2  setosa\n",
      "2           4.7          3.2           1.3          0.2  setosa\n",
      "3           4.6          3.1           1.5          0.2  setosa\n",
      "4           5.0          3.6           1.4          0.2  setosa\n",
      "   sepal_length  sepal_width  petal_length  petal_width\n",
      "0           5.1          3.5           1.4          0.2\n",
      "1           4.9          3.0           1.4          0.2\n",
      "2           4.7          3.2           1.3          0.2\n",
      "3           4.6          3.1           1.5          0.2\n",
      "4           5.0          3.6           1.4          0.2\n",
      "0    setosa\n",
      "1    setosa\n",
      "2    setosa\n",
      "3    setosa\n",
      "4    setosa\n",
      "Name: species, dtype: object\n",
      "(5110, 12)\n",
      "      id  gender   age  hypertension  heart_disease ever_married  \\\n",
      "0   9046    Male  67.0             0              1          Yes   \n",
      "1  51676  Female  61.0             0              0          Yes   \n",
      "2  31112    Male  80.0             0              1          Yes   \n",
      "3  60182  Female  49.0             0              0          Yes   \n",
      "4   1665  Female  79.0             1              0          Yes   \n",
      "\n",
      "       work_type Residence_type  avg_glucose_level   bmi   smoking_status  \n",
      "0        Private          Urban             228.69  36.6  formerly smoked  \n",
      "1  Self-employed          Rural             202.21   NaN     never smoked  \n",
      "2        Private          Rural             105.92  32.5     never smoked  \n",
      "3        Private          Urban             171.23  34.4           smokes  \n",
      "4  Self-employed          Rural             174.12  24.0     never smoked  \n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: stroke, dtype: int64\n",
      "{'uci_id': 186, 'name': 'Wine Quality', 'repository_url': 'https://archive.ics.uci.edu/dataset/186/wine+quality', 'data_url': 'https://archive.ics.uci.edu/static/public/186/data.csv', 'abstract': 'Two datasets are included, related to red and white vinho verde wine samples, from the north of Portugal. The goal is to model wine quality based on physicochemical tests (see [Cortez et al., 2009], http://www3.dsi.uminho.pt/pcortez/wine/).', 'area': 'Business', 'tasks': ['Classification', 'Regression'], 'characteristics': ['Multivariate'], 'num_instances': 4898, 'num_features': 11, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['quality'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2009, 'last_updated': 'Wed Nov 15 2023', 'dataset_doi': '10.24432/C56S3T', 'creators': ['Paulo Cortez', 'A. Cerdeira', 'F. Almeida', 'T. Matos', 'J. Reis'], 'intro_paper': {'ID': 252, 'type': 'NATIVE', 'title': 'Modeling wine preferences by data mining from physicochemical properties', 'authors': 'P. Cortez, A. Cerdeira, Fernando Almeida, Telmo Matos, J. Reis', 'venue': 'Decision Support Systems', 'year': 2009, 'journal': None, 'DOI': None, 'URL': 'https://www.semanticscholar.org/paper/Modeling-wine-preferences-by-data-mining-from-Cortez-Cerdeira/bf15a0ccc14ac1deb5cea570c870389c16be019c', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'The two datasets are related to red and white variants of the Portuguese \"Vinho Verde\" wine. For more details, consult: http://www.vinhoverde.pt/en/ or the reference [Cortez et al., 2009].  Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).\\n\\nThese datasets can be viewed as classification or regression tasks.  The classes are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods.\\n', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'For more information, read [Cortez et al., 2009].\\r\\nInput variables (based on physicochemical tests):\\r\\n   1 - fixed acidity\\r\\n   2 - volatile acidity\\r\\n   3 - citric acid\\r\\n   4 - residual sugar\\r\\n   5 - chlorides\\r\\n   6 - free sulfur dioxide\\r\\n   7 - total sulfur dioxide\\r\\n   8 - density\\r\\n   9 - pH\\r\\n   10 - sulphates\\r\\n   11 - alcohol\\r\\nOutput variable (based on sensory data): \\r\\n   12 - quality (score between 0 and 10)', 'citation': None}}\n",
      "                    name     role         type demographic  \\\n",
      "0          fixed_acidity  Feature   Continuous        None   \n",
      "1       volatile_acidity  Feature   Continuous        None   \n",
      "2            citric_acid  Feature   Continuous        None   \n",
      "3         residual_sugar  Feature   Continuous        None   \n",
      "4              chlorides  Feature   Continuous        None   \n",
      "5    free_sulfur_dioxide  Feature   Continuous        None   \n",
      "6   total_sulfur_dioxide  Feature   Continuous        None   \n",
      "7                density  Feature   Continuous        None   \n",
      "8                     pH  Feature   Continuous        None   \n",
      "9              sulphates  Feature   Continuous        None   \n",
      "10               alcohol  Feature   Continuous        None   \n",
      "11               quality   Target      Integer        None   \n",
      "12                 color    Other  Categorical        None   \n",
      "\n",
      "               description units missing_values  \n",
      "0                     None  None             no  \n",
      "1                     None  None             no  \n",
      "2                     None  None             no  \n",
      "3                     None  None             no  \n",
      "4                     None  None             no  \n",
      "5                     None  None             no  \n",
      "6                     None  None             no  \n",
      "7                     None  None             no  \n",
      "8                     None  None             no  \n",
      "9                     None  None             no  \n",
      "10                    None  None             no  \n",
      "11  score between 0 and 10  None             no  \n",
      "12            red or white  None             no  \n",
      "   fixed_acidity  volatile_acidity  citric_acid  residual_sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free_sulfur_dioxide  total_sulfur_dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  \n",
      "0      9.4  \n",
      "1      9.8  \n",
      "2      9.8  \n",
      "3      9.8  \n",
      "4      9.4  \n",
      "   quality\n",
      "0        5\n",
      "1        5\n",
      "2        5\n",
      "3        6\n",
      "4        5\n",
      "(6497, 11) (6497, 1)\n",
      "(10000, 8)\n",
      "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "19121  3.7917      40.0  4.959799   1.030151      1039.0  2.610553     38.24   \n",
      "20019  4.0217       9.0  5.804577   1.000000      1749.0  3.079225     36.09   \n",
      "15104  4.0882      12.0  5.360360   1.070571      3321.0  4.986486     32.85   \n",
      "3720   2.2377      27.0  3.376582   1.023207      3403.0  3.589662     34.20   \n",
      "8938   4.4211      41.0  5.656904   1.165272      1047.0  2.190377     34.01   \n",
      "\n",
      "       Longitude  \n",
      "19121    -122.64  \n",
      "20019    -119.05  \n",
      "15104    -116.98  \n",
      "3720     -118.42  \n",
      "8938     -118.47  \n",
      "19121    1.516\n",
      "20019    0.992\n",
      "15104    1.345\n",
      "3720     2.317\n",
      "8938     4.629\n",
      "Name: MedHouseVal, dtype: float64\n",
      "(10000, 784)\n",
      "       pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
      "21971       0       0       0       0       0       0       0       0       0   \n",
      "16953       0       0       0       0       0       0       0       0       0   \n",
      "48447       0       0       0       0       0       0       0       0       0   \n",
      "42344       0       0       0       0       0       0       0       0       0   \n",
      "4376        0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "       pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
      "21971        0  ...         0         0         0         0         0   \n",
      "16953        0  ...         0         0         0         0         0   \n",
      "48447        0  ...         0         0         0         0         0   \n",
      "42344        0  ...         0         0         0         0         0   \n",
      "4376         0  ...         0         0         0         0         0   \n",
      "\n",
      "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
      "21971         0         0         0         0         0  \n",
      "16953         0         0         0         0         0  \n",
      "48447         0         0         0         0         0  \n",
      "42344         0         0         0         0         0  \n",
      "4376          0         0         0         0         0  \n",
      "\n",
      "[5 rows x 784 columns]\n",
      "21971    0\n",
      "16953    9\n",
      "48447    4\n",
      "42344    9\n",
      "4376     5\n",
      "Name: digit, dtype: category\n",
      "Categories (10, object): ['0', '1', '2', '3', ..., '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "split_seed = 123\n",
    "R = [42, 1337, 2025, 31415, 27182]\n",
    "datasets = {}\n",
    "\n",
    "# --- Iris (classification) ---\n",
    "X_iris, y_iris = load_iris_data()\n",
    "splits = split_train_val_test(X_iris, y_iris,\n",
    "                              test_size=0.15, val_fraction_of_total=0.15,\n",
    "                              random_state=split_seed, stratify=y_iris)\n",
    "X_train_iris, X_val_iris, X_test_iris, y_train_iris, y_val_iris, y_test_iris = preprocess_data(*splits, scale=True, impute=False, classification=True)\n",
    "datasets[\"iris\"] = {\n",
    "    \"train\": (X_train_iris, y_train_iris),\n",
    "    \"val\":   (X_val_iris, y_val_iris),\n",
    "    \"test\":  (X_test_iris, y_test_iris),\n",
    "}\n",
    "\n",
    "# --- Stroke Prediction (classification, needs imputation) ---\n",
    "X_stroke, y_stroke = load_stroke_data()\n",
    "splits = split_train_val_test(X_stroke, y_stroke,\n",
    "                              test_size=0.15, val_fraction_of_total=0.15,\n",
    "                              random_state=split_seed, stratify=y_stroke)\n",
    "X_train_stroke, X_val_stroke, X_test_stroke, y_train_stroke, y_val_stroke, y_test_stroke = preprocess_data(*splits, scale=True, impute=True, classification=True)\n",
    "datasets[\"stroke\"] = {\n",
    "    \"train\": (X_train_stroke, y_train_stroke),\n",
    "    \"val\":   (X_val_stroke, y_val_stroke),\n",
    "    \"test\":  (X_test_stroke, y_test_stroke),\n",
    "}\n",
    "\n",
    "# --- Sine Wave (regression, synthetic) ---\n",
    "X_sine, y_sine = generate_sine_wave_data(num_samples=1000, noise_level=0.1, random_state=split_seed)\n",
    "splits = split_train_val_test(X_sine, y_sine,\n",
    "                              test_size=0.15, val_fraction_of_total=0.15,\n",
    "                              random_state=split_seed, stratify=None)\n",
    "X_train_sine, X_val_sine, X_test_sine, y_train_sine, y_val_sine, y_test_sine = preprocess_data(*splits, scale=True, impute=False, classification=False)\n",
    "datasets[\"sine\"] = {\n",
    "    \"train\": (X_train_sine, y_train_sine),\n",
    "    \"val\":   (X_val_sine, y_val_sine),\n",
    "    \"test\":  (X_test_sine, y_test_sine),\n",
    "}\n",
    "\n",
    "# --- Wine Quality (regression) ---\n",
    "X_wine, y_wine = load_wine_quality_data()\n",
    "splits = split_train_val_test(X_wine, y_wine,\n",
    "                              test_size=0.15, val_fraction_of_total=0.15,\n",
    "                              random_state=split_seed, stratify=None)\n",
    "X_train_wine, X_val_wine, X_test_wine, y_train_wine, y_val_wine, y_test_wine = preprocess_data(*splits, scale=True, impute=False, classification=False)\n",
    "datasets[\"wine\"] = {\n",
    "    \"train\": (X_train_wine, y_train_wine),\n",
    "    \"val\":   (X_val_wine, y_val_wine),\n",
    "    \"test\":  (X_test_wine, y_test_wine),\n",
    "}\n",
    "\n",
    "# --- California Housing (regression) ---\n",
    "X_california, y_california = load_california_housing_data(n_samples=10000, random_state=split_seed)\n",
    "splits = split_train_val_test(X_california, y_california,\n",
    "                              test_size=0.15, val_fraction_of_total=0.15,\n",
    "                              random_state=split_seed, stratify=None)\n",
    "X_train_cal, X_val_cal, X_test_cal, y_train_cal, y_val_cal, y_test_cal = preprocess_data(*splits, scale=True, impute=False, classification=False)\n",
    "datasets[\"california\"] = {\n",
    "    \"train\": (X_train_cal, y_train_cal),\n",
    "    \"val\":   (X_val_cal, y_val_cal),\n",
    "    \"test\":  (X_test_cal, y_test_cal),\n",
    "}\n",
    "\n",
    "# --- MNIST (classification, pixel values 0–255 → scale to [0,1]) ---\n",
    "X_mnist, y_mnist = load_mnist_subset(n_samples=10000, random_state=split_seed)\n",
    "splits = split_train_val_test(X_mnist, y_mnist,\n",
    "                              test_size=0.15, val_fraction_of_total=0.15,\n",
    "                              random_state=split_seed, stratify=y_mnist)\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = splits\n",
    "X_train = np.array(X_train) / 255.0\n",
    "X_val   = np.array(X_val) / 255.0\n",
    "X_test  = np.array(X_test) / 255.0\n",
    "y_train = np.array(y_train).astype(int)\n",
    "y_val   = np.array(y_val).astype(int)\n",
    "y_test  = np.array(y_test).astype(int)\n",
    "datasets[\"mnist\"] = {\n",
    "    \"train\": (X_train, y_train),\n",
    "    \"val\":   (X_val, y_val),\n",
    "    \"test\":  (X_test, y_test),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1b96fe",
   "metadata": {},
   "source": [
    "### Baseline sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "92b4ba81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sine SGD baseline: Final train loss=0.0788, test loss=0.0762\n",
      "Sine SCG baseline: Final train loss=0.1503, test loss=0.1416\n",
      "Initial loss: 0.360470, Gradient norm: 0.502571\n",
      "Sine LeapFrog baseline: Final train loss=0.2942, test loss=0.2955\n",
      "Iris SGD baseline: Final train loss=0.6040, test loss=0.6381\n",
      "Iris SCG baseline: Final train loss=0.0515, test loss=0.0960\n",
      "Initial loss: 1.149910, Gradient norm: 0.417881\n",
      "Iris LeapFrog baseline: Final train loss=1.0972, test loss=1.0877\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 8  # Small value for hidden units\n",
    "\n",
    "# Sine dataset (regression)\n",
    "X_train_sine, y_train_sine = datasets[\"sine\"][\"train\"]\n",
    "X_test_sine, y_test_sine = datasets[\"sine\"][\"test\"]\n",
    "\n",
    "model = FeedforwardNN(input_dim=X_train_sine.shape[1], hidden_dim=hidden_dim, output_dim=1)\n",
    "torch.save(model.state_dict(), \"init_sine_baseline.pth\")\n",
    "\n",
    "# SGD\n",
    "train_losses, test_losses = train_sgd(\n",
    "    model,\n",
    "    X_train_sine,\n",
    "    np.array(y_train_sine).reshape(-1, 1).astype(np.float32),\n",
    "    X_test_sine,\n",
    "    np.array(y_test_sine).reshape(-1, 1).astype(np.float32),\n",
    "    epochs=100,\n",
    "    lr=0.01,\n",
    "    batch_size=32,\n",
    "    classification=False\n",
    ")\n",
    "print(f\"Sine SGD baseline: Final train loss={train_losses[-1]:.4f}, test loss={test_losses[-1]:.4f}\")\n",
    "\n",
    "# SCG\n",
    "model.load_state_dict(torch.load(\"init_sine_baseline.pth\"))\n",
    "X_train_tensor = torch.tensor(X_train_sine, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(np.array(y_train_sine).reshape(-1, 1).astype(np.float32))\n",
    "X_test_tensor = torch.tensor(X_test_sine, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(np.array(y_test_sine).reshape(-1, 1).astype(np.float32))\n",
    "train_losses, test_losses = train_scg(\n",
    "    model,\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    X_test_tensor,\n",
    "    y_test_tensor,\n",
    "    max_epochs=100,\n",
    "    tolerance=1e-5,\n",
    "    verbose=False,\n",
    "    eval_freq=1\n",
    ")\n",
    "print(f\"Sine SCG baseline: Final train loss={train_losses[-1]:.4f}, test loss={test_losses[-1]:.4f}\")\n",
    "\n",
    "# LeapFrog\n",
    "model.load_state_dict(torch.load(\"init_sine_baseline.pth\"))\n",
    "train_losses, test_losses = train_lfrog(\n",
    "    model=model,\n",
    "    X_train=X_train_tensor,\n",
    "    y_train=y_train_tensor,\n",
    "    X_test=X_test_tensor,\n",
    "    y_test=y_test_tensor,   \n",
    "    epochs=100,\n",
    "    dt=0.005,\n",
    "    max_step=0.05,\n",
    "    early_stopping_patience=50,  \n",
    "    min_improvement=1e-6,      \n",
    "    print_every=0,\n",
    "    loss_fn=nn.MSELoss()\n",
    ")\n",
    "print(f\"Sine LeapFrog baseline: Final train loss={train_losses[-1]:.4f}, test loss={test_losses[-1]:.4f}\")\n",
    "\n",
    "# Iris dataset (classification)\n",
    "X_train_iris, y_train_iris = datasets[\"iris\"][\"train\"]\n",
    "X_test_iris, y_test_iris = datasets[\"iris\"][\"test\"]\n",
    "output_dim_iris = len(np.unique(y_train_iris))\n",
    "\n",
    "model = FeedforwardNN(input_dim=X_train_iris.shape[1], hidden_dim=hidden_dim, output_dim=output_dim_iris)\n",
    "torch.save(model.state_dict(), \"init_iris_baseline.pth\")\n",
    "\n",
    "# SGD\n",
    "train_losses, test_losses = train_sgd(\n",
    "    model,\n",
    "    X_train_iris,\n",
    "    y_train_iris,\n",
    "    X_test_iris,\n",
    "    y_test_iris,\n",
    "    epochs=100,\n",
    "    lr=0.01,\n",
    "    batch_size=32,\n",
    "    classification=True\n",
    ")\n",
    "print(f\"Iris SGD baseline: Final train loss={train_losses[-1]:.4f}, test loss={test_losses[-1]:.4f}\")\n",
    "\n",
    "# SCG\n",
    "model.load_state_dict(torch.load(\"init_iris_baseline.pth\"))\n",
    "X_train_tensor = torch.tensor(X_train_iris, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_iris, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_iris, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_iris, dtype=torch.long)\n",
    "train_losses, test_losses = train_scg(\n",
    "    model,\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    X_test_tensor,\n",
    "    y_test_tensor,\n",
    "    max_epochs=100,\n",
    "    tolerance=1e-5,\n",
    "    verbose=False,\n",
    "    eval_freq=1\n",
    ")\n",
    "print(f\"Iris SCG baseline: Final train loss={train_losses[-1]:.4f}, test loss={test_losses[-1]:.4f}\")\n",
    "\n",
    "# LeapFrog\n",
    "model.load_state_dict(torch.load(\"init_iris_baseline.pth\"))\n",
    "train_losses, test_losses = train_lfrog(\n",
    "    model=model,\n",
    "    X_train=X_train_tensor,\n",
    "    y_train=y_train_tensor,\n",
    "    X_test=X_test_tensor,\n",
    "    y_test=y_test_tensor,\n",
    "    epochs=100,\n",
    "    dt=0.005,\n",
    "    max_step=0.05,\n",
    "    early_stopping_patience=50,  # Stop if no improvement for 50 epochs\n",
    "    min_improvement=1e-6,        # Minimum improvement threshold\n",
    "    print_every=0,\n",
    "    loss_fn=nn.CrossEntropyLoss()\n",
    ")\n",
    "print(f\"Iris LeapFrog baseline: Final train loss={train_losses[-1]:.4f}, test loss={test_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152ff200",
   "metadata": {},
   "source": [
    "### Optimal Hidden units search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "8c410659",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "n_runs = 3  \n",
    "\n",
    "def hidden_unit_search(dataset_name, classification=True):\n",
    "    X_train, y_train = datasets[dataset_name][\"train\"]\n",
    "    X_val, y_val = datasets[dataset_name][\"val\"]\n",
    "    output_dim = len(np.unique(y_train)) if classification else 1\n",
    "\n",
    "    results = []\n",
    "    for h in hidden_sizes:\n",
    "        val_metrics = []\n",
    "        accs = []\n",
    "        f1s = []\n",
    "        r2s = []\n",
    "        mses = []\n",
    "        for seed in R[:n_runs]:\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            model = FeedforwardNN(\n",
    "                input_dim=X_train.shape[1],\n",
    "                hidden_dim=h,\n",
    "                output_dim=output_dim\n",
    "            )\n",
    "            # For regression, ensure targets are float32 and shaped correctly\n",
    "            if classification:\n",
    "                y_train_run = y_train\n",
    "                y_val_run = y_val\n",
    "            else:\n",
    "                y_train_run = np.array(y_train).reshape(-1, 1).astype(np.float32)\n",
    "                y_val_run = np.array(y_val).reshape(-1, 1).astype(np.float32)\n",
    "            _, val_losses = train_sgd(\n",
    "                model,\n",
    "                X_train,\n",
    "                y_train_run,\n",
    "                X_val,\n",
    "                y_val_run,\n",
    "                epochs=100,\n",
    "                lr=0.01,\n",
    "                batch_size=32,\n",
    "                classification=classification\n",
    "            )\n",
    "            val_metrics.append(val_losses[-1])\n",
    "\n",
    "            # --- Calculate additional metrics ---\n",
    "            model.eval()\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(X_val_tensor)\n",
    "\n",
    "                if classification:\n",
    "                    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "                    accs.append(accuracy_score(y_val, preds))\n",
    "                    f1s.append(f1_score(y_val, preds, average=\"weighted\"))\n",
    "                else:\n",
    "                    preds = outputs.cpu().numpy().flatten()\n",
    "                    mses.append(mean_squared_error(y_val_run.flatten(), preds))\n",
    "                    r2s.append(r2_score(y_val_run.flatten(), preds))\n",
    "\n",
    "        result = {\n",
    "            \"hidden_dim\": h,\n",
    "            \"mean_val\": float(np.mean(val_metrics)),\n",
    "            \"std_val\": float(np.std(val_metrics)),\n",
    "            \"all_val\": [float(x) for x in val_metrics],\n",
    "        }\n",
    "\n",
    "        if classification:\n",
    "            result.update({\n",
    "                \"mean_acc\": float(np.mean(accs)),\n",
    "                \"std_acc\": float(np.std(accs)),\n",
    "                \"all_acc\": accs,\n",
    "                \"mean_f1\": float(np.mean(f1s)),\n",
    "                \"std_f1\": float(np.std(f1s)),\n",
    "                \"all_f1\": f1s,\n",
    "            })\n",
    "        else:\n",
    "            result.update({\n",
    "                \"mean_mse\": float(np.mean(mses)),\n",
    "                \"std_mse\": float(np.std(mses)),\n",
    "                \"all_mse\": mses,\n",
    "                \"mean_r2\": float(np.mean(r2s)),\n",
    "                \"std_r2\": float(np.std(r2s)),\n",
    "                \"all_r2\": r2s,\n",
    "            })\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "def select_best_hidden(results, task=\"classification\", metric=None, n_runs=3, relative_rule=False, relative_pct=0.01):\n",
    "    \"\"\"\n",
    "    Select best hidden units using one-standard-error rule or 1% relative rule.\n",
    "\n",
    "    Args:\n",
    "        results: list of dicts (output of hidden_unit_search)\n",
    "        task: \"classification\" or \"regression\"\n",
    "        metric: which metric to optimize\n",
    "            - classification: \"val\" (loss), \"acc\", \"f1\"\n",
    "            - regression: \"val\" (loss), \"mse\", \"r2\"\n",
    "        n_runs: number of runs used (for SE calculation)\n",
    "        relative_rule: if True, use 1% relative rule instead of SE rule\n",
    "        relative_pct: percent tolerance for relative rule (default 0.01 = 1%)\n",
    "\n",
    "    Returns:\n",
    "        parsimonious (dict), best (dict), se_best (float), threshold (float)\n",
    "    \"\"\"\n",
    "    # Default metric if not provided\n",
    "    if metric is None:\n",
    "        metric = \"val\" if task == \"classification\" else \"mse\"\n",
    "\n",
    "    key = f\"mean_{metric}\"\n",
    "\n",
    "    if relative_rule:\n",
    "        # 1% relative rule\n",
    "        if metric in [\"val\", \"mse\"]:  # lower is better\n",
    "            best = min(results, key=lambda x: x[key])\n",
    "            threshold = best[key] * (1 + relative_pct)\n",
    "            candidates = [r for r in results if r[key] <= threshold]\n",
    "        else:  # acc, f1, r2 → higher is better\n",
    "            best = max(results, key=lambda x: x[key])\n",
    "            threshold = best[key] * (1 - relative_pct)\n",
    "            candidates = [r for r in results if r[key] >= threshold]\n",
    "        se_best = None  # Not used in relative rule\n",
    "    else:\n",
    "        # One-standard-error rule (original)\n",
    "        if metric in [\"val\", \"mse\"]:  # lower is better\n",
    "            best = min(results, key=lambda x: x[key])\n",
    "            se_best = best[f\"std_{metric}\"] / np.sqrt(n_runs)\n",
    "            threshold = best[key] + se_best\n",
    "            candidates = [r for r in results if r[key] <= threshold]\n",
    "        else:  # acc, f1, r2 → higher is better\n",
    "            best = max(results, key=lambda x: x[key])\n",
    "            se_best = best[f\"std_{metric}\"] / np.sqrt(n_runs)\n",
    "            threshold = best[key] - se_best\n",
    "            candidates = [r for r in results if r[key] >= threshold]\n",
    "\n",
    "    # Parsimonious = smallest hidden_dim among candidates\n",
    "    parsimonious = min(candidates, key=lambda x: x[\"hidden_dim\"])\n",
    "    return parsimonious, best, se_best, threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "1437bbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sine: Selected hidden units = 256 (mean val loss: 0.0115)\n",
      "Iris: Selected hidden units = 256 (mean val loss: 0.2048)\n",
      "Stroke: Selected hidden units = 2 (mean val loss: 0.1661)\n",
      "Wine: Selected hidden units = 8 (mean val loss: 0.4846)\n",
      "California: Selected hidden units = 256 (mean val loss: 0.3012)\n",
      "MNIST: Selected hidden units = 256 (mean val loss: 0.2642)\n",
      "All hidden unit search results saved to results/hidden_unit_search_results_20250930_220038.json\n"
     ]
    }
   ],
   "source": [
    "# --- Run hidden unit search for each dataset ---\n",
    "search_results = {}\n",
    "\n",
    "# Sine (regression)\n",
    "sine_results = hidden_unit_search(\"sine\", classification=False)\n",
    "parsimonious, best, se_best, threshold = select_best_hidden(sine_results, task=\"regression\", metric=\"mse\", n_runs=n_runs, relative_rule=True)\n",
    "search_results[\"sine\"] = {\n",
    "    \"results\": sine_results,\n",
    "    \"selected\": parsimonious,\n",
    "    \"best\": best,\n",
    "    \"se_best\": se_best,\n",
    "    \"threshold\": threshold\n",
    "}\n",
    "print(f\"Sine: Selected hidden units = {parsimonious['hidden_dim']} (mean val loss: {parsimonious['mean_val']:.4f})\")\n",
    "\n",
    "# Iris (classification)\n",
    "iris_results = hidden_unit_search(\"iris\", classification=True)\n",
    "parsimonious, best, se_best, threshold = select_best_hidden(iris_results, task=\"classification\", metric=\"val\", n_runs=n_runs, relative_rule=True)\n",
    "search_results[\"iris\"] = {\n",
    "    \"results\": iris_results,\n",
    "    \"selected\": parsimonious,\n",
    "    \"best\": best,\n",
    "    \"se_best\": se_best,\n",
    "    \"threshold\": threshold\n",
    "}\n",
    "print(f\"Iris: Selected hidden units = {parsimonious['hidden_dim']} (mean val loss: {parsimonious['mean_val']:.4f})\")\n",
    "\n",
    "# Stroke (classification)\n",
    "stroke_results = hidden_unit_search(\"stroke\", classification=True)\n",
    "parsimonious, best, se_best, threshold = select_best_hidden(stroke_results, task=\"classification\", metric=\"val\", n_runs=n_runs, relative_rule=True)\n",
    "search_results[\"stroke\"] = {\n",
    "    \"results\": stroke_results,\n",
    "    \"selected\": parsimonious,\n",
    "    \"best\": best,\n",
    "    \"se_best\": se_best,\n",
    "    \"threshold\": threshold\n",
    "}\n",
    "print(f\"Stroke: Selected hidden units = {parsimonious['hidden_dim']} (mean val loss: {parsimonious['mean_val']:.4f})\")\n",
    "\n",
    "# Wine (regression)\n",
    "wine_results = hidden_unit_search(\"wine\", classification=False)\n",
    "parsimonious, best, se_best, threshold = select_best_hidden(wine_results, task=\"regression\", metric=\"mse\", n_runs=n_runs, relative_rule=True)\n",
    "search_results[\"wine\"] = {\n",
    "    \"results\": wine_results,\n",
    "    \"selected\": parsimonious,\n",
    "    \"best\": best,\n",
    "    \"se_best\": se_best,\n",
    "    \"threshold\": threshold\n",
    "}\n",
    "print(f\"Wine: Selected hidden units = {parsimonious['hidden_dim']} (mean val loss: {parsimonious['mean_val']:.4f})\")\n",
    "\n",
    "# California Housing (regression)\n",
    "california_results = hidden_unit_search(\"california\", classification=False)\n",
    "parsimonious, best, se_best, threshold = select_best_hidden(california_results, task=\"regression\", metric=\"mse\", n_runs=n_runs, relative_rule=True)\n",
    "search_results[\"california\"] = {\n",
    "    \"results\": california_results,\n",
    "    \"selected\": parsimonious,\n",
    "    \"best\": best,\n",
    "    \"se_best\": se_best,\n",
    "    \"threshold\": threshold\n",
    "}\n",
    "print(f\"California: Selected hidden units = {parsimonious['hidden_dim']} (mean val loss: {parsimonious['mean_val']:.4f})\")\n",
    "\n",
    "# MNIST (classification)\n",
    "mnist_results = hidden_unit_search(\"mnist\", classification=True)\n",
    "parsimonious, best, se_best, threshold = select_best_hidden(mnist_results, task=\"classification\", metric=\"val\", n_runs=n_runs, relative_rule=True)\n",
    "search_results[\"mnist\"] = {\n",
    "    \"results\": mnist_results,\n",
    "    \"selected\": parsimonious,\n",
    "    \"best\": best,\n",
    "    \"se_best\": se_best,\n",
    "    \"threshold\": threshold\n",
    "}\n",
    "print(f\"MNIST: Selected hidden units = {parsimonious['hidden_dim']} (mean val loss: {parsimonious['mean_val']:.4f})\")\n",
    "\n",
    "# --- Save results to file ---\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "run_id = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"results/hidden_unit_search_results_{run_id}.json\"\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump(search_results, f, indent=2)\n",
    "\n",
    "print(f\"All hidden unit search results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "b7fc4bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAGGCAYAAACNCg6xAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAATVhJREFUeJzt3Qd8VFX2wPGTHgIk9CodkSagICxFBWkCq4KuorKKDQVRWbCs6EqxYVsEFGVdV1gVKSLgH0EU6UiTtqJSRKpUEUmAkJAy/8+5yQwzk0kySabP7/txnJk3b97cufPInDn3vvMiLBaLRQAAAFCoyMJXAQAAgCJwAgAAcBOBEwAAgJsInAAAANxE4AQAAOAmAicAAAA3ETgBAAC4icAJAADATQROAAAAbiJwAgowZswYiYiIkGAybdo00+b9+/fblnXu3NlcCrNixQrzXL32JN2m9qWvBePnV1LWz3DOnDmFrnvPPfdI3bp1C11P9yXdpu5bntqmP3hr/0Z4IXBC0LIGCHpZs2ZNnsf1bEK1atUyj//5z3/2WbuOHDlivrC3bdsm4WTRokV+CY4AwJcInBD04uPj5ZNPPsmzfOXKlfLrr79KXFxcsbf9j3/8Q86fP1/kwGns2LEBFTh9/fXX5uLtwEnftyvah9qXCCz//ve/ZdeuXRIurrnmGrMv6jVQXAROCHq9e/eWTz/9VDIzMx2WazDVunVrqVatWrG3HR0dbQKzYBcbG2su/qJ9qH2JwBITE1OiHxbBJjIy0uyLeg0UF3sPgt4dd9whv//+uyxZssS27MKFC2aOx5133pnvfI033nhD3nvvPWnQoIH58rjqqqvku+++K3SOjL5Op06dpFy5clKmTBm57LLL5JlnnjGP6dwJ3Y669957bUOJ+c0N0Tbq45odc/avf/3LPPbDDz+Y+99//72ZP1K/fn3zx18Dwvvuu8+898K4muOk2bi+fftK6dKlpUqVKjJ8+HBJT0/P89zVq1fLrbfeKrVr1zb9pMOfuq59Jk7bNXnyZHPb+p7t+83VHKetW7dKr169JDEx0fRj165dZf369S6HY7/99lsZMWKEVK5c2bS3X79+8ttvv0lxaID9wgsv2D53nY+jn5/ze9+0aZP07NlTKlWqJKVKlZJ69eqZ/rY3c+ZME5yXLVvWvI/LL79cJk6cmO9rZ2RkSIUKFcy+4SwlJcV8rk888YRt2VtvvSXNmjWThIQEKV++vLRp08ZldtWV7Oxseemll+SSSy4x29X+3bNnT6HzkU6fPm2WJyUlmX184MCBZpkr8+fPl+bNm5vt6/W8efPybcuECRPMe9F1q1atKg899JD88ccfDutpW3RYXYfe27Zta9bV/f3DDz906z0X9nk4z3GyH+53vjj/e/n444/NtnVf0M/w9ttvl0OHDrnVLoQWfgIi6Okf2/bt28uMGTPMF7H68ssvJTk52fxxmzRpksvn6RfQmTNnzB9w/UP52muvyc033yx79+41v8Rd+fHHH80f9hYtWsjzzz9vvnj1y0i/2FWTJk3M8lGjRsmDDz4oV199tVneoUMHl9vr06ePCRpmz54t1157rcNjs2bNMl80+oVkDdi0bfqlq0GTtkUDP73WgKMok6A16NEv0oMHD8pjjz0mNWrUkI8++kiWLVuWZ13N5qWmpsqQIUOkYsWKsnHjRvOFroGXPqa0D3WIUtuo2ymMtln7Rr/cnnrqKdPfGijql5UGke3atXNY/9FHHzWBw+jRo03gq1/CjzzyiOmjonrggQfkv//9r/zlL3+Rxx9/XDZs2CDjxo2THTt22L74T5w4IT169DCB2tNPP20CCH3duXPn2raj71WDdu3HV1991SzTbei+MGzYMJevre9Tgz7djr5f+yygBiEavOk+ax1G089G26nbS0tLM8GzttfVDwJnr7zyismsaCCm/xZ0/x4wYIB5fn50XuBNN91kApfBgweb/Vn7RIMnZzr0e8stt0jTpk1N/2kAr/umBmrOdP/QIEUf1/e0b98+efvtt03wrP1l/+9N/z3pe77//vvN637wwQcmkNOgRf895Kc4n4cO2TnvrwcOHDDDyvpjwkoD0Oeee05uu+02s/9o0K7/BvT5+h50/0AYsQBBaurUqRbdhb/77jvL22+/bSlbtqwlNTXVPHbrrbdaunTpYm7XqVPH0qdPH9vz9u3bZ55XsWJFy6lTp2zLP//8c7N8wYIFtmWjR482y6zefPNNc/+3337Lt13aHl1H2+eOO+64w1KlShVLZmambdnRo0ctkZGRlueff962zPre7M2YMcO81qpVq/L0i75Pq2uvvdZcrCZMmGDWmT17tm3ZuXPnLA0bNjTLly9fXuDrjhs3zhIREWE5cOCAbdnQoUMd+sqeLte+tOrbt68lNjbW8ssvv9iWHTlyxHyG11xzTZ730q1bN0t2drZt+fDhwy1RUVGW06dPWwri/Plt27bN3H/ggQcc1nviiSfM8mXLlpn78+bNs+1b+Rk2bJglMTHR4XNzx1dffZVnP1O9e/e21K9f33b/pptusjRr1sxSVPrZ6fabNGliSU9Pty2fOHGiWb59+3bbsoEDB5p/H1bz588367z22mu2Zfr+rr766jz7dKtWrSzVq1d3+Ay+/vprs579NlevXm2WTZ8+3aGdixcvzrNcn+e8P584ccISFxdnefzxxwt83+58Hta+sd+/7Z0/f97SunVrS40aNcy/QbV//36zr7300ksO62o/RkdH51mO0MdQHUKC/hLULMoXX3xhskh6Xdiv8v79+5sshpU1O6RZnfxYf1l+/vnnZvjBE7QdmuGwP0Rah/B0+/qYlQ4RWGn24eTJk/KnP/3J3N+yZUuRJ3JXr17d/LK30uEgzZI5s3/dc+fOmdfVDJrGQ/pru6iysrJMtkKHCXUYxkrbo5+ZZjt02Mqetss+o6aflW5HswNFfd9Kh/3saeZJLVy40OFz1v1Ih9dc0XW0P+yHiN1x3XXXmeE/+2yZDlnpduw/b92+ZvWch4/dpdkd+4yWO/u39o/ORdPsolVUVJTJ+Nk7evSoOfhBM0I6pGfVvXt3k4Gyp1lJXUcf033HetEMkmZbly9f7rC+Pt/aVqVZPx0OL6jdJfk87D388MOyfft2+eyzz2xzIzU7qP8W9W+Mffv18UsvvTRP+xH6CJwQEvSPa7du3czwm/6h0y9V+6DAFZ2zY88aRDnPu7CnX2wdO3Y06Xqdp6HDKjrMVpIg6vrrrzdfLPZfpHq7VatW0qhRI9uyU6dOmSEHfV0NZvQ967wbpUMxRaEBR8OGDfMM7+kXlDMdztOhEp3XoV90+rrWYcWivq7SYQ4d+nP1Wjo0pH3pPHekOJ9Vfu9bh6/0vdvTL0H94rUGYvr+dBhKjxLUIEeHr6ZOneowD0q/ZPXz0eFhHZ7S+U+LFy8utA0amOi2Nfi2bk/3WQ3Q7AOnv//976a/da6PfkEPHTrUNiTsjuL0mb5/DWD1de05f1bWftJ2OXNe9+effzb7iQ596b5jfzl79qz50VBQu61tL+yzLu7nYaVDp/oZ6xCc9QeJtf36I0Hfq3P7dSjQuf0IfcxxQsjQbMWgQYPk2LFj5o9nYfMO9Je0KzkjS65pwLJq1SrzK1OzE/qHWYMczSJoFiW/bRZE50lp9kXnkrzzzjty/Phx8wX58ssvO6ynv3jXrl0rTz75pAmq9MtNgwwNvDyV/XKmAahmCjRo0y/yxo0bm8nZhw8fNsGUt17XE59VQQqbD2YtIKlzxxYsWCBfffWV+SL+5z//aZZp32sgoFkXfUzn1OlFv3jvvvtuM4eqIBpw6xe1Pkc/ew2+tW9btmzpEERqqQDNeul+plkQ3T90/lx+ZR+82WfFpfuI9tX06dNdPq4BiCfaXZLPQ+ft6Y8S/UHknHXV9uv+oNtz1TbnIBOhj8AJIUMn3eokVP1iK86kYXdpxkInoOpl/PjxJsB59tlnTTClWa/iVKrWTIP+cV+6dKn5FatfEvbZB/21rY/pF6Z+cdr/Gi6OOnXqmKP19HXs2+tc00eHLXbv3m3apl9AVq6GQ9x93/pFqcOCruoH7dy50/SvHrnnDfq+9YtQ+00DEysNVvXIMX3cnmYe9KKTgzWbqZOr9cgt/YJVOhR2ww03mItuV7MeGhDpRGLnrJY9nVSsmR3dT/UITZ2Ur/uQMw1SdT/Qix4pqgcvaFtGjhzplTIZ+v51P9NMkH1A4PxZWfvJ1f7nvK4evfjNN9+YTK39sK83FOfz0AyoZqf1x4j1yFDn9uu/E83u2meAEb4YqkPI0D/07777rjnsXf9weoNmXpzpH1xlHXbRLzuV3yHcrmjApUNh+kWqFx2esQ7DKesvXedf3Xp0WXFrX+lRcPan5dDhMz1Kz56r19Xbrg65d/d96zb1iDUdqrI/LYwGLxqcaCChR9t5g75vV/2mAbD1KEdroOrc186fs3MZCA349GhL+3Xyo+vql7Vms/SoLi2RYB8ou9q+BgU6/0fbld+8K0/0j7ZF/x3ZZx11+MqeBn3aHxpQ2w/XakD9008/5cmU6ja0BIQzfa2i/DspSHE+D22XZv80KNWMnqtaZxqs6j6rP1qc9wm97045EIQWMk4IKa4Om/YkLTWgQ3X6Bau/unV+gw6f6JwK/cK3/kLVYcIpU6aYejIaUOjh9faBkDM9HFv/QGs2Qye4ao0pexpIaJZCDynXL82aNWuaoUE9rLs4dEhTDwfXLNLmzZvNF6F+gWsmyJ4OH+n70UPadXhO26FfMK7mm+hkX6WHm2v9I/2ysR5a7+zFF1+01cPSrIDO+9HMgH7B6Xv0Fh0K031EA0T9wta5TDpMowGADpl16dLFrKf39XPVLKa+fz3gQMsD6Pu3Bl+addJAWodp9fPXeT8aYGhAYZ/Nyo8GSrq+lljQekPOz9HgUudeaaZG57VpJlI/M933dL/yBv3Boa+nJRg0qNVATedfuZrLpiUItC36GeowpvaFte6UZqystI81E6zr61Cavi/d3zVbpRPHNQgvbD6iO4rzeei/Uc32aekF50ne2uc6TK2fv+6vmuXTPtH9RPtf/+3p8LoO7dnX3kIY8PdhfYAnyhEUJL9yBK+//nqhh807H86+dOlSc5i4Hq6sh9PrtZYT2L17t8N2tLRB06ZNzeHK7pYmWLJkiVlXD/M/dOhQnsd//fVXS79+/SzlypWzJCUlmZILegi/c5vdKUegtJTAjTfeaElISLBUqlTJHM5tPUTc/nDtn376yZQDKFOmjFlv0KBBlv/973953pceBv7oo49aKleubN6Dfb85t1Ft2bLF0rNnT7NdbYOWj1i7dq1bn3Fhh5Xn9/mpjIwMy9ixYy316tWzxMTEWGrVqmUZOXKkJS0tzaFt+rnWrl3bHAqv5SL+/Oc/WzZt2mRbZ86cOZYePXqYx3Rf0HUfeugh22HshdHyCvra2r4XX3wxz+P/+te/TGkGLZuhbWjQoIHlySeftCQnJxe4XWvffPrppw7Lrfu9/WfmXI5A/f7775a77rrLHNqv+5ne3rp1q8v9+LPPPjNlD7R9ur/PnTvX5TbVe++9Zw71L1WqlCk7cfnll1ueeuopsw/n92+1oP3XmTufh/N+Y90/XF2cX0/fa6dOnSylS5c2l8aNG5sSHLt27SqwXQg9Efo/fwdvAAAAwYA5TgAAAG4icAIAAHATgRMAAICbCJwAAADcROAEAADgJgInAACAYCiAqYUEX3/9dVOAT8+2rcXEtLiYO/RcXlpYrXnz5qaomru0DL9WTNYCZsU5NQYAAAgtWplJC93WqFHDVJ0P2MBJKyRrJV+tOqtVk92lFX+14rGeK0xP01AUGjR56zxYAAAgeB06dMhUng/YwEnPYK+XotLy+Hfeeac5pcP8+fOL9FzrqQq0czxxPiw9/YWe+sJ6GgF4H33ue/S579HnvkV/h3efp6SkmKSKO6czCrpz1U2dOlX27t0rH3/8sTl/UFFZh+c0aPJU4KTn99Jt+fuDDxf0ue/R575Hn/sW/e17GQHY5+5M4QmqwElPCqknn1y9erU5Kag79KSh9mfG1qjS+oF54gzj1m1462zlyIs+9z363Pfoc9+iv8O7zzOK0IagCZyysrLM8NzYsWOlUaNGbj9Pz8itz3Gm6UHnM8GXhJ7pHb5Fn/sefe579Llv0d/h2eepqalurxswJ/nV9FhBR9XphPDy5cubeU32R8hp83WZBkLXXXedWxknHcc8efKkx4bq9EPv3r17wKQaQx197nv0ue/R575Ff4d3n6ekpEilSpUkOTm50NggaDJO+ka2b9/usOydd96RZcuWyZw5c6RevXounxcXF2cuzvRD8uQH5entoXD0ue/R575Hn/sW/R2efR5ThNf3a+B09uxZ2bNnj+3+vn37TE2mChUqSO3atWXkyJFy+PBh+fDDD01dBa3ZZK9KlSoSHx+fZzkAAIA3+DVw2rRpk3Tp0sV2f8SIEeZ64MCBMm3aNFMU8+DBg35sIQAAQIAETp07dzZzlPKjwVNBxowZYy4AAAC+wLnqAAAA3ETgBAAA4CYCJwAAADcFTTmCQHQiJU2O/HFODp0V+fFISqHVzKuUjZMqifE+ax8AAPAsAqcSmL7hoExc+rPpxje2ry90/WFdL5Xh3d2veg4AAAILgVMJDGhXW7o0qihr1qyRTp06SaYlQv4yZZ15bM7g9hIfc7HKuTXjBAAAgheBUwnosFv5UlFyoIxIsxqJkmG5eFblpjUSJSGW7gUAIJQwORwAAMBNBE4AAABuInACAABwE4ETAACAmwicAAAA3ETgBAAA4CYCJwAAADcROHlQVrbFdnvjvlMO9wEAQPAjcPKQr348Lt3Gr7Tdv2fqd9Lp1WWy+Iejfm0XAADwHAInD/jf7xHy6Mz/yfGUdIflx5LTZMjHWwieAAAIEQROJaTDcXP3R4qrQTnrsrELfmLYDgCAEEDgVEKbDvwhpy9cPEedMw2XjianmTlPAAAguBE4ldCJM+lurpfm9bYAAADvInAqoSpl49xcL97rbQEAAN5F4FRCbeqUl3KxFslvsE6XV0+Kl7b1Kvi4ZQAAwNMInEooKjJCbq6b7fIxazA1+oamZj0AABDcCJw8oGVFi7x1e0up7DRsVy0pXt7965VyffPqfmsbAADwnGgPbius9WxWVVrVqSCdXl1u7r8/sLV0uawqmSYAAEIIGScPio+52J2tapUjaAIAIMQQOHlQdOTF7szMouAlAAChhsDJg2KiLmaYMqkUDgBAyCFw8qCIiIuBU0aW6yPtAABA8CJw8hKG6gAACD0ETl5CxgkAgNBD4OQlGcxxAgAg5FDHqQROpKTJkT/OyaGzIj8eSZFMy8U5TruOnZEouzlP1vPaVUnknHUAAAQrAqcSmL7hoExc+rPpxje2r3d47Kk53+dZf1jXS2V490Y+bCEAAPAkAqcSGNCutnRpVFHWrFkjnTp1kujoaHnwo01y5HSavHZLC2laIzFPxgkAAAQvAqcS0GG38qWi5EAZkWY1EiUmJkZKx+Z06SUVSknzmkn+biIAAPAgJod7WHRUTpdSjgAAgNBD4ORhsbnVwylHAABA6CFw8lLGKYOMEwAAIYfAyUvnq8vMJuMEAECoIXDysBhbxonACQCAUOPXwGnVqlVyww03SI0aNcwJcufPn1/g+nPnzpXu3btL5cqVJTExUdq3by9fffWVBJLoSOscJ4bqAAAINX4NnM6dOyctW7aUyZMnux1oaeC0aNEi2bx5s3Tp0sUEXlu3bpVAQcYJAIDQ5dc6Tr169TIXd02YMMHh/ssvvyyff/65LFiwQK644goJpMCJcgQAAISeoJ7jlJ2dLWfOnJEKFSpIoE0OJ+MEAEDoCerK4W+88YacPXtWbrvttnzXSU9PNxerlJQUc52RkWEuJWXdhvU6d4qTpF3I9Mj2UXifw/voc9+jz32L/g7vPs8oQhsiLBZLQIwp6eTwefPmSd++fd1a/5NPPpFBgwaZobpu3brlu96YMWNk7NixLp+fkJAgnjbrl0hZeyJSetfKkp6XBETXAgCAAqSmpsqdd94pycnJ5uCzkMs4zZw5Ux544AH59NNPCwya1MiRI2XEiBEOGadatWpJjx49Cu0cd6PUJUuWmEnreq66777YIWtPHJJ6DS6V3l0blnj7KLzP4X30ue/R575Ff4d3n6fkjka5I+gCpxkzZsh9991ngqc+ffoUun5cXJy5ONMPyZMflHV7sdE5XZotEX7fEUKdpz9DFI4+9z363Lfo7/Ds85givL5fAyedn7Rnzx7b/X379sm2bdvMZO/atWubbNHhw4flww8/tA2vDRw4UCZOnCjt2rWTY8eOmeWlSpWSpKQkCQQx0bmTwzOZHA4AQKjx61F1mzZtMmUErKUEdEhNb48aNcrcP3r0qBw8eNC2/nvvvSeZmZkydOhQqV69uu0ybNgwCRQxkbnlCLKZ3wQAQKjxa8apc+fOUtDc9GnTpjncX7FihQQ6CmACABC6grqOUyCKpo4TAAAhi8DJSwUwqRwOAEDoIXDy0lDdBTJOAACEHAInD4vmXHUAAIQsAicPi2WOEwAAIYvAycOic8sRZFCOAACAkEPg5GEx0dahOjJOAACEGgInD4uJZKgOAIBQReDkpcnhGUwOBwAg5BA4eamOExknAABCD4GTl+o4UY4AAIDQQ+DkYZyrDgCA0EXg5K1z1WUTOAEAEGoInDwsJreOE0N1AACEHgInD4uJZnI4AAChisDJW5XDyTgBABByCJw8LJbJ4QAAhCwCJy9NDmeOEwAAoYfAyVvlCLKzxWIheAIAIJQQOHmpcrjGTFnZBE4AAIQSAicvnatOZRI4AQAQUkocOKWkpMj8+fNlx44dnmlRiGSc1AUmiAMAEN6B02233SZvv/22uX3+/Hlp06aNWdaiRQv57LPPJNxZC2AqJogDABDmgdOqVavk6quvNrfnzZtnJkCfPn1aJk2aJC+++KKEu8jICImKpAgmAAChqMiBU3JyslSoUMHcXrx4sdxyyy2SkJAgffr0kZ9//tkbbQw60QROAACEpCIHTrVq1ZJ169bJuXPnTODUo0cPs/yPP/6Q+Ph4b7QxaEsSMFQHAEBoiS7qE/72t7/JgAEDpEyZMlKnTh3p3LmzbQjv8ssv90Ybg3aCOBknAADCPHB6+OGHpW3btnLo0CHp3r27ROZOhq5fvz5znJxKEnC+OgAAwjxwUnoknV5UVlaWbN++XTp06CDly5f3dPuCEuerAwAgNEUWZ6juP//5jy1ouvbaa+XKK680c59WrFjhjTYG7/nqsgmcAAAI68Bpzpw50rJlS3N7wYIFsm/fPtm5c6cMHz5cnn32WW+0MWgnh1/IZKgOAICwDpxOnjwp1apVM7cXLVokt956qzRq1Ejuu+8+M2SHi+UIyDgBABDmgVPVqlXlp59+MsN0Wo5AJ4ir1NRUiYqK8kYbgw7lCAAACE1Fnhx+7733mlOsVK9eXSIiIqRbt25m+YYNG6Rx48beaGPQliPgXHUAAIR54DRmzBhp3ry5KUegw3RxcXFmuWabnn76aW+0MWjLEZBxAgAgtBSrHMFf/vKXPMsGDhzoifaEBMoRAAAQmoo8x0mtXLlSbrjhBmnYsKG53HjjjbJ69WrPty7IyxEQOAEAEOaB08cff2zmNemJfR977DFzKVWqlHTt2lU++eQT77QyWCeHZzNUBwBAWA/VvfTSS/Laa6+Zuk1WGjyNHz9eXnjhBbnzzjsl3HGuOgAAQlORM0579+41w3TOdLhOi2FC6zhxrjoAAEJRkQMnPbXK0qVL8yz/5ptvzGO4OFRHxgkAgDAPnB5//HEzNDdkyBD56KOPzGXw4MHmHHZPPPFEkba1atUqk72qUaOGqQk1f/78Qp+j58PTc+NpGQSdmD5t2jQJ1KG6TAInAADCe46TBkx6ypV//vOfMnv2bLOsSZMmMmvWLLnpppuKtK1z586Z897p6VpuvvnmQtfXocA+ffqYQG369Okm8/XAAw+YYpw9e/aUgDtXHUN1AACElGLVcerXr5+5lFSvXr3MxV1TpkyRevXqmaDNGrCtWbNG3nzzzYAKnKzlCMg4AQAQWopVx8lf1q1bZzvFi5UGTLo8kFCOAACAMM44lS9f3sxBcsepU6fEW44dO2ZOMmxP76ekpMj58+dNPSln6enp5mKl66qMjAxzKSnrNuy3FSU5AVPahUyPvAYK73N4F33ue/S5b9Hf4d3nGUVog1uB04QJEyRYjRs3TsaOHZtn+ddff22KeHrKkiVLbLf3HdKMU6T8sm+/LFq012Ovgfz7HL5Bn/sefe5b9Hd49nlqaqpnA6dAOQ+dTko/fvy4wzK9n5iY6DLbpEaOHCkjRoxwyDhp2YQePXqY53kiStUPvXv37hITE2OWHVi5Vxb/ukdqXFJLevduVuLXQOF9Du+iz32PPvct+ju8+zwldzTKa5PD/aV9+/ayaNEih2Xa6bo8P1q2QC/O9EPy5Adlv724mJxu1YPq/L0zhDJPf4YoHH3ue/S5b9Hf4dnnMUV4fb9ODj979qxs27bNXKzlBvT2wYMHbdmiu+++27a+liHQyuVPPfWU7Ny5U9555x1TEsH+9C8BNTmccgQAAIQUvwZOmzZtkiuuuMJclA6p6e1Ro0aZ+0ePHrUFUUpLESxcuNBkmbT+k5YleP/99wOqFIHiXHUAAIQmvw7Vde7cWSyW/LMyrqqC63O2bt0qgSzadsoVMk4AAISSoKrjFCw4Vx0AAKGpyBknPU3KK6+8Yk53cuLECcnOdgwOdA5SuLOdq86pbwAAQJgFTnpuuJUrV8pdd91lzhHnbmHMsMw4ZTJUBwBAWAdOX375pZmg3bFjR++0KARER+ZODifjBABAeM9x0tOvVKhQwTutCRGUIwAAIDQVOXB64YUXTLmAopQnDzdMDgcAIDQVeahOayf98ssv5uS6devWzVNtc8uWLRLuoqnjBABASCpy4NS3b1/vtCQkM04M1QEAENaB0+jRo73TklAsR0DGCQCAkFLsyuGbN2+WHTt2mNvNmjWznTYFFzNOF8g4AQAQ3oGTFr28/fbbZcWKFVKuXDmz7PTp09KlSxeZOXOmVK5cWcIdBTABAAhNRT6q7tFHH5UzZ87Ijz/+KKdOnTKXH374QVJSUuSxxx7zTiuDTHQk5QgAAAhFRc44LV68WL755htp0qSJbVnTpk1l8uTJ0qNHD0+3LyjFRFuH6sg4AQAQ1hknPTedcwkCpcucz1sXrmJyK4czORwAgDAPnK677joZNmyYHDlyxLbs8OHDMnz4cOnataun2xfUk8OzLSJZ+j8AABCegdPbb79t5jNp8csGDRqYS7169cyyt956yzutDNICmIoimAAAhPEcp1q1apnq4DrPaefOnWaZznfq1q2bN9oX1BknlUnGCQCA8K7jFBERId27dzcXFBw4ZWRmi8T5tTkAAMCXgdOkSZPkwQcflPj4eHO7IJQkEImKjJCICBGLRSSDCfMAAIRX4PTmm2/KgAEDTOCktwvKRBE4Xcw6XcjM5nx1AACEW+C0b98+l7dRcEmCC5QkAAAgvI+qe/755yU1NTXP8vPnz5vH4FgEk6PqAAAI48Bp7Nixcvbs2TzLNZjSx+B42hWG6gAACOPAyWKxmLlMzv73v/9JhQoVPNWu0DnRL4ETAADhV46gfPnyJmDSS6NGjRyCp6ysLJOFGjx4sLfaGbQlCThfHQAAYRg4TZgwwWSb7rvvPjMkl5SUZHssNjbWVBJv3769t9oZtNXDmRwOAEAYBk4DBw4013p6lQ4dOrg80S8uis3NODHHCQCAMK4cfu2119pup6WlyYULetD9RYmJiZ5pWYhknCiACQBAGE8O16PnHnnkEalSpYqULl3azH2yv8BxjhOTwwEACOPA6cknn5Rly5bJu+++K3FxcfL++++bOU81atSQDz/80DutDEIxtnIEZJwAAAjboboFCxaYAKlz585y7733ytVXXy0NGzaUOnXqyPTp082pWWA3VEfgBABA+GacTp06JfXr17fNZ9L7qlOnTrJq1SrPtzDIh+qYHA4AQBgHTho0Wc9X17hxY5k9e7YtE1WuXDnPtzDoC2CScQIAIGwDJx2e0yrh6umnn5bJkydLfHy8DB8+3Mx/gnPGicAJAICwneOkAZJVt27dZOfOnbJ582Yzz6lFixaebl/QimaoDgCAkFPkwMmZTgrXCxzFROYO1VHHCQCA8AqcJk2a5PYGH3vssZK0J2QwORwAgDANnN58802H+7/99psphGmdDH769GlJSEgwRTEJnHJQjgAAgDCdHK5H0VkvL730krRq1Up27NhhShHoRW9feeWV8sILL3i/xUGCyeEAAISeIh9V99xzz8lbb70ll112mW2Z3tas1D/+8Q9Pty8EyhEwVAcAQNgGTkePHpXMzMw8y7OysuT48eOealfIZJwukHECACB8A6euXbvKQw89JFu2bLEt03IEQ4YMMeUJ4FiOgIwTAABhHDh98MEHUq1aNWnTpo05ya9e2rZtK1WrVjUn/C0qLaBZt25dU0SzXbt2snHjxgLXnzBhghkaLFWqlNSqVcvUlUpLS5NAQzkCAABCT5HrOFWuXFkWLVoku3fvNsUvradeadSoUZFffNasWTJixAiZMmWKCZo0KOrZs6fs2rXLHKHn7JNPPjHVyjV469Chg2nDPffcIxERETJ+/HgJJDHRuUN1mWScAACQcC+AqYFScYIlexrsDBo0yJzGRWkAtXDhQhMYaYDkbO3atdKxY0e58847zX3NVN1xxx2yYcMGCTTRZJwAAAjPwEmzQlpqoHTp0uZ2QdzN/Fy4cMHMjRo5cqRtWWRkpJkntW7dOpfP0SzTxx9/bIbzdHhw7969Jvt111135fs66enp5mKVkpJirjMyMsylpKzbcN5WVEROpik9I8sjr4PC+xzeQ5/7Hn3uW/R3ePd5RhHa4FbgtHXrVttG9XZ+dMjMXSdPnjRH4uncKHt63zoE6EwzTfq8Tp06icViMUf3DR48WJ555pl8X2fcuHEyduzYPMu//vprU7TTU5YsWeJwf+dx7YsoOXzkqCxadNhjr4P8+xzeR5/7Hn3uW/R3ePZ5amqqZwOn5cuXu7ztaytWrJCXX35Z3nnnHTMnas+ePTJs2DCTDdP6Uq5oRss+S6YZJ51U3qNHD0lMTCxxmzSg1A+9e/fuEhMTY1ueuuWwzNr7o1SoXEV6976yxK+Dwvsc3kOf+x597lv0d3j3eUruaJRPTvJbXJUqVZKoqKg8tZ/0vh6154oGRzos98ADD5j7l19+uZw7d04efPBBefbZZ81QnzPrkX/O9EPy5AflvL1SsTm3tYyTv3eIUOXpzxCFo899jz73Lfo7PPs8pgiv71bgdPPNN7u9wblz57q1XmxsrLRu3VqWLl0qffv2Ncuys7PN/UceeSTfVJpzcKTBl9Khu0DCueoAAAg9bgVOSUlJXnlxHUIbOHCgqQmlk721HIFmkKxH2d19991Ss2ZNM09J3XDDDWby+RVXXGEbqtMslC63BlCBgnPVAQAQpoHT1KlTvfLi/fv3l99++01GjRolx44dMycPXrx4sW3C+MGDBx0yTHouPJ2ArteHDx82NaU0aNITDwfsueqyAysTBgAAis9vc5ysdFguv6E5nQxuLzo6WkaPHm0uQXOuukwyTgAAhHXgNGfOHJk9e7bJCGk9Jnv257ALZ9G5mTIyTgAAhPG56iZNmmTmIOlwmtZ00rlJFStWNMUoe/Xq5Z1WBiHbUB1znAAACN/ASWsovffee/LWW2+ZI+OeeuopU4fhsccek+TkZO+0MghdnBxOxgkAgLANnHR4Tk99okqVKiVnzpwxt7W+0owZMzzfwiBFOQIAAEJPkQMnLU556tQpc7t27dqyfv16c3vfvn0BV0vJn2IpRwAAQMgpcuB03XXXyf/93/+Z2zrXafjw4aZcupYW6NevnzfaGJSicwOnTIbqAAAI36PqdH6TVvhWQ4cONRPD165dKzfeeKM89NBD3mhjUIqOzBmqu0DGCQCA8A2ctCClfVHK22+/3VzgKDaacgQAAEi4D9U1bNhQxowZI7t37/ZOi0Is45SVbZFsgicAAMIzcNLhuYULF0qTJk3kqquukokTJ5rTpcBRTG7GSWXkDm0CAIAwC5x0Mvh3330nO3bskN69e8vkyZOlVq1a0qNHD/nwww+908ogFGM3nMkEcQAAwjRwsmrUqJGMHTvWDNmtXr3anKxXj7KDY+VwRUkCAABCQ4lO8rtx40b55JNPZNasWZKSkiK33nqr51oW5KJy5zgpqocDABCmGSfNMI0ePdpknDp27GiG7F599VU5fvy4zJw50zutDEIREREXz1fHHCcAAMIz49S4cWMzKVwniWsZAj3ZL/I/X11GVpZkZJJxAgAgLAOnXbt2yaWXXuqd1oRoSQKOqgMAIEyH6giail4Ek8nhAACE+VF1KFx0bkkCyhEAABAaCJy8KDp3cjjnqwMAIDQQOHlRbBQZJwAAQgmBkw8yTplknAAACM+j6rKysmTatGmydOlSOXHihGQ7HTG2bNkyT7Yv6MsRKIbqAAAI08Bp2LBhJnDq06ePNG/e3BR6hGvRDNUBABBSihw4aXXw2bNnmxP8omCxuUN1lCMAACBM5zjFxsZKw4YNvdOaEC1HkJFNxgkAgLAMnB5//HGZOHGiWCwEA4VhcjgAAGE+VLdmzRpZvny5fPnll9KsWTOJiYlxeHzu3LmebF9IlCNgqA4AgDANnMqVKyf9+vXzTmtCNOOUweRwAADCM3CaOnWqd1oSwuUIyDgBABAaKIDpg8CJcgQAAIRpxknNmTPHlCQ4ePCgXLhwweGxLVu2eKptQS+Gc9UBABDeGadJkybJvffeK1WrVpWtW7dK27ZtpWLFirJ3717p1auXd1oZpCiACQBAmGec3nnnHXnvvffkjjvuMBXEn3rqKalfv76MGjVKTp06JeHsREqanDiTbrufnJqTjVu396RU3hArzWokSVTkxUrrVcrGSZXEeL+0FQAA+CBw0uG5Dh06mNulSpWSM2fOmNt33XWX/OlPf5K3335bwtX0DQdl4tKf8yxfv/eUuTgb1vVSGd69kY9aBwAAfB44VatWzWSW6tSpI7Vr15b169dLy5YtZd++fWFfFHNAu9rSvWlVWbvnpLz85c5813umV2Pp0LCSyTgBAIAQnuN03XXXyf/93/+Z2zrXafjw4dK9e3fp379/2Nd30mG3JtUTZera/fmuowN1+riuxzAdAAAhnnHS+U3Z2TlHiQ0dOtRMDF+7dq3ceOON8tBDD0m427jvlBxNTsv3cc3J6eO6XvsGFX3aNgAA4OPAKTIy0lysbr/9dnNBjhNn0jy6HgAACPICmKtXr5a//vWv0r59ezl8+LBZ9tFHH5nz2IW7KmXjPboeAAAI4sDps88+k549e5oj6rSOU3p6zuH3ycnJ8vLLL0u4a1uvglRPijdzmVzR5fq4rgcAAEI8cHrxxRdlypQp8u9//1tiYmJsyzt27Bj2VcO1jtOOoylyb4e6Zi6TK7pcH9f1dH0AABDCgdOuXbvkmmuuybM8KSlJTp8+XeQGTJ48WerWrSvx8fHSrl072bhxY4Hr62vopPTq1atLXFycNGrUSBYtWiSBUsfpz2+tKbAUgdLHdT1dHwAAhHgdpz179phgx57Ob9IK4kUxa9YsGTFihMlgadA0YcIEMwyowVmVKlXyrK/nxdPSB/qYni+vZs2acuDAASlXrpwEUh0nq6xsizzw3+/kt7MX5L6OdeWmVjXzVA4HAAAhHDgNGjRIhg0bJh988IFERETIkSNHZN26dfLEE0/Ic889V6RtjR8/3mxP60EpDaAWLlxotv3000/nWV+Xa/FNLX9gHSZ0DuD8SesyOddmalojSVbu/k0uq1ZWWtYKjAAPAAD4KHDSgEbrOHXt2lVSU1PNsJ0OmWng9Oijj7q9Hc0ebd68WUaOHGlbpmUOunXrZgIxV7Twph7Jp0N1n3/+uVSuXFnuvPNO+fvf/y5RUVEun6OT160T2FVKSoq5zsjIMJeSsm4jv21VT8rJKh36/ZxHXg+F9zk8jz73Pfrct+jv8O7zjCK0IcJSzPOkaOCjQ3Znz56Vpk2bSpkyZYr0fM1U6VCbZo80GLLSkwavXLlSNmzYkOc5jRs3lv3798uAAQPk4YcfNq+v14899piMHj3a5euMGTNGxo4dm2f5J598IgkJCeJtX/8aIQsPRUnbytkyoGFO4VAAABA4NBGkiRitEJCYmOjZjJNVbGysCZh8STNdOr9Jq5drhql169amjtTrr7+eb+CkGS2dR2WfcapVq5b06NGj0M5xN0pdsmSJmXtlf5Sh7fH/HZWFh7ZLZJmK0rv3VSV+PRTe5/A8+tz36HPfor/Du89Tckej3OF24HTfffe5tZ7OQ3JHpUqVTPBz/Phxh+V6Xyegu6JH0mnn2g/LNWnSRI4dO2YyYBrMOdNhRL040+148oPKb3u1K+Zk4o4kp/t9xwg1nv4MUTj63Pfoc9+iv8Ozz2OK8PpulyOYNm2aLF++3JQD+OOPP/K9uEuDHM0YLV261CGjpPfth+7saa0oHZ6znitP7d692wRUroKmQFCzfClzfTT5vGRnF2tUFAAABAi3M05DhgyRGTNmyL59+8xRcHrKlQoVSlb9WofQBg4cKG3atJG2bduacgTnzp2zHWV39913m3lQ48aNs7Xh7bffNkf16UT0n3/+2VQr1zlOgapq2ThTgiAjyyK/nU2Xqk5H3QEAgOARWZRClUePHjWTtxcsWGDmCd12223y1VdfSTHnl0v//v3ljTfekFGjRkmrVq1k27ZtsnjxYqlaNacW0sGDB81rWulr6ut999130qJFCxMwaRDlqnRBoIiOipRqucHSr3+c93dzAABACRRpcrjOFbrjjjvMRQtP6vCdHtWWmZkpP/74Y5GPrFOPPPKIubiyYsWKPMt0GG/9+vUSTGqUi5fDp8/LkdPnpXWd8v5uDgAA8NUpV2xPjIw0BTA125SVlVXczYSFmuVy5jlp8AQAAMIkcNJCkjrPSQ8d1HPEbd++3cw50iG14mSbwoV1grhmnAAAQBgM1emQ3MyZM808Iy1NoAGUlhRA4WpYM07McQIAIDwCJz2PXO3atc2JfLWyt15cmTt3rifbFxIYqgMAIMwCJy0NoHOaUHQETgAAhFngpEfQoWRDdWfSMiUlLUMS46lKCwBAWB1VB/eVjouWcgk5wRITxAEACF4ETr4ermOCOAAAQYvAycfDdWScAAAIXgROPs44/UrgBABA0CJw8pFLbEUw0/zdFAAAUEwETj4vgpnq76YAAIBiInDy8VAdGScAAIIXgZOPM07Hz6TJhcxsfzcHAAAUA4GTj1QqEyux0ZFisYgcSybrBABAMCJw8hE9XQ2nXgEAILgROPkQgRMAAMGNwMmHapSLN9cUwQQAIDgROPlQzXIJ5prTrgAAEJwInHyoprUIZjKBEwAAwYjAyQ9DdWScAAAITgROPnSJdaju9HmxaF0CAAAQVAicfKhaUrxERIikZ2bL7+cu+Ls5AACgiAicfEgLYFYpG2duM1wHAEDwIXDy06lXKEkAAEDwIXDyMYpgAgAQvAic/FSSgMAJAIDgQ+Dkr4wTc5wAAAg6BE4+xlAdAADBi8DJx5gcDgBA8CJw8tMcpz9SMyT1Qqa/mwMAAIqAwMnHEuNjpGxctLlN1gkAgOBC4OTHrNOvTBAHACCoEDj5dZ5Tmr+bAgAAioDAya9H1qX6uykAAKAICJz8WQSToToAAIIKgZMfMFQHAEBwInDyA4pgAgAQnAic/Bg4HUtJk8ysbH83BwAAuInAyQ+qlI2TmKgIycq2yPEz6f5uDgAAcBOBkx9ERkZItaR4c5simAAABI+ACJwmT54sdevWlfj4eGnXrp1s3LjRrefNnDlTIiIipG/fvhK085w4sg4AgKDh98Bp1qxZMmLECBk9erRs2bJFWrZsKT179pQTJ04U+Lz9+/fLE088IVdffbUEo+q5GadvdhyXdb/8bobtAABAYPN74DR+/HgZNGiQ3HvvvdK0aVOZMmWKJCQkyAcffJDvc7KysmTAgAEyduxYqV+/vgSbxT8clSU/HTe3v/j+qNzx7/XS6dVlZjkAAAhcfg2cLly4IJs3b5Zu3bpdbFBkpLm/bt26fJ/3/PPPS5UqVeT++++XYHEiJU1+OJws7638RQZ/vEXOpmc5PH40Oc0s18d1PV0fAAAElmh/vvjJkydN9qhq1aoOy/X+zp07XT5nzZo18p///Ee2bdvm1mukp6ebi1VKSoq5zsjIMJeSsm6jsG19tG6fvLV8b6Hbe/nLnPf9aJf68th1DUvcvlDkbp/Dc+hz36PPfYv+Du8+zyhCG/waOBXVmTNn5K677pJ///vfUqlSJbeeM27cODOk5+zrr782Q4KesmTJkgIfr3xB5LZ6ETJ7X1Sh27qtXpZUTtktixbt9lj7QlFhfQ7Po899jz73Lfo7PPs8NTU1OAInDX6ioqLk+PGc+T5Wer9atWp51v/ll1/MpPAbbrjBtiw7O6eAZHR0tOzatUsaNGjg8JyRI0eayef2GadatWpJjx49JDEx0SNRqn7o3bt3l5iYmALXXfD9UZm9b3uh2/xTm1ZyQ4vqJW5bqCpKn8Mz6HPfo899i/4O7z5PyR2NCvjAKTY2Vlq3bi1Lly61lRTQQEjvP/LII3nWb9y4sWzf7hh4/OMf/zCZqIkTJ5qAyFlcXJy5ONMPyZMflDvbq16utFvb0vX8vRMFA09/higcfe579Llv0d/h2ecxRXh9vw/VaTZo4MCB0qZNG2nbtq1MmDBBzp07Z46yU3fffbfUrFnTDLlpnafmzZs7PL9cuXLm2nl5oNHJ3gmxUVKpTKycPHsh3/X0cV1P16+SmFOyAAAABAa/B079+/eX3377TUaNGiXHjh2TVq1ayeLFi20Txg8ePGiOtAt20zcclIlLfy50PQ2qbpr8rQzreqkM797IJ20DAABBEjgpHZZzNTSnVqxYUeBzp02bJsFgQLva0r1pTjC4ds9JeW/1XofMU1x0pKRnZpvrl/o1l2surezH1gIAgIANnMKBDrtZh96a10yS+6+uLxv3nZITZ9KkStl4uaJ2OXnoo82ycvdv8tLCHXLFkPJSxd+NBgAADoJ/DCxIRUVGSPsGFeWmVjXNdXxMlLwz4EppcUmS/JGaIXf/ZyNFMAEACDAETgGkdFy0fHDPVVK3YoIcPn1eBk79Ts6k+b8wGAAAyEHgFGAqlYmT/97X1hxdt+Noigz+eLOkZzqengUAAPgHgVMAqlOxtEy9p62Ujo2Sb/f8Lk98+r1kZ1v83SwAAMIegVOAuvySJHn3r60lOjJCFvzviLy8aIe/mwQAQNgjcApg1zSqLK/9pYW5/f6affL+6sJPEgwAALyHcgQB7uYrL5G9v52Tt5fvkRcX7jC1njo1rCQ/Hkk2R9+VT4iRZjWSzFF6VlXKxlF1HAAALyBwCgJ2MZG8/tUucykIVccBAPAOAqcg8Nc/1ZFuTavK3+d8LzuOncl3vWd6NZYODSuZjJM3ZGVbHIp2tq1XwSHTBQBAqCNwCgI67FaxTJz8kZr/yYE1fJm6dr+pSO6NYGbxD0dl7IKf5GjyxaKc1ZPiZfQNTeX65tU9/noAAAQiAqcgoZmeYynp+T6uxQo0qPnzpNVSt1JpKV86ViokxOZcl46R8gl6HWuudZmWOoiIKDjA0srlJ86km3PrvfzlzjyP6+sN/niLQ6aLuVUAgFBG4BQkdHjMHTqUV9BwnlVsVKSUtw+o7AOthBhzvXznCZm/7Uih27IGVaE0t4phydDFZwugJAicgoT+gXfHo9c1lMpl4+TUuQvyx7kLcio1I+da76fmXOuReReysuV4Srq5eMKVtcrJ4T9SZdyiHVIqNkoSYqOkVGy0JMTodc5FbyfERtsez1knygRxhWW/fIlhydDFZwugpAicgoT+KtY/8MeS08ywnDMNO6olxcvfujUq8NezxWKR8xlZppSBc0CVE2jpdYZt2ZHk85JyPrPQ9m05dNpcikPbq0FVvDWYMgGWiyArJtpcx0aJ7DsaIWc3/SplSsWa9axBmFk3JlriYyNzlsdESWQRsgn6xTrk4y15+lj7XZe/+9cr+YINUny2QHDKCrAsMYFTELDONbq3Q12Xc42Ufhno43p+u4LmGmlmJyfQiJaa5UoV+rrf7Dguz8z7odA29mxWVWqVTxBNHKVeyJLzF7LMdWqG3s60LdOgzXpbs17WfxRn0jPNxX1RMm//T26tGRcdmScQswZnJhsWEy0RYpEsi0W++vG4y8DUuuzJOd/L3pPnzLBmUqkY8341bNXriNz+tf5zNsvM8pwHrY/nPOPiY9bnWh+IcN5e7oPW5zi/nqvt2TZnvz2nx4uyvczMTPk9TczJp2NiMl2+XkQ+7XfenvnPoQ0R7rXf1fbczFTqPqaZpvw+W92KPt69aTWG7YAAsjgAs8QETkFg+oaDMnHpzz6fa+Tu6yoNOIr6uplZ2bmBVW6QdSHTdvu8q+W5y86mZcgvBw5JuUpVJD3TcjFQy3B8viX3W1KHJvWiWbaSOpOWKa8tLriOVuiKlue3rpZAVFAgprIs2ZIbpxd4cEWr5782QbWe6kgzlZERESaQ0m2Z27mBrHW5xlgRzrcj9Lk56+RccrKqEXlu67o56+Vsw/G27sAHD0TK1kU7JToqKp9tSO7r5bTL3M69r48533Zuv8v3kqf9jve1bRfbX3B/2L9+zrqu+yPnfTu2H1gcoFliAqcgMKBdbenetKrDr+fCKod78nXzO6qupPWjoqMiJVEv8TFFel5GRoYsWnRAeve+UmJiYvIdkkzLyDZBlwZSabmZrpygKmeZfWbs6x+PydYiDDVWTYyT2hUSTHBmyX29nOuL2SlxWGbJuc590PoccXrc/jFz1277Fx/Luz3re87z+MWmmGc7t9f2+m68XlZmlkRERdk2av8a9q/tD/Z959gIS5EDY70EjkhZdeyghKMCA79I620XwVfubdvz3QwkIyIscupkpMw9uUWioiKdnu8YCFuDaldBbH6Bo0Mgms82TCBtu53PNvJszzEQzhOIunr9QgJhhz7M73ZE3vZ7UiBniQmcgoAOuzkPvbWsVc5nr9u8ZpLUrpgQcOnSgug/Yuuk9IpurH/LlTXdHpZ8uV9z6dakaliVXsgJVhdJ79498w1WnZlgKp/AyiFQLCBwNMtyH7+4bt4g0fpEV4HjW9/8LDM3HXL7vfZtVUPu7VhPsi2W3ItIdnbOUK5uU5fpH3X727qOtknXsd3OXW62kXs7Zxs597Ny18vZRs56FrvbmVlZsvvnPVK/fgMTRWQXuL2c2znbyPnScb5te67de8rzXly0/+Lt3O1Zt5FdvO25y9reIj2pxCJlZ/JJH75eaIlyzlwWGviJpJ2Pkn/uWi3RkXqQUM42dN86m54px1LSCs0Sz/ruoM//HhM4wS0aHGlkH0gT9DxJ/9H1v6q2vLVsT6ET8HW9UHnf3mQ/P8tuFpfPjejRSO5oV1vu/+93cvJs/kVkK5WJlf8MvMr8IAiEoNgEq+m7pXePS90OVoNBnqDSPqjLJzjU2/ZBm32AmTeIKzjYzS+QvJCRKdu2bZPLW7SQiIgo2/LCgl2HNrsIJM02zGvnH+zm2YZzUOoiWC8oOL342vlvV9t/8b14JtjN0j7K+ZSLsEdEyO/p56W49Meuns/1H39uKr5C4AS3abDQvoE7+ZvgfX+aQdOx8winf/rWr319nKApuFgzpy/2bW4+W8nns9XHfZHJDXcaUEfrmFSA0UA15vBW6X1FzZAKVD3BPgjMtuQTEDoFhwUFu9ZA7UJGhqz59lv5U/sOEhkVZQtqP1p3QL784ZgEKgInwCmzphMOnYclqwXwsCTcw2cLFI8ZXvNC1jgjI0N+LStyZe1yDsFqw8plZPC1DYqUJfYlAicgzIYlwxmfLRD4qgR4lpjACQjDYclwxmcLBIfrAzRLTOAEAAAC0vUBmCUmcAIAAAErKsCyxFqfFgAAAG4gcAIAAHATgRMAAICbCJwAAADcROAEAADgJgInAAAANxE4AQAAuCns6jjpiQZVSkqKx861k5qaarbHiSF9gz73Pfrc9+hz36K/w7vPU3JjAmuMUJCwC5zOnDljrmvVquXvpgAAgACLEZKSkgpcJ8LiTngVQrKzs+XIkSNStmxZiYiI8EiUqkHYoUOHJDEx0SNtRMHoc9+jz32PPvct+ju8+9xisZigqUaNGhIZWfAsprDLOGmHXHLJJR7frn7o/v7gww197nv0ue/R575Ff4dvnycVkmmyYnI4AACAmwicAAAA3ETgVEJxcXEyevRocw3foM99jz73Pfrct+hv34sL0j4Pu8nhAAAAxUXGCQAAwE0ETgAAAG4icAIAAHATgVMJTZ48WerWrSvx8fHSrl072bhxo7+bFBLGjBljCpTaXxo3bmx7PC0tTYYOHSoVK1aUMmXKyC233CLHjx/3a5uDzapVq+SGG24wBd+0f+fPn+/wuE5/HDVqlFSvXl1KlSol3bp1k59//tlhnVOnTsmAAQNMDZZy5crJ/fffL2fPnvXxOwmdPr/nnnvy7PfXX3+9wzr0ufvGjRsnV111lSl4XKVKFenbt6/s2rXLYR13/pYcPHhQ+vTpIwkJCWY7Tz75pGRmZvr43YROn3fu3DnPfj548OCg6XMCpxKYNWuWjBgxwhwVsGXLFmnZsqX07NlTTpw44e+mhYRmzZrJ0aNHbZc1a9bYHhs+fLgsWLBAPv30U1m5cqWpBn/zzTf7tb3B5ty5c2af1eDflddee00mTZokU6ZMkQ0bNkjp0qXN/q1fNFb6Bf7jjz/KkiVL5IsvvjCBwYMPPujDdxFafa40ULLf72fMmOHwOH3uPv3boEHR+vXrTX/pudF69OhhPgd3/5ZkZWWZL/ALFy7I2rVr5b///a9MmzbN/KhA8fpcDRo0yGE/1783QdPnelQdiqdt27aWoUOH2u5nZWVZatSoYRk3bpxf2xUKRo8ebWnZsqXLx06fPm2JiYmxfPrpp7ZlO3bs0KNDLevWrfNhK0OH9t28efNs97Ozsy3VqlWzvP766w79HhcXZ5kxY4a5/9NPP5nnfffdd7Z1vvzyS0tERITl8OHDPn4Hwd/nauDAgZabbrop3+fQ5yVz4sQJ038rV650+2/JokWLLJGRkZZjx47Z1nn33XctiYmJlvT0dD+8i+Duc3Xttddahg0bZslPoPc5Gadi0kh48+bNZvjC/nQuen/dunV+bVuo0GEhHdKoX7+++ZWtqVul/a6/Yuz7XofxateuTd97yL59++TYsWMOfaynI9DhaGsf67UOFbVp08a2jq6v/w40Q4XiWbFihRmauOyyy2TIkCHy+++/2x6jz0smOTnZXFeoUMHtvyV6ffnll0vVqlVt62jmVc+zppk/FK3PraZPny6VKlWS5s2by8iRIyU1NdX2WKD3edidq85TTp48adKJ9h+s0vs7d+70W7tChX5Ba2pWvzw0jTt27Fi5+uqr5YcffjBf6LGxseYLxLnv9TGUnLUfXe3f1sf0Wr/g7UVHR5s/kHwOxaPDdDpMVK9ePfnll1/kmWeekV69epkvkqioKPq8hCd4/9vf/iYdO3Y0X9bKnb8leu3q34H1MRStz9Wdd94pderUMT+Mv//+e/n73/9u5kHNnTs3KPqcwAkBSb8srFq0aGECKf2HNnv2bDNRGQhFt99+u+22/uLWfb9BgwYmC9W1a1e/ti3Y6bwb/eFlP1cS/unzB+3m5Ol+rgeg6P6tPxZ0fw90DNUVk6YY9Reg89EXer9atWp+a1eo0l+EjRo1kj179pj+1aHS06dPO6xD33uOtR8L2r/12vlACD3qRY/64nPwDB2m1r81ut8r+rx4HnnkETORfvny5XLJJZfYlrvzt0SvXf07sD6GovW5K/rDWNnv54Hc5wROxaTp3datW8vSpUsd0pJ6v3379n5tWyjSw63114j+MtF+j4mJceh7TfPqHCj63jN0qEj/QNn3sc4v0Hk01j7Wa/3C0XkiVsuWLTP/Dqx/CFEyv/76q5njpPu9os+LRufg6xf4vHnzTD/pfm3Pnb8ler19+3aHgFWPFtNyEE2bNvXhuwmNPndl27Zt5tp+Pw/oPvf37PRgNnPmTHOU0bRp08zRLg8++KClXLlyDkcCoHgef/xxy4oVKyz79u2zfPvtt5Zu3bpZKlWqZI7QUIMHD7bUrl3bsmzZMsumTZss7du3Nxe478yZM5atW7eai/4pGD9+vLl94MAB8/grr7xi9ufPP//c8v3335ujverVq2c5f/68bRvXX3+95YorrrBs2LDBsmbNGsull15queOOO/z4roK3z/WxJ554whzNpfv9N998Y7nyyitNn6alpdm2QZ+7b8iQIZakpCTzt+To0aO2S2pqqm2dwv6WZGZmWpo3b27p0aOHZdu2bZbFixdbKleubBk5cqSf3lVw9/mePXsszz//vOlr3c/170v9+vUt11xzTdD0OYFTCb311lvmH11sbKwpT7B+/Xp/Nykk9O/f31K9enXTrzVr1jT39R+clX55P/zww5by5ctbEhISLP369TP/OOG+5cuXmy9v54seEm8tSfDcc89Zqlatan4gdO3a1bJr1y6Hbfz+++/mS7tMmTLmUOF7773XBAAoep/rF4t+UegXhB4iX6dOHcugQYPy/BCjz93nqq/1MnXq1CL9Ldm/f7+lV69ellKlSpkfcPrDLiMjww/vKPj7/ODBgyZIqlChgvm70rBhQ8uTTz5pSU5ODpo+j9D/+TvrBQAAEAyY4wQAAOAmAicAAAA3ETgBAAC4icAJAADATQROAAAAbiJwAgAAcBOBEwAAgJsInAAAANxE4ATAb6ZNm2ZO4FyQMWPGSKtWrQpc55577pG+fftKMAimtgLIi8AJgM+CgxUrVkhERITtbPT9+/eX3bt3SziZOHGiCRitOnfuLH/729/82iYA7osuwroA4FGlSpUyl3CSlJTk7yYAKAEyTgACaqjulVdekapVq0rZsmXl/vvvl7S0NIfHs7KyZMSIEeZ5FStWlKeeekpPVu6wTnZ2towbN07q1atnArOWLVvKnDlz8mS+li5dKm3atJGEhATp0KGD7Nq1K9+2OmfL1LZt28yy/fv3O7yfr776Spo0aSJlypSR66+/Xo4ePeoyG6e3V65cabJQuh3rtv744w8ZMGCAVK5c2bT/0ksvlalTpxa7nwF4DoETgIAxe/ZsM6fp5Zdflk2bNkn16tXlnXfecVjnn//8pwlQPvjgA1mzZo2cOnVK5s2b57COBk0ffvihTJkyRX788UcZPny4/PWvfzVBir1nn33WbE9fKzo6Wu67774Sv4fU1FR544035KOPPpJVq1bJwYMH5YknnnC5rgZM7du3l0GDBpngSi+1atWS5557Tn766Sf58ssvZceOHfLuu+9KpUqVStw2ACXHUB0Ar/jiiy9MxsU5W1SQCRMmmCyTXtSLL74o33zzjUPWSdcZOXKk3Hzzzea+Bkea4bFKT083gZc+T4MSVb9+fRNk/etf/5Jrr73Wtu5LL71ku//0009Lnz59zGvFx8cX+31nZGSYNjVo0MDcf+SRR+T555/Pd9guNjbWZLyqVatmW67B1hVXXGGyYapu3brFbg8AzyLjBMArunTpYoay7C/vv/9+gc/R7Eq7du0cllmDH5WcnGyyMvbraKbIGmCoPXv2mKxP9+7dTeBmvWgG6pdffnHYdosWLWy3NbulTpw4UYJ3LSYIsgZN1u0WdZtDhgyRmTNnmqMJdShy7dq1JWoTAM8h4wTAK0qXLi0NGzZ0WPbrr796/XXPnj1rrhcuXCg1a9Z0eCwuLs7hfkxMjO22zi+yzo9yJTIy53em/XwqzS45s9+mdbvOc7AK06tXLzlw4IAsWrRIlixZIl27dpWhQ4eaIUAA/kXGCUDA0AnVGzZscFi2fv16h6EtzeDYr5OZmSmbN2+23W/atKkJkHS4SwM3+4vOHyounait7Cd6axatpHSoztUQpr7ewIED5eOPPzbDk++9916JXwtAyZFxAhAwhg0bZo4006G3jh07yvTp083kbp2jZL+OHnmnR5o1btxYxo8f73Ckmx6Np5OxdUK4Zo86depkhvi+/fZbSUxMNMFIcVgDL528rnOjtP6UTiwvKZ2/pIGgHk2nQ4oVKlQwr9G6dWtp1qyZmbOl88U0qATgf2ScAAQMLYipR5TpvB4NHHS4Suf72Hv88cflrrvuMgGQzn/SQKlfv34O67zwwgtmO3p0nQYcWhJAh+60PEFx6RDcjBkzZOfOnWZu1Kuvvmomr5eUBnlRUVEmU6ZZJs2UaRZKJ8Dr61xzzTXmcZ3zBMD/IixFHXwHAAAIU2ScAAAA3ETgBAAA4CYCJwAAADcROAEAALiJwAkAAMBNBE4AAABuInACAABwE4ETAACAmwicAAAA3ETgBAAA4CYCJwAAADcROAEAAIh7/h9f180gEg8JmgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load results\n",
    "with open(\"results/hidden_unit_search_results_20250930_134929.json\", \"r\") as f:\n",
    "    search_results = json.load(f)\n",
    "\n",
    "datasets = list(search_results.keys())\n",
    "\n",
    "for i, ds in enumerate(datasets):\n",
    "    results = search_results[ds][\"results\"]\n",
    "    hidden_sizes = [r[\"hidden_dim\"] for r in results]\n",
    "    mean_vals = [r[\"mean_val\"] for r in results]\n",
    "    std_vals = [r[\"std_val\"] for r in results]\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.errorbar(hidden_sizes, mean_vals, yerr=std_vals, fmt='-o', capsize=4)\n",
    "    plt.title(f\"{ds.capitalize()} validation loss vs hidden size\")\n",
    "    plt.xlabel(\"Hidden units\")\n",
    "    plt.ylabel(\"Mean validation loss\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2872aeda",
   "metadata": {},
   "source": [
    "### Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "b0b12d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_tuning(\n",
    "    dataset_name,\n",
    "    param_grid,\n",
    "    hidden_dim,\n",
    "    optimizer_name=\"sgd\",  # \"sgd\", \"scg\", or \"lfrog\"\n",
    "    classification=True,\n",
    "    n_runs=3,\n",
    "    epochs=50,\n",
    "    batch_size=32\n",
    "):\n",
    "    X_train, y_train = datasets[dataset_name][\"train\"]\n",
    "    X_val, y_val = datasets[dataset_name][\"val\"]\n",
    "    output_dim = len(np.unique(y_train)) if classification else 1\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Select the training function\n",
    "    if optimizer_name == \"sgd\":\n",
    "        train_fn = train_sgd\n",
    "    elif optimizer_name == \"scg\":\n",
    "        train_fn = train_scg\n",
    "    elif optimizer_name == \"lfrog\":\n",
    "        train_fn = train_lfrog\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "\n",
    "    # Use R seed list for reproducibility\n",
    "    seeds = R[:n_runs] if 'R' in globals() else list(range(n_runs))\n",
    "\n",
    "\n",
    "    for params in param_grid:\n",
    "        val_losses = []\n",
    "        accs, f1s, mses, r2s = [], [], [], []\n",
    "        times = []\n",
    "\n",
    "        for seed in seeds:\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "            model = FeedforwardNN(\n",
    "                input_dim=input_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                output_dim=output_dim\n",
    "            )\n",
    "\n",
    "            if classification:\n",
    "                y_train_run, y_val_run = y_train, y_val\n",
    "            else:\n",
    "                y_train_run = np.array(y_train).reshape(-1, 1).astype(np.float32)\n",
    "                y_val_run = np.array(y_val).reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "            extra_kwargs = {k: v for k, v in params.items() if k != \"hidden_dim\"}\n",
    "\n",
    "            start_time = time.time()\n",
    "            if optimizer_name == \"sgd\":\n",
    "                _, val_loss_curve = train_fn(\n",
    "                    model,\n",
    "                    X_train,\n",
    "                    y_train_run,\n",
    "                    X_val,\n",
    "                    y_val_run,\n",
    "                    epochs=epochs,\n",
    "                    lr=extra_kwargs.get(\"lr\", 0.01),\n",
    "                    batch_size=batch_size,\n",
    "                    classification=classification,\n",
    "                    momentum=extra_kwargs.get(\"momentum\", 0.0)\n",
    "                )\n",
    "            elif optimizer_name == \"scg\":\n",
    "                # Convert data to torch tensors\n",
    "                X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "                y_train_tensor = torch.tensor(y_train_run, dtype=torch.long if classification else torch.float32)\n",
    "                X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "                y_val_tensor = torch.tensor(y_val_run, dtype=torch.long if classification else torch.float32)\n",
    "                _, val_loss_curve = train_fn(\n",
    "                    model,\n",
    "                    X_train_tensor,\n",
    "                    y_train_tensor,\n",
    "                    X_val_tensor,\n",
    "                    y_val_tensor,\n",
    "                    max_epochs=epochs,\n",
    "                    **extra_kwargs\n",
    "                )\n",
    "            elif optimizer_name == \"lfrog\":\n",
    "                # Convert data to torch tensors\n",
    "                X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "                y_train_tensor = torch.tensor(y_train_run, dtype=torch.long if classification else torch.float32)\n",
    "                X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "                y_val_tensor = torch.tensor(y_val_run, dtype=torch.long if classification else torch.float32)\n",
    "                # Choose loss_fn\n",
    "                loss_fn = nn.CrossEntropyLoss() if classification else nn.MSELoss()\n",
    "                _, val_loss_curve = train_fn(\n",
    "                    model,\n",
    "                    X_train_tensor,\n",
    "                    y_train_tensor,\n",
    "                    X_val_tensor,\n",
    "                    y_val_tensor,\n",
    "                    epochs=epochs,\n",
    "                    loss_fn=loss_fn,\n",
    "                    **extra_kwargs\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed = end_time - start_time\n",
    "            times.append(elapsed)\n",
    "            val_losses.append(val_loss_curve[-1])\n",
    "\n",
    "            # --- metrics ---\n",
    "            model.eval()\n",
    "            X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(X_val_tensor)\n",
    "                if classification:\n",
    "                    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "                    accs.append(accuracy_score(y_val, preds))\n",
    "                    f1s.append(f1_score(y_val, preds, average=\"weighted\"))\n",
    "                else:\n",
    "                    preds = outputs.cpu().numpy().flatten()\n",
    "                    # Check for NaNs before appending metrics\n",
    "                    if (\n",
    "                        np.isnan(y_val_run.flatten()).any()\n",
    "                        or np.isnan(preds).any()\n",
    "                    ):\n",
    "                        print(f\"Warning: NaNs detected in validation targets or predictions for {dataset_name} (seed={seed}). Skipping metrics for this run.\")\n",
    "                        print(\"y_val_run:\", y_val_run.flatten())\n",
    "                        print(\"preds:\", preds)\n",
    "                    else:\n",
    "                        mses.append(mean_squared_error(y_val_run.flatten(), preds))\n",
    "                        r2s.append(r2_score(y_val_run.flatten(), preds))\n",
    "\n",
    "        results.append({\n",
    "            \"optimizer\": optimizer_name,\n",
    "            \"params\": params,\n",
    "            \"mean_val_loss\": float(np.mean(val_losses)),\n",
    "            \"std_val_loss\": float(np.std(val_losses)),\n",
    "            \"mean_acc\": float(np.mean(accs)) if accs else None,\n",
    "            \"std_acc\": float(np.std(accs)) if accs else None,\n",
    "            \"mean_f1\": float(np.mean(f1s)) if f1s else None,\n",
    "            \"std_f1\": float(np.std(f1s)) if f1s else None,\n",
    "            \"mean_mse\": float(np.mean(mses)) if mses else None,\n",
    "            \"std_mse\": float(np.std(mses)) if mses else None,\n",
    "            \"mean_r2\": float(np.mean(r2s)) if r2s else None,\n",
    "            \"std_r2\": float(np.std(r2s)) if r2s else None,\n",
    "            \"mean_time\": float(np.mean(times)),\n",
    "            \"std_time\": float(np.std(times)),\n",
    "            \"all_times\": [float(t) for t in times]\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "c2e13534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial - Train 0.332554 Test 0.333872\n",
      "Epoch    0 f=0.218495 test=0.198538 ||g||=9.67e-01 r=1.003 λ=5.00e-05 α=7.417e-02\n",
      "Epoch   10 f=0.224030 test=0.199490 ||g||=8.00e-01 r=-0.557 λ=5.00e-05 α=5.061e-01\n",
      "Epoch   20 f=0.224030 test=0.199490 ||g||=8.00e-01 r=-0.557 λ=5.12e-02 α=5.061e-01\n",
      "Epoch   30 f=0.224030 test=0.199490 ||g||=8.00e-01 r=-0.557 λ=5.24e+01 α=5.061e-01\n",
      "Epoch   40 f=0.224030 test=0.199490 ||g||=8.00e-01 r=-0.557 λ=5.37e+04 α=5.061e-01\n",
      "Epoch   49 f=0.224030 test=0.199490 ||g||=8.00e-01 r=-0.557 λ=2.75e+07 α=5.061e-01\n",
      "Final - Train 0.224030 Test 0.199490\n",
      "Initial - Train 0.472124 Test 0.426829\n",
      "Epoch    0 f=0.207522 test=0.206309 ||g||=8.27e-01 r=1.001 λ=5.00e-05 α=7.622e-02\n",
      "Epoch   10 f=0.055242 test=0.049043 ||g||=8.30e-01 r=0.956 λ=9.77e-08 α=2.352e-02\n",
      "Epoch   20 f=0.013100 test=0.014017 ||g||=2.46e-02 r=1.000 λ=9.54e-11 α=7.976e-02\n",
      "Epoch   30 f=0.012818 test=0.013702 ||g||=2.94e-02 r=-0.196 λ=1.53e-09 α=1.000e+00\n",
      "Epoch   40 f=0.012818 test=0.013702 ||g||=2.94e-02 r=-0.196 λ=1.56e-06 α=1.000e+00\n",
      "Epoch   49 f=0.012818 test=0.013702 ||g||=2.94e-02 r=-0.196 λ=8.00e-04 α=1.000e+00\n",
      "Final - Train 0.012818 Test 0.013702\n",
      "Initial - Train 0.389784 Test 0.347626\n",
      "Epoch    0 f=0.202349 test=0.201786 ||g||=7.36e-01 r=0.998 λ=5.00e-05 α=7.887e-02\n",
      "Epoch   10 f=0.015752 test=0.015471 ||g||=1.25e-01 r=1.022 λ=9.77e-08 α=9.008e-02\n",
      "Epoch   20 f=0.012700 test=0.012420 ||g||=3.97e-02 r=1.027 λ=9.54e-11 α=1.592e-01\n",
      "Epoch   30 f=0.011644 test=0.012069 ||g||=3.78e-02 r=1.066 λ=9.31e-14 α=1.090e-01\n",
      "Epoch   40 f=0.012415 test=0.013335 ||g||=3.16e-02 r=-4.161 λ=5.82e-15 α=1.000e+00\n",
      "Epoch   49 f=0.012415 test=0.013335 ||g||=3.16e-02 r=-4.161 λ=2.98e-12 α=1.000e+00\n",
      "Final - Train 0.012415 Test 0.013335\n",
      "Initial - Train 0.332554 Test 0.333872\n",
      "Epoch    0 f=0.218495 test=0.198538 ||g||=9.67e-01 r=1.003 λ=5.00e-06 α=7.417e-02\n",
      "Epoch   10 f=0.224030 test=0.199490 ||g||=8.00e-01 r=-0.557 λ=5.00e-06 α=5.061e-01\n",
      "Epoch   20 f=0.224030 test=0.199490 ||g||=8.00e-01 r=-0.557 λ=5.12e-03 α=5.061e-01\n",
      "Epoch   30 f=0.224030 test=0.199490 ||g||=8.00e-01 r=-0.557 λ=5.24e+00 α=5.061e-01\n",
      "Epoch   40 f=0.224030 test=0.199490 ||g||=8.00e-01 r=-0.557 λ=5.37e+03 α=5.061e-01\n",
      "Epoch   49 f=0.224030 test=0.199490 ||g||=8.00e-01 r=-0.557 λ=2.75e+06 α=5.061e-01\n",
      "Final - Train 0.224030 Test 0.199490\n",
      "Initial - Train 0.472124 Test 0.426829\n",
      "Epoch    0 f=0.207522 test=0.206309 ||g||=8.27e-01 r=1.001 λ=5.00e-06 α=7.622e-02\n",
      "Epoch   10 f=0.055242 test=0.049043 ||g||=8.30e-01 r=0.956 λ=9.77e-09 α=2.352e-02\n",
      "Epoch   20 f=0.013100 test=0.014017 ||g||=2.46e-02 r=1.000 λ=9.54e-12 α=7.976e-02\n",
      "Epoch   30 f=0.012818 test=0.013702 ||g||=2.94e-02 r=-0.196 λ=1.53e-10 α=1.000e+00\n",
      "Epoch   40 f=0.012818 test=0.013702 ||g||=2.94e-02 r=-0.196 λ=1.56e-07 α=1.000e+00\n",
      "Epoch   49 f=0.012818 test=0.013702 ||g||=2.94e-02 r=-0.196 λ=8.00e-05 α=1.000e+00\n",
      "Final - Train 0.012818 Test 0.013702\n",
      "Initial - Train 0.389784 Test 0.347626\n",
      "Epoch    0 f=0.202349 test=0.201786 ||g||=7.36e-01 r=0.998 λ=5.00e-06 α=7.887e-02\n",
      "Epoch   10 f=0.015752 test=0.015471 ||g||=1.25e-01 r=1.022 λ=9.77e-09 α=9.008e-02\n",
      "Epoch   20 f=0.012700 test=0.012420 ||g||=3.97e-02 r=1.027 λ=9.54e-12 α=1.592e-01\n",
      "Epoch   30 f=0.011644 test=0.012069 ||g||=3.78e-02 r=1.066 λ=9.31e-15 α=1.090e-01\n",
      "Epoch   40 f=0.012415 test=0.013335 ||g||=3.16e-02 r=-4.161 λ=5.82e-16 α=1.000e+00\n",
      "Epoch   49 f=0.012415 test=0.013335 ||g||=3.16e-02 r=-4.161 λ=2.98e-13 α=1.000e+00\n",
      "Final - Train 0.012415 Test 0.013335\n",
      "Initial - Train 0.332554 Test 0.333872\n",
      "Epoch    0 f=0.218495 test=0.198538 ||g||=9.67e-01 r=1.003 λ=5.00e-07 α=7.417e-02\n",
      "Epoch   10 f=0.224030 test=0.199490 ||g||=8.00e-01 r=-0.557 λ=5.00e-07 α=5.061e-01\n",
      "Epoch   20 f=0.224030 test=0.199490 ||g||=8.00e-01 r=-0.557 λ=5.12e-04 α=5.061e-01\n",
      "Epoch   30 f=0.224030 test=0.199490 ||g||=8.00e-01 r=-0.557 λ=5.24e-01 α=5.061e-01\n",
      "Epoch   40 f=0.224030 test=0.199490 ||g||=8.00e-01 r=-0.557 λ=5.37e+02 α=5.061e-01\n",
      "Epoch   49 f=0.224030 test=0.199490 ||g||=8.00e-01 r=-0.557 λ=2.75e+05 α=5.061e-01\n",
      "Final - Train 0.224030 Test 0.199490\n",
      "Initial - Train 0.472124 Test 0.426829\n",
      "Epoch    0 f=0.207522 test=0.206309 ||g||=8.27e-01 r=1.001 λ=5.00e-07 α=7.622e-02\n",
      "Epoch   10 f=0.055242 test=0.049043 ||g||=8.30e-01 r=0.956 λ=9.77e-10 α=2.352e-02\n",
      "Epoch   20 f=0.013100 test=0.014017 ||g||=2.46e-02 r=1.000 λ=9.54e-13 α=7.976e-02\n",
      "Epoch   30 f=0.012818 test=0.013702 ||g||=2.94e-02 r=-0.196 λ=1.53e-11 α=1.000e+00\n",
      "Epoch   40 f=0.012818 test=0.013702 ||g||=2.94e-02 r=-0.196 λ=1.56e-08 α=1.000e+00\n",
      "Epoch   49 f=0.012818 test=0.013702 ||g||=2.94e-02 r=-0.196 λ=8.00e-06 α=1.000e+00\n",
      "Final - Train 0.012818 Test 0.013702\n",
      "Initial - Train 0.389784 Test 0.347626\n",
      "Epoch    0 f=0.202349 test=0.201786 ||g||=7.36e-01 r=0.998 λ=5.00e-07 α=7.887e-02\n",
      "Epoch   10 f=0.015752 test=0.015471 ||g||=1.25e-01 r=1.022 λ=9.77e-10 α=9.008e-02\n",
      "Epoch   20 f=0.012700 test=0.012420 ||g||=3.97e-02 r=1.027 λ=9.54e-13 α=1.592e-01\n",
      "Epoch   30 f=0.011644 test=0.012069 ||g||=3.78e-02 r=1.066 λ=9.31e-16 α=1.090e-01\n",
      "Epoch   40 f=0.012415 test=0.013335 ||g||=3.16e-02 r=-4.161 λ=5.82e-17 α=1.000e+00\n",
      "Epoch   49 f=0.012415 test=0.013335 ||g||=3.16e-02 r=-4.161 λ=2.98e-14 α=1.000e+00\n",
      "Final - Train 0.012415 Test 0.013335\n",
      "Initial - Train 0.332554 Test 0.333872\n",
      "Epoch    0 f=0.218495 test=0.198538 ||g||=9.67e-01 r=1.003 λ=5.00e-08 α=7.417e-02\n",
      "Epoch   10 f=0.224030 test=0.199490 ||g||=8.00e-01 r=-0.557 λ=5.00e-08 α=5.061e-01\n",
      "Epoch   20 f=0.224030 test=0.199490 ||g||=8.00e-01 r=-0.557 λ=5.12e-05 α=5.061e-01\n",
      "Epoch   30 f=0.224030 test=0.199490 ||g||=8.00e-01 r=-0.557 λ=5.24e-02 α=5.061e-01\n",
      "Epoch   40 f=0.224030 test=0.199490 ||g||=8.00e-01 r=-0.557 λ=5.37e+01 α=5.061e-01\n",
      "Epoch   49 f=0.224030 test=0.199490 ||g||=8.00e-01 r=-0.557 λ=2.75e+04 α=5.061e-01\n",
      "Final - Train 0.224030 Test 0.199490\n",
      "Initial - Train 0.472124 Test 0.426829\n",
      "Epoch    0 f=0.207522 test=0.206309 ||g||=8.27e-01 r=1.001 λ=5.00e-08 α=7.622e-02\n",
      "Epoch   10 f=0.055242 test=0.049043 ||g||=8.30e-01 r=0.956 λ=9.77e-11 α=2.352e-02\n",
      "Epoch   20 f=0.013100 test=0.014017 ||g||=2.46e-02 r=1.000 λ=9.54e-14 α=7.976e-02\n",
      "Epoch   30 f=0.012818 test=0.013702 ||g||=2.94e-02 r=-0.196 λ=1.53e-12 α=1.000e+00\n",
      "Epoch   40 f=0.012818 test=0.013702 ||g||=2.94e-02 r=-0.196 λ=1.56e-09 α=1.000e+00\n",
      "Epoch   49 f=0.012818 test=0.013702 ||g||=2.94e-02 r=-0.196 λ=8.00e-07 α=1.000e+00\n",
      "Final - Train 0.012818 Test 0.013702\n",
      "Initial - Train 0.389784 Test 0.347626\n",
      "Epoch    0 f=0.202349 test=0.201786 ||g||=7.36e-01 r=0.998 λ=5.00e-08 α=7.887e-02\n",
      "Epoch   10 f=0.015752 test=0.015471 ||g||=1.25e-01 r=1.022 λ=9.77e-11 α=9.008e-02\n",
      "Epoch   20 f=0.012700 test=0.012420 ||g||=3.97e-02 r=1.027 λ=9.54e-14 α=1.592e-01\n",
      "Epoch   30 f=0.011644 test=0.012069 ||g||=3.78e-02 r=1.066 λ=9.31e-17 α=1.090e-01\n",
      "Epoch   40 f=0.012415 test=0.013335 ||g||=3.16e-02 r=-4.161 λ=5.82e-18 α=1.000e+00\n",
      "Epoch   49 f=0.012415 test=0.013335 ||g||=3.16e-02 r=-4.161 λ=2.98e-15 α=1.000e+00\n",
      "Final - Train 0.012415 Test 0.013335\n",
      "Initial - Train 0.332554 Test 0.333872\n",
      "Epoch    0 f=0.218495 test=0.198539 ||g||=9.67e-01 r=1.003 λ=5.00e-05 α=7.417e-02\n",
      "Epoch   10 f=0.041282 test=0.044658 ||g||=2.42e-01 r=0.995 λ=9.77e-08 α=4.474e-01\n",
      "Epoch   20 f=0.014274 test=0.014651 ||g||=4.27e-02 r=1.007 λ=9.54e-11 α=1.048e-01\n",
      "Epoch   30 f=0.012683 test=0.013113 ||g||=4.67e-02 r=0.998 λ=1.86e-13 α=2.527e-01\n",
      "Epoch   40 f=0.011032 test=0.011690 ||g||=2.68e-02 r=1.068 λ=3.64e-16 α=5.227e-02\n",
      "Epoch   49 f=0.010691 test=0.011435 ||g||=1.58e-02 r=1.000 λ=7.11e-19 α=7.522e-02\n",
      "Final - Train 0.010691 Test 0.011435\n",
      "Initial - Train 0.472124 Test 0.426829\n",
      "Epoch    0 f=0.207522 test=0.206307 ||g||=8.27e-01 r=1.001 λ=5.00e-05 α=7.622e-02\n",
      "Epoch   10 f=0.056275 test=0.050346 ||g||=8.76e-01 r=0.961 λ=9.77e-08 α=1.296e-02\n",
      "Epoch   20 f=0.038723 test=0.034744 ||g||=1.92e-01 r=-0.439 λ=3.13e-06 α=1.000e+00\n",
      "Epoch   30 f=0.012826 test=0.013949 ||g||=2.95e-02 r=1.001 λ=2.56e-02 α=7.504e-02\n",
      "Epoch   40 f=0.012132 test=0.013202 ||g||=2.83e-02 r=0.989 λ=5.00e-05 α=1.000e+00\n",
      "Epoch   49 f=0.011023 test=0.011898 ||g||=2.29e-02 r=1.078 λ=9.77e-08 α=6.830e-01\n",
      "Final - Train 0.011023 Test 0.011898\n",
      "Initial - Train 0.389784 Test 0.347626\n",
      "Epoch    0 f=0.202349 test=0.201787 ||g||=7.36e-01 r=0.998 λ=5.00e-05 α=7.887e-02\n",
      "Epoch   10 f=0.014813 test=0.014342 ||g||=9.23e-02 r=1.139 λ=4.88e-08 α=7.414e-01\n",
      "Epoch   20 f=0.011767 test=0.012281 ||g||=4.01e-02 r=0.974 λ=3.81e-10 α=2.573e-01\n",
      "Epoch   30 f=0.010823 test=0.011651 ||g||=7.43e-03 r=1.040 λ=3.73e-13 α=1.221e-01\n",
      "Epoch   40 f=0.011448 test=0.011570 ||g||=4.74e-02 r=-2.470 λ=5.96e-12 α=2.919e-01\n",
      "Epoch   49 f=0.011448 test=0.011570 ||g||=4.74e-02 r=-2.470 λ=3.05e-09 α=2.919e-01\n",
      "Final - Train 0.011448 Test 0.011570\n",
      "Initial - Train 0.332554 Test 0.333872\n",
      "Epoch    0 f=0.218495 test=0.198539 ||g||=9.67e-01 r=1.003 λ=5.00e-06 α=7.417e-02\n",
      "Epoch   10 f=0.041282 test=0.044658 ||g||=2.42e-01 r=0.995 λ=9.77e-09 α=4.474e-01\n",
      "Epoch   20 f=0.014274 test=0.014651 ||g||=4.27e-02 r=1.007 λ=9.54e-12 α=1.048e-01\n",
      "Epoch   30 f=0.012683 test=0.013113 ||g||=4.67e-02 r=0.998 λ=1.86e-14 α=2.527e-01\n",
      "Epoch   40 f=0.011032 test=0.011690 ||g||=2.68e-02 r=1.068 λ=3.64e-17 α=5.227e-02\n",
      "Epoch   49 f=0.010691 test=0.011435 ||g||=1.58e-02 r=1.000 λ=7.11e-20 α=7.522e-02\n",
      "Final - Train 0.010691 Test 0.011435\n",
      "Initial - Train 0.472124 Test 0.426829\n",
      "Epoch    0 f=0.207522 test=0.206307 ||g||=8.27e-01 r=1.001 λ=5.00e-06 α=7.622e-02\n",
      "Epoch   10 f=0.056275 test=0.050346 ||g||=8.76e-01 r=0.961 λ=9.77e-09 α=1.296e-02\n",
      "Epoch   20 f=0.038723 test=0.034744 ||g||=1.92e-01 r=-0.439 λ=3.13e-07 α=1.000e+00\n",
      "Epoch   30 f=0.013630 test=0.014646 ||g||=1.27e-01 r=1.012 λ=8.19e-02 α=4.477e-01\n",
      "Epoch   40 f=0.011501 test=0.012172 ||g||=1.30e-02 r=1.049 λ=8.00e-05 α=1.000e+00\n",
      "Epoch   49 f=0.011095 test=0.011827 ||g||=2.57e-02 r=1.031 λ=1.56e-07 α=2.731e-01\n",
      "Final - Train 0.011095 Test 0.011827\n",
      "Initial - Train 0.389784 Test 0.347626\n",
      "Epoch    0 f=0.202349 test=0.201787 ||g||=7.36e-01 r=0.998 λ=5.00e-06 α=7.887e-02\n",
      "Epoch   10 f=0.014813 test=0.014342 ||g||=9.23e-02 r=1.139 λ=4.88e-09 α=7.414e-01\n",
      "Epoch   20 f=0.011767 test=0.012281 ||g||=4.01e-02 r=0.974 λ=3.81e-11 α=2.573e-01\n",
      "Epoch   30 f=0.010823 test=0.011651 ||g||=7.43e-03 r=1.040 λ=3.73e-14 α=1.221e-01\n",
      "Epoch   40 f=0.011448 test=0.011570 ||g||=4.74e-02 r=-2.470 λ=5.96e-13 α=2.919e-01\n",
      "Epoch   49 f=0.011448 test=0.011570 ||g||=4.74e-02 r=-2.470 λ=3.05e-10 α=2.919e-01\n",
      "Final - Train 0.011448 Test 0.011570\n",
      "Initial - Train 0.332554 Test 0.333872\n",
      "Epoch    0 f=0.218495 test=0.198539 ||g||=9.67e-01 r=1.003 λ=5.00e-07 α=7.417e-02\n",
      "Epoch   10 f=0.041282 test=0.044658 ||g||=2.42e-01 r=0.995 λ=9.77e-10 α=4.474e-01\n",
      "Epoch   20 f=0.014274 test=0.014651 ||g||=4.27e-02 r=1.007 λ=9.54e-13 α=1.048e-01\n",
      "Epoch   30 f=0.012683 test=0.013113 ||g||=4.67e-02 r=0.998 λ=1.86e-15 α=2.527e-01\n",
      "Epoch   40 f=0.011032 test=0.011690 ||g||=2.68e-02 r=1.068 λ=3.64e-18 α=5.227e-02\n",
      "Epoch   49 f=0.010691 test=0.011435 ||g||=1.58e-02 r=1.000 λ=7.11e-21 α=7.522e-02\n",
      "Final - Train 0.010691 Test 0.011435\n",
      "Initial - Train 0.472124 Test 0.426829\n",
      "Epoch    0 f=0.207522 test=0.206307 ||g||=8.27e-01 r=1.001 λ=5.00e-07 α=7.622e-02\n",
      "Epoch   10 f=0.056275 test=0.050346 ||g||=8.76e-01 r=0.961 λ=9.77e-10 α=1.296e-02\n",
      "Epoch   20 f=0.038723 test=0.034744 ||g||=1.92e-01 r=-0.439 λ=3.12e-08 α=1.000e+00\n",
      "Epoch   30 f=0.038723 test=0.034744 ||g||=1.92e-01 r=-0.494 λ=3.28e-02 α=1.000e+00\n",
      "Epoch   40 f=0.012608 test=0.013803 ||g||=3.63e-02 r=0.984 λ=5.12e-04 α=8.792e-02\n",
      "Epoch   49 f=0.011688 test=0.012868 ||g||=5.77e-02 r=1.109 λ=1.00e-06 α=1.000e+00\n",
      "Final - Train 0.011688 Test 0.012868\n",
      "Initial - Train 0.389784 Test 0.347626\n",
      "Epoch    0 f=0.202349 test=0.201787 ||g||=7.36e-01 r=0.998 λ=5.00e-07 α=7.887e-02\n",
      "Epoch   10 f=0.014813 test=0.014342 ||g||=9.23e-02 r=1.139 λ=4.88e-10 α=7.414e-01\n",
      "Epoch   20 f=0.011767 test=0.012281 ||g||=4.01e-02 r=0.974 λ=3.81e-12 α=2.573e-01\n",
      "Epoch   30 f=0.010823 test=0.011651 ||g||=7.43e-03 r=1.040 λ=3.73e-15 α=1.221e-01\n",
      "Epoch   40 f=0.011448 test=0.011570 ||g||=4.74e-02 r=-2.470 λ=5.96e-14 α=2.919e-01\n",
      "Epoch   49 f=0.011448 test=0.011570 ||g||=4.74e-02 r=-2.470 λ=3.05e-11 α=2.919e-01\n",
      "Final - Train 0.011448 Test 0.011570\n",
      "Initial - Train 0.332554 Test 0.333872\n",
      "Epoch    0 f=0.218495 test=0.198539 ||g||=9.67e-01 r=1.003 λ=5.00e-08 α=7.417e-02\n",
      "Epoch   10 f=0.041282 test=0.044658 ||g||=2.42e-01 r=0.995 λ=9.77e-11 α=4.474e-01\n",
      "Epoch   20 f=0.014274 test=0.014651 ||g||=4.27e-02 r=1.007 λ=9.54e-14 α=1.048e-01\n",
      "Epoch   30 f=0.012683 test=0.013113 ||g||=4.67e-02 r=0.998 λ=1.86e-16 α=2.527e-01\n",
      "Epoch   40 f=0.011032 test=0.011690 ||g||=2.68e-02 r=1.068 λ=3.64e-19 α=5.227e-02\n",
      "Epoch   49 f=0.010691 test=0.011435 ||g||=1.58e-02 r=1.000 λ=7.11e-22 α=7.522e-02\n",
      "Final - Train 0.010691 Test 0.011435\n",
      "Initial - Train 0.472124 Test 0.426829\n",
      "Epoch    0 f=0.207522 test=0.206307 ||g||=8.27e-01 r=1.001 λ=5.00e-08 α=7.622e-02\n",
      "Epoch   10 f=0.056275 test=0.050346 ||g||=8.76e-01 r=0.961 λ=9.77e-11 α=1.296e-02\n",
      "Epoch   20 f=0.038723 test=0.034744 ||g||=1.92e-01 r=-0.439 λ=3.12e-09 α=1.000e+00\n",
      "Epoch   30 f=0.038723 test=0.034744 ||g||=1.92e-01 r=-0.444 λ=3.28e-03 α=1.000e+00\n",
      "Epoch   40 f=0.012220 test=0.012966 ||g||=2.29e-02 r=1.006 λ=3.28e-03 α=1.486e-01\n",
      "Epoch   49 f=0.011245 test=0.011827 ||g||=1.28e-02 r=1.001 λ=6.40e-06 α=7.174e-02\n",
      "Final - Train 0.011245 Test 0.011827\n",
      "Initial - Train 0.389784 Test 0.347626\n",
      "Epoch    0 f=0.202349 test=0.201787 ||g||=7.36e-01 r=0.998 λ=5.00e-08 α=7.887e-02\n",
      "Epoch   10 f=0.014813 test=0.014342 ||g||=9.23e-02 r=1.139 λ=4.88e-11 α=7.414e-01\n",
      "Epoch   20 f=0.011767 test=0.012281 ||g||=4.01e-02 r=0.974 λ=3.81e-13 α=2.573e-01\n",
      "Epoch   30 f=0.010823 test=0.011651 ||g||=7.43e-03 r=1.040 λ=3.73e-16 α=1.221e-01\n",
      "Epoch   40 f=0.011448 test=0.011570 ||g||=4.74e-02 r=-2.470 λ=5.96e-15 α=2.919e-01\n",
      "Epoch   49 f=0.011448 test=0.011570 ||g||=4.74e-02 r=-2.470 λ=3.05e-12 α=2.919e-01\n",
      "Final - Train 0.011448 Test 0.011570\n",
      "Initial - Train 0.332554 Test 0.333872\n",
      "Epoch    0 f=0.218495 test=0.198538 ||g||=9.67e-01 r=1.003 λ=5.00e-05 α=7.417e-02\n",
      "Epoch   10 f=0.179527 test=0.160307 ||g||=7.86e-01 r=-0.280 λ=5.00e-05 α=5.034e-01\n",
      "Epoch   20 f=0.179527 test=0.160307 ||g||=7.86e-01 r=-0.280 λ=5.12e-02 α=5.034e-01\n",
      "Epoch   30 f=0.179527 test=0.160307 ||g||=7.86e-01 r=-0.280 λ=5.24e+01 α=5.034e-01\n",
      "Epoch   40 f=0.179527 test=0.160307 ||g||=7.86e-01 r=-0.280 λ=5.37e+04 α=5.034e-01\n",
      "Epoch   49 f=0.179527 test=0.160307 ||g||=7.86e-01 r=-0.280 λ=2.75e+07 α=5.034e-01\n",
      "Final - Train 0.179527 Test 0.160307\n",
      "Initial - Train 0.472124 Test 0.426829\n",
      "Epoch    0 f=0.207522 test=0.206306 ||g||=8.27e-01 r=1.001 λ=5.00e-05 α=7.622e-02\n",
      "Epoch   10 f=0.054755 test=0.049057 ||g||=8.50e-01 r=0.965 λ=9.77e-08 α=1.540e-02\n",
      "Epoch   20 f=0.012715 test=0.013205 ||g||=5.82e-02 r=1.010 λ=9.54e-11 α=8.487e-02\n",
      "Epoch   30 f=0.014889 test=0.015786 ||g||=4.69e-02 r=-1.406 λ=1.49e-12 α=1.000e+00\n",
      "Epoch   40 f=0.014889 test=0.015786 ||g||=4.69e-02 r=-1.406 λ=1.56e-06 α=1.000e+00\n",
      "Epoch   49 f=0.011656 test=0.012132 ||g||=5.67e-02 r=0.331 λ=2.05e-01 α=3.036e-01\n",
      "Final - Train 0.011656 Test 0.012132\n",
      "Initial - Train 0.389784 Test 0.347626\n",
      "Epoch    0 f=0.202349 test=0.201787 ||g||=7.36e-01 r=0.998 λ=5.00e-05 α=7.887e-02\n",
      "Epoch   10 f=0.015428 test=0.014531 ||g||=1.19e-01 r=1.071 λ=9.77e-08 α=1.951e-01\n",
      "Epoch   20 f=0.012629 test=0.012173 ||g||=4.22e-02 r=1.013 λ=9.54e-11 α=1.242e-01\n",
      "Epoch   30 f=0.011678 test=0.012032 ||g||=2.70e-02 r=1.220 λ=9.31e-14 α=1.590e-01\n",
      "Epoch   40 f=0.010813 test=0.011883 ||g||=2.36e-02 r=1.045 λ=9.09e-17 α=1.154e-01\n",
      "Epoch   49 f=0.010483 test=0.011640 ||g||=2.02e-02 r=1.066 λ=1.78e-19 α=1.510e-01\n",
      "Final - Train 0.010483 Test 0.011640\n",
      "Initial - Train 0.332554 Test 0.333872\n",
      "Epoch    0 f=0.218495 test=0.198538 ||g||=9.67e-01 r=1.003 λ=5.00e-06 α=7.417e-02\n",
      "Epoch   10 f=0.179527 test=0.160307 ||g||=7.86e-01 r=-0.280 λ=5.00e-06 α=5.034e-01\n",
      "Epoch   20 f=0.179527 test=0.160307 ||g||=7.86e-01 r=-0.280 λ=5.12e-03 α=5.034e-01\n",
      "Epoch   30 f=0.179527 test=0.160307 ||g||=7.86e-01 r=-0.280 λ=5.24e+00 α=5.034e-01\n",
      "Epoch   40 f=0.179527 test=0.160307 ||g||=7.86e-01 r=-0.280 λ=5.37e+03 α=5.034e-01\n",
      "Epoch   49 f=0.179527 test=0.160307 ||g||=7.86e-01 r=-0.280 λ=2.75e+06 α=5.034e-01\n",
      "Final - Train 0.179527 Test 0.160307\n",
      "Initial - Train 0.472124 Test 0.426829\n",
      "Epoch    0 f=0.207522 test=0.206306 ||g||=8.27e-01 r=1.001 λ=5.00e-06 α=7.622e-02\n",
      "Epoch   10 f=0.054755 test=0.049057 ||g||=8.50e-01 r=0.965 λ=9.77e-09 α=1.540e-02\n",
      "Epoch   20 f=0.012715 test=0.013205 ||g||=5.82e-02 r=1.010 λ=9.54e-12 α=8.487e-02\n",
      "Epoch   30 f=0.014889 test=0.015786 ||g||=4.69e-02 r=-1.406 λ=1.49e-13 α=1.000e+00\n",
      "Epoch   40 f=0.014889 test=0.015786 ||g||=4.69e-02 r=-1.406 λ=1.56e-07 α=1.000e+00\n",
      "Epoch   49 f=0.014889 test=0.015786 ||g||=4.69e-02 r=-1.683 λ=4.10e-02 α=1.000e+00\n",
      "Final - Train 0.014889 Test 0.015786\n",
      "Initial - Train 0.389784 Test 0.347626\n",
      "Epoch    0 f=0.202349 test=0.201787 ||g||=7.36e-01 r=0.998 λ=5.00e-06 α=7.887e-02\n",
      "Epoch   10 f=0.015428 test=0.014531 ||g||=1.19e-01 r=1.071 λ=9.77e-09 α=1.951e-01\n",
      "Epoch   20 f=0.012629 test=0.012173 ||g||=4.22e-02 r=1.013 λ=9.54e-12 α=1.242e-01\n",
      "Epoch   30 f=0.011678 test=0.012032 ||g||=2.70e-02 r=1.220 λ=9.31e-15 α=1.590e-01\n",
      "Epoch   40 f=0.010813 test=0.011883 ||g||=2.36e-02 r=1.045 λ=9.09e-18 α=1.154e-01\n",
      "Epoch   49 f=0.010483 test=0.011640 ||g||=2.02e-02 r=1.066 λ=1.78e-20 α=1.510e-01\n",
      "Final - Train 0.010483 Test 0.011640\n",
      "Initial - Train 0.332554 Test 0.333872\n",
      "Epoch    0 f=0.218495 test=0.198538 ||g||=9.67e-01 r=1.003 λ=5.00e-07 α=7.417e-02\n",
      "Epoch   10 f=0.179527 test=0.160307 ||g||=7.86e-01 r=-0.280 λ=5.00e-07 α=5.034e-01\n",
      "Epoch   20 f=0.179527 test=0.160307 ||g||=7.86e-01 r=-0.280 λ=5.12e-04 α=5.034e-01\n",
      "Epoch   30 f=0.179527 test=0.160307 ||g||=7.86e-01 r=-0.280 λ=5.24e-01 α=5.034e-01\n",
      "Epoch   40 f=0.179527 test=0.160307 ||g||=7.86e-01 r=-0.280 λ=5.37e+02 α=5.034e-01\n",
      "Epoch   49 f=0.179527 test=0.160307 ||g||=7.86e-01 r=-0.280 λ=2.75e+05 α=5.034e-01\n",
      "Final - Train 0.179527 Test 0.160307\n",
      "Initial - Train 0.472124 Test 0.426829\n",
      "Epoch    0 f=0.207522 test=0.206306 ||g||=8.27e-01 r=1.001 λ=5.00e-07 α=7.622e-02\n",
      "Epoch   10 f=0.054755 test=0.049057 ||g||=8.50e-01 r=0.965 λ=9.77e-10 α=1.540e-02\n",
      "Epoch   20 f=0.012715 test=0.013205 ||g||=5.82e-02 r=1.010 λ=9.54e-13 α=8.487e-02\n",
      "Epoch   30 f=0.014889 test=0.015786 ||g||=4.69e-02 r=-1.406 λ=1.49e-14 α=1.000e+00\n",
      "Epoch   40 f=0.014889 test=0.015786 ||g||=4.69e-02 r=-1.406 λ=1.56e-08 α=1.000e+00\n",
      "Epoch   49 f=0.014889 test=0.015786 ||g||=4.69e-02 r=-1.430 λ=4.10e-03 α=1.000e+00\n",
      "Final - Train 0.014889 Test 0.015786\n",
      "Initial - Train 0.389784 Test 0.347626\n",
      "Epoch    0 f=0.202349 test=0.201787 ||g||=7.36e-01 r=0.998 λ=5.00e-07 α=7.887e-02\n",
      "Epoch   10 f=0.015428 test=0.014531 ||g||=1.19e-01 r=1.071 λ=9.77e-10 α=1.951e-01\n",
      "Epoch   20 f=0.012629 test=0.012173 ||g||=4.22e-02 r=1.013 λ=9.54e-13 α=1.242e-01\n",
      "Epoch   30 f=0.011678 test=0.012032 ||g||=2.70e-02 r=1.220 λ=9.31e-16 α=1.590e-01\n",
      "Epoch   40 f=0.010813 test=0.011883 ||g||=2.36e-02 r=1.045 λ=9.09e-19 α=1.154e-01\n",
      "Epoch   49 f=0.010483 test=0.011640 ||g||=2.02e-02 r=1.066 λ=1.78e-21 α=1.510e-01\n",
      "Final - Train 0.010483 Test 0.011640\n",
      "Initial - Train 0.332554 Test 0.333872\n",
      "Epoch    0 f=0.218495 test=0.198538 ||g||=9.67e-01 r=1.003 λ=5.00e-08 α=7.417e-02\n",
      "Epoch   10 f=0.179527 test=0.160307 ||g||=7.86e-01 r=-0.280 λ=5.00e-08 α=5.034e-01\n",
      "Epoch   20 f=0.179527 test=0.160307 ||g||=7.86e-01 r=-0.280 λ=5.12e-05 α=5.034e-01\n",
      "Epoch   30 f=0.179527 test=0.160307 ||g||=7.86e-01 r=-0.280 λ=5.24e-02 α=5.034e-01\n",
      "Epoch   40 f=0.179527 test=0.160307 ||g||=7.86e-01 r=-0.280 λ=5.37e+01 α=5.034e-01\n",
      "Epoch   49 f=0.179527 test=0.160307 ||g||=7.86e-01 r=-0.280 λ=2.75e+04 α=5.034e-01\n",
      "Final - Train 0.179527 Test 0.160307\n",
      "Initial - Train 0.472124 Test 0.426829\n",
      "Epoch    0 f=0.207522 test=0.206306 ||g||=8.27e-01 r=1.001 λ=5.00e-08 α=7.622e-02\n",
      "Epoch   10 f=0.054755 test=0.049057 ||g||=8.50e-01 r=0.965 λ=9.77e-11 α=1.540e-02\n",
      "Epoch   20 f=0.012715 test=0.013205 ||g||=5.82e-02 r=1.010 λ=9.54e-14 α=8.487e-02\n",
      "Epoch   30 f=0.014889 test=0.015786 ||g||=4.69e-02 r=-1.406 λ=1.49e-15 α=1.000e+00\n",
      "Epoch   40 f=0.014889 test=0.015786 ||g||=4.69e-02 r=-1.406 λ=1.56e-09 α=1.000e+00\n",
      "Epoch   49 f=0.014889 test=0.015786 ||g||=4.69e-02 r=-1.408 λ=4.10e-04 α=1.000e+00\n",
      "Final - Train 0.014889 Test 0.015786\n",
      "Initial - Train 0.389784 Test 0.347626\n",
      "Epoch    0 f=0.202349 test=0.201787 ||g||=7.36e-01 r=0.998 λ=5.00e-08 α=7.887e-02\n",
      "Epoch   10 f=0.015428 test=0.014531 ||g||=1.19e-01 r=1.071 λ=9.77e-11 α=1.951e-01\n",
      "Epoch   20 f=0.012629 test=0.012173 ||g||=4.22e-02 r=1.013 λ=9.54e-14 α=1.242e-01\n",
      "Epoch   30 f=0.011678 test=0.012032 ||g||=2.70e-02 r=1.220 λ=9.31e-17 α=1.590e-01\n",
      "Epoch   40 f=0.010813 test=0.011883 ||g||=2.36e-02 r=1.045 λ=9.09e-20 α=1.154e-01\n",
      "Epoch   49 f=0.010483 test=0.011640 ||g||=2.02e-02 r=1.066 λ=1.78e-22 α=1.510e-01\n",
      "Final - Train 0.010483 Test 0.011640\n",
      "Initial - Train 0.332554 Test 0.333872\n",
      "Epoch    0 f=0.218496 test=0.198543 ||g||=9.67e-01 r=1.003 λ=5.00e-05 α=7.416e-02\n",
      "Epoch   10 f=0.178103 test=0.158902 ||g||=7.97e-01 r=-0.270 λ=5.00e-05 α=4.947e-01\n",
      "Epoch   20 f=0.178103 test=0.158902 ||g||=7.97e-01 r=-0.270 λ=5.12e-02 α=4.947e-01\n",
      "Epoch   30 f=0.178103 test=0.158902 ||g||=7.97e-01 r=-0.270 λ=5.24e+01 α=4.947e-01\n",
      "Epoch   40 f=0.178103 test=0.158902 ||g||=7.97e-01 r=-0.270 λ=5.37e+04 α=4.947e-01\n",
      "Epoch   49 f=0.178103 test=0.158902 ||g||=7.97e-01 r=-0.270 λ=2.75e+07 α=4.947e-01\n",
      "Final - Train 0.178103 Test 0.158902\n",
      "Initial - Train 0.472124 Test 0.426829\n",
      "Epoch    0 f=0.207522 test=0.206308 ||g||=8.27e-01 r=1.001 λ=5.00e-05 α=7.622e-02\n",
      "Epoch   10 f=0.062431 test=0.059985 ||g||=5.93e-01 r=0.919 λ=4.88e-08 α=1.218e-01\n",
      "Epoch   20 f=0.018814 test=0.018104 ||g||=1.47e-01 r=0.972 λ=4.77e-11 α=3.695e-01\n",
      "Epoch   30 f=0.013074 test=0.014089 ||g||=5.66e-02 r=1.023 λ=4.66e-14 α=5.144e-01\n",
      "Epoch   40 f=0.018253 test=0.017163 ||g||=3.38e-02 r=-5.129 λ=1.49e-12 α=1.000e+00\n",
      "Epoch   49 f=0.018253 test=0.017163 ||g||=3.38e-02 r=-5.129 λ=3.91e-07 α=1.000e+00\n",
      "Final - Train 0.018253 Test 0.017163\n",
      "Initial - Train 0.389784 Test 0.347626\n",
      "Epoch    0 f=0.202349 test=0.201790 ||g||=7.36e-01 r=0.998 λ=5.00e-05 α=7.888e-02\n",
      "Epoch   10 f=0.014836 test=0.014298 ||g||=5.55e-02 r=1.023 λ=9.77e-08 α=7.523e-02\n",
      "Epoch   20 f=0.012932 test=0.012741 ||g||=3.71e-02 r=1.021 λ=9.54e-11 α=1.196e-01\n",
      "Epoch   30 f=0.012069 test=0.012476 ||g||=2.90e-02 r=1.004 λ=9.31e-14 α=2.029e-01\n",
      "Epoch   40 f=0.010943 test=0.012059 ||g||=2.69e-02 r=1.055 λ=9.09e-17 α=1.019e-01\n",
      "Epoch   49 f=0.010486 test=0.011511 ||g||=1.88e-02 r=0.975 λ=1.78e-19 α=6.456e-02\n",
      "Final - Train 0.010486 Test 0.011511\n",
      "Initial - Train 0.332554 Test 0.333872\n",
      "Epoch    0 f=0.218496 test=0.198543 ||g||=9.67e-01 r=1.003 λ=5.00e-06 α=7.416e-02\n",
      "Epoch   10 f=0.178103 test=0.158902 ||g||=7.97e-01 r=-0.270 λ=5.00e-06 α=4.947e-01\n",
      "Epoch   20 f=0.178103 test=0.158902 ||g||=7.97e-01 r=-0.270 λ=5.12e-03 α=4.947e-01\n",
      "Epoch   30 f=0.178103 test=0.158902 ||g||=7.97e-01 r=-0.270 λ=5.24e+00 α=4.947e-01\n",
      "Epoch   40 f=0.178103 test=0.158902 ||g||=7.97e-01 r=-0.270 λ=5.37e+03 α=4.947e-01\n",
      "Epoch   49 f=0.178103 test=0.158902 ||g||=7.97e-01 r=-0.270 λ=2.75e+06 α=4.947e-01\n",
      "Final - Train 0.178103 Test 0.158902\n",
      "Initial - Train 0.472124 Test 0.426829\n",
      "Epoch    0 f=0.207522 test=0.206308 ||g||=8.27e-01 r=1.001 λ=5.00e-06 α=7.622e-02\n",
      "Epoch   10 f=0.062431 test=0.059985 ||g||=5.93e-01 r=0.919 λ=4.88e-09 α=1.218e-01\n",
      "Epoch   20 f=0.018814 test=0.018104 ||g||=1.47e-01 r=0.972 λ=4.77e-12 α=3.695e-01\n",
      "Epoch   30 f=0.013074 test=0.014089 ||g||=5.66e-02 r=1.023 λ=4.66e-15 α=5.144e-01\n",
      "Epoch   40 f=0.018253 test=0.017163 ||g||=3.38e-02 r=-5.129 λ=1.49e-13 α=1.000e+00\n",
      "Epoch   49 f=0.018253 test=0.017163 ||g||=3.38e-02 r=-5.129 λ=3.91e-08 α=1.000e+00\n",
      "Final - Train 0.018253 Test 0.017163\n",
      "Initial - Train 0.389784 Test 0.347626\n",
      "Epoch    0 f=0.202349 test=0.201790 ||g||=7.36e-01 r=0.998 λ=5.00e-06 α=7.888e-02\n",
      "Epoch   10 f=0.014836 test=0.014298 ||g||=5.55e-02 r=1.023 λ=9.77e-09 α=7.523e-02\n",
      "Epoch   20 f=0.012932 test=0.012741 ||g||=3.71e-02 r=1.021 λ=9.54e-12 α=1.196e-01\n",
      "Epoch   30 f=0.012069 test=0.012476 ||g||=2.90e-02 r=1.004 λ=9.31e-15 α=2.029e-01\n",
      "Epoch   40 f=0.010943 test=0.012059 ||g||=2.69e-02 r=1.055 λ=9.09e-18 α=1.019e-01\n",
      "Epoch   49 f=0.010486 test=0.011511 ||g||=1.88e-02 r=0.975 λ=1.78e-20 α=6.456e-02\n",
      "Final - Train 0.010486 Test 0.011511\n",
      "Initial - Train 0.332554 Test 0.333872\n",
      "Epoch    0 f=0.218496 test=0.198543 ||g||=9.67e-01 r=1.003 λ=5.00e-07 α=7.416e-02\n",
      "Epoch   10 f=0.178103 test=0.158902 ||g||=7.97e-01 r=-0.270 λ=5.00e-07 α=4.947e-01\n",
      "Epoch   20 f=0.178103 test=0.158902 ||g||=7.97e-01 r=-0.270 λ=5.12e-04 α=4.947e-01\n",
      "Epoch   30 f=0.178103 test=0.158902 ||g||=7.97e-01 r=-0.270 λ=5.24e-01 α=4.947e-01\n",
      "Epoch   40 f=0.178103 test=0.158902 ||g||=7.97e-01 r=-0.270 λ=5.37e+02 α=4.947e-01\n",
      "Epoch   49 f=0.178103 test=0.158902 ||g||=7.97e-01 r=-0.270 λ=2.75e+05 α=4.947e-01\n",
      "Final - Train 0.178103 Test 0.158902\n",
      "Initial - Train 0.472124 Test 0.426829\n",
      "Epoch    0 f=0.207522 test=0.206308 ||g||=8.27e-01 r=1.001 λ=5.00e-07 α=7.622e-02\n",
      "Epoch   10 f=0.062431 test=0.059985 ||g||=5.93e-01 r=0.919 λ=4.88e-10 α=1.218e-01\n",
      "Epoch   20 f=0.018814 test=0.018104 ||g||=1.47e-01 r=0.972 λ=4.77e-13 α=3.695e-01\n",
      "Epoch   30 f=0.013074 test=0.014089 ||g||=5.66e-02 r=1.023 λ=4.66e-16 α=5.144e-01\n",
      "Epoch   40 f=0.018253 test=0.017163 ||g||=3.38e-02 r=-5.129 λ=1.49e-14 α=1.000e+00\n",
      "Epoch   49 f=0.018253 test=0.017163 ||g||=3.38e-02 r=-5.129 λ=3.91e-09 α=1.000e+00\n",
      "Final - Train 0.018253 Test 0.017163\n",
      "Initial - Train 0.389784 Test 0.347626\n",
      "Epoch    0 f=0.202349 test=0.201790 ||g||=7.36e-01 r=0.998 λ=5.00e-07 α=7.888e-02\n",
      "Epoch   10 f=0.014836 test=0.014298 ||g||=5.55e-02 r=1.023 λ=9.77e-10 α=7.523e-02\n",
      "Epoch   20 f=0.012932 test=0.012741 ||g||=3.71e-02 r=1.021 λ=9.54e-13 α=1.196e-01\n",
      "Epoch   30 f=0.012069 test=0.012476 ||g||=2.90e-02 r=1.004 λ=9.31e-16 α=2.029e-01\n",
      "Epoch   40 f=0.010943 test=0.012059 ||g||=2.69e-02 r=1.055 λ=9.09e-19 α=1.019e-01\n",
      "Epoch   49 f=0.010486 test=0.011511 ||g||=1.88e-02 r=0.975 λ=1.78e-21 α=6.456e-02\n",
      "Final - Train 0.010486 Test 0.011511\n",
      "Initial - Train 0.332554 Test 0.333872\n",
      "Epoch    0 f=0.218496 test=0.198543 ||g||=9.67e-01 r=1.003 λ=5.00e-08 α=7.416e-02\n",
      "Epoch   10 f=0.178103 test=0.158902 ||g||=7.97e-01 r=-0.270 λ=5.00e-08 α=4.947e-01\n",
      "Epoch   20 f=0.178103 test=0.158902 ||g||=7.97e-01 r=-0.270 λ=5.12e-05 α=4.947e-01\n",
      "Epoch   30 f=0.178103 test=0.158902 ||g||=7.97e-01 r=-0.270 λ=5.24e-02 α=4.947e-01\n",
      "Epoch   40 f=0.178103 test=0.158902 ||g||=7.97e-01 r=-0.270 λ=5.37e+01 α=4.947e-01\n",
      "Epoch   49 f=0.178103 test=0.158902 ||g||=7.97e-01 r=-0.270 λ=2.75e+04 α=4.947e-01\n",
      "Final - Train 0.178103 Test 0.158902\n",
      "Initial - Train 0.472124 Test 0.426829\n",
      "Epoch    0 f=0.207522 test=0.206308 ||g||=8.27e-01 r=1.001 λ=5.00e-08 α=7.622e-02\n",
      "Epoch   10 f=0.062431 test=0.059985 ||g||=5.93e-01 r=0.919 λ=4.88e-11 α=1.218e-01\n",
      "Epoch   20 f=0.018814 test=0.018104 ||g||=1.47e-01 r=0.972 λ=4.77e-14 α=3.695e-01\n",
      "Epoch   30 f=0.013074 test=0.014089 ||g||=5.66e-02 r=1.023 λ=4.66e-17 α=5.144e-01\n",
      "Epoch   40 f=0.018253 test=0.017163 ||g||=3.38e-02 r=-5.129 λ=1.49e-15 α=1.000e+00\n",
      "Epoch   49 f=0.018253 test=0.017163 ||g||=3.38e-02 r=-5.129 λ=3.91e-10 α=1.000e+00\n",
      "Final - Train 0.018253 Test 0.017163\n",
      "Initial - Train 0.389784 Test 0.347626\n",
      "Epoch    0 f=0.202349 test=0.201790 ||g||=7.36e-01 r=0.998 λ=5.00e-08 α=7.888e-02\n",
      "Epoch   10 f=0.014836 test=0.014298 ||g||=5.55e-02 r=1.023 λ=9.77e-11 α=7.523e-02\n",
      "Epoch   20 f=0.012932 test=0.012741 ||g||=3.71e-02 r=1.021 λ=9.54e-14 α=1.196e-01\n",
      "Epoch   30 f=0.012069 test=0.012476 ||g||=2.90e-02 r=1.004 λ=9.31e-17 α=2.029e-01\n",
      "Epoch   40 f=0.010943 test=0.012059 ||g||=2.69e-02 r=1.055 λ=9.09e-20 α=1.019e-01\n",
      "Epoch   49 f=0.010486 test=0.011511 ||g||=1.88e-02 r=0.975 λ=1.78e-22 α=6.456e-02\n",
      "Final - Train 0.010486 Test 0.011511\n",
      "Initial - Train 0.332554 Test 0.333872\n",
      "Epoch    0 f=0.218496 test=0.198543 ||g||=9.67e-01 r=1.003 λ=5.00e-05 α=7.416e-02\n",
      "Epoch   10 f=0.145752 test=0.129801 ||g||=8.51e-01 r=-0.073 λ=5.00e-05 α=4.217e-01\n",
      "Epoch   20 f=0.145752 test=0.129801 ||g||=8.51e-01 r=-0.073 λ=5.12e-02 α=4.217e-01\n",
      "Epoch   30 f=0.145752 test=0.129801 ||g||=8.51e-01 r=-0.073 λ=5.24e+01 α=4.217e-01\n",
      "Epoch   40 f=0.145752 test=0.129801 ||g||=8.51e-01 r=-0.073 λ=5.37e+04 α=4.217e-01\n",
      "Epoch   49 f=0.145752 test=0.129801 ||g||=8.51e-01 r=-0.073 λ=2.75e+07 α=4.217e-01\n",
      "Final - Train 0.145752 Test 0.129801\n",
      "Initial - Train 0.472124 Test 0.426829\n",
      "Epoch    0 f=0.207523 test=0.206267 ||g||=8.26e-01 r=1.002 λ=5.00e-05 α=7.617e-02\n",
      "Epoch   10 f=0.053708 test=0.047808 ||g||=7.43e-01 r=0.938 λ=9.77e-08 α=4.122e-02\n",
      "Epoch   20 f=0.015195 test=0.016040 ||g||=2.73e-02 r=1.026 λ=9.54e-11 α=4.040e-01\n",
      "Epoch   30 f=0.012265 test=0.013386 ||g||=2.64e-02 r=0.996 λ=9.31e-14 α=2.381e-01\n",
      "Epoch   40 f=0.010930 test=0.011938 ||g||=2.08e-02 r=0.935 λ=3.64e-16 α=1.325e-01\n",
      "Epoch   49 f=0.010689 test=0.011614 ||g||=7.99e-03 r=1.004 λ=7.11e-19 α=1.430e-01\n",
      "Final - Train 0.010689 Test 0.011614\n",
      "Initial - Train 0.389784 Test 0.347626\n",
      "Epoch    0 f=0.202349 test=0.201780 ||g||=7.36e-01 r=0.999 λ=5.00e-05 α=7.886e-02\n",
      "Epoch   10 f=0.014262 test=0.013811 ||g||=1.65e-01 r=0.935 λ=9.77e-08 α=7.432e-01\n",
      "Epoch   20 f=0.011538 test=0.011886 ||g||=1.77e-02 r=1.011 λ=9.54e-11 α=2.396e-01\n",
      "Epoch   30 f=0.010753 test=0.011879 ||g||=3.12e-02 r=1.030 λ=9.31e-14 α=1.702e-01\n",
      "Epoch   40 f=0.010343 test=0.011467 ||g||=1.66e-02 r=0.946 λ=9.09e-17 α=9.047e-02\n",
      "Epoch   49 f=0.010234 test=0.011434 ||g||=6.03e-03 r=1.020 λ=1.78e-19 α=1.370e-01\n",
      "Final - Train 0.010234 Test 0.011434\n",
      "Initial - Train 0.332554 Test 0.333872\n",
      "Epoch    0 f=0.218496 test=0.198543 ||g||=9.67e-01 r=1.003 λ=5.00e-06 α=7.416e-02\n",
      "Epoch   10 f=0.145752 test=0.129801 ||g||=8.51e-01 r=-0.073 λ=5.00e-06 α=4.217e-01\n",
      "Epoch   20 f=0.145752 test=0.129801 ||g||=8.51e-01 r=-0.073 λ=5.12e-03 α=4.217e-01\n",
      "Epoch   30 f=0.145752 test=0.129801 ||g||=8.51e-01 r=-0.073 λ=5.24e+00 α=4.217e-01\n",
      "Epoch   40 f=0.145752 test=0.129801 ||g||=8.51e-01 r=-0.073 λ=5.37e+03 α=4.217e-01\n",
      "Epoch   49 f=0.145752 test=0.129801 ||g||=8.51e-01 r=-0.073 λ=2.75e+06 α=4.217e-01\n",
      "Final - Train 0.145752 Test 0.129801\n",
      "Initial - Train 0.472124 Test 0.426829\n",
      "Epoch    0 f=0.207523 test=0.206267 ||g||=8.26e-01 r=1.002 λ=5.00e-06 α=7.617e-02\n",
      "Epoch   10 f=0.053708 test=0.047808 ||g||=7.43e-01 r=0.938 λ=9.77e-09 α=4.122e-02\n",
      "Epoch   20 f=0.015195 test=0.016040 ||g||=2.73e-02 r=1.026 λ=9.54e-12 α=4.040e-01\n",
      "Epoch   30 f=0.012265 test=0.013386 ||g||=2.64e-02 r=0.996 λ=9.31e-15 α=2.381e-01\n",
      "Epoch   40 f=0.010930 test=0.011938 ||g||=2.08e-02 r=0.935 λ=3.64e-17 α=1.325e-01\n",
      "Epoch   49 f=0.010689 test=0.011614 ||g||=7.99e-03 r=1.004 λ=7.11e-20 α=1.430e-01\n",
      "Final - Train 0.010689 Test 0.011614\n",
      "Initial - Train 0.389784 Test 0.347626\n",
      "Epoch    0 f=0.202349 test=0.201780 ||g||=7.36e-01 r=0.999 λ=5.00e-06 α=7.886e-02\n",
      "Epoch   10 f=0.014262 test=0.013811 ||g||=1.65e-01 r=0.935 λ=9.77e-09 α=7.432e-01\n",
      "Epoch   20 f=0.011538 test=0.011886 ||g||=1.77e-02 r=1.011 λ=9.54e-12 α=2.396e-01\n",
      "Epoch   30 f=0.010753 test=0.011879 ||g||=3.12e-02 r=1.030 λ=9.31e-15 α=1.702e-01\n",
      "Epoch   40 f=0.010343 test=0.011467 ||g||=1.66e-02 r=0.946 λ=9.09e-18 α=9.047e-02\n",
      "Epoch   49 f=0.010234 test=0.011434 ||g||=6.03e-03 r=1.020 λ=1.78e-20 α=1.370e-01\n",
      "Final - Train 0.010234 Test 0.011434\n",
      "Initial - Train 0.332554 Test 0.333872\n",
      "Epoch    0 f=0.218496 test=0.198543 ||g||=9.67e-01 r=1.003 λ=5.00e-07 α=7.416e-02\n",
      "Epoch   10 f=0.145752 test=0.129801 ||g||=8.51e-01 r=-0.073 λ=5.00e-07 α=4.217e-01\n",
      "Epoch   20 f=0.145752 test=0.129801 ||g||=8.51e-01 r=-0.073 λ=5.12e-04 α=4.217e-01\n",
      "Epoch   30 f=0.145752 test=0.129801 ||g||=8.51e-01 r=-0.073 λ=5.24e-01 α=4.217e-01\n",
      "Epoch   40 f=0.145752 test=0.129801 ||g||=8.51e-01 r=-0.073 λ=5.37e+02 α=4.217e-01\n",
      "Epoch   49 f=0.145752 test=0.129801 ||g||=8.51e-01 r=-0.073 λ=2.75e+05 α=4.217e-01\n",
      "Final - Train 0.145752 Test 0.129801\n",
      "Initial - Train 0.472124 Test 0.426829\n",
      "Epoch    0 f=0.207523 test=0.206267 ||g||=8.26e-01 r=1.002 λ=5.00e-07 α=7.617e-02\n",
      "Epoch   10 f=0.053708 test=0.047808 ||g||=7.43e-01 r=0.938 λ=9.77e-10 α=4.122e-02\n",
      "Epoch   20 f=0.015195 test=0.016040 ||g||=2.73e-02 r=1.026 λ=9.54e-13 α=4.040e-01\n",
      "Epoch   30 f=0.012265 test=0.013386 ||g||=2.64e-02 r=0.996 λ=9.31e-16 α=2.381e-01\n",
      "Epoch   40 f=0.010930 test=0.011938 ||g||=2.08e-02 r=0.935 λ=3.64e-18 α=1.325e-01\n",
      "Epoch   49 f=0.010689 test=0.011614 ||g||=7.99e-03 r=1.004 λ=7.11e-21 α=1.430e-01\n",
      "Final - Train 0.010689 Test 0.011614\n",
      "Initial - Train 0.389784 Test 0.347626\n",
      "Epoch    0 f=0.202349 test=0.201780 ||g||=7.36e-01 r=0.999 λ=5.00e-07 α=7.886e-02\n",
      "Epoch   10 f=0.014262 test=0.013811 ||g||=1.65e-01 r=0.935 λ=9.77e-10 α=7.432e-01\n",
      "Epoch   20 f=0.011538 test=0.011886 ||g||=1.77e-02 r=1.011 λ=9.54e-13 α=2.396e-01\n",
      "Epoch   30 f=0.010753 test=0.011879 ||g||=3.12e-02 r=1.030 λ=9.31e-16 α=1.702e-01\n",
      "Epoch   40 f=0.010343 test=0.011467 ||g||=1.66e-02 r=0.946 λ=9.09e-19 α=9.047e-02\n",
      "Epoch   49 f=0.010234 test=0.011434 ||g||=6.03e-03 r=1.020 λ=1.78e-21 α=1.370e-01\n",
      "Final - Train 0.010234 Test 0.011434\n",
      "Initial - Train 0.332554 Test 0.333872\n",
      "Epoch    0 f=0.218496 test=0.198543 ||g||=9.67e-01 r=1.003 λ=5.00e-08 α=7.416e-02\n",
      "Epoch   10 f=0.145752 test=0.129801 ||g||=8.51e-01 r=-0.073 λ=5.00e-08 α=4.217e-01\n",
      "Epoch   20 f=0.145752 test=0.129801 ||g||=8.51e-01 r=-0.073 λ=5.12e-05 α=4.217e-01\n",
      "Epoch   30 f=0.145752 test=0.129801 ||g||=8.51e-01 r=-0.073 λ=5.24e-02 α=4.217e-01\n",
      "Epoch   40 f=0.145752 test=0.129801 ||g||=8.51e-01 r=-0.073 λ=5.37e+01 α=4.217e-01\n",
      "Epoch   49 f=0.145752 test=0.129801 ||g||=8.51e-01 r=-0.073 λ=2.75e+04 α=4.217e-01\n",
      "Final - Train 0.145752 Test 0.129801\n",
      "Initial - Train 0.472124 Test 0.426829\n",
      "Epoch    0 f=0.207523 test=0.206267 ||g||=8.26e-01 r=1.002 λ=5.00e-08 α=7.617e-02\n",
      "Epoch   10 f=0.053708 test=0.047808 ||g||=7.43e-01 r=0.938 λ=9.77e-11 α=4.122e-02\n",
      "Epoch   20 f=0.015195 test=0.016040 ||g||=2.73e-02 r=1.026 λ=9.54e-14 α=4.040e-01\n",
      "Epoch   30 f=0.012265 test=0.013386 ||g||=2.64e-02 r=0.996 λ=9.31e-17 α=2.381e-01\n",
      "Epoch   40 f=0.010930 test=0.011938 ||g||=2.08e-02 r=0.935 λ=3.64e-19 α=1.325e-01\n",
      "Epoch   49 f=0.010689 test=0.011614 ||g||=7.99e-03 r=1.004 λ=7.11e-22 α=1.430e-01\n",
      "Final - Train 0.010689 Test 0.011614\n",
      "Initial - Train 0.389784 Test 0.347626\n",
      "Epoch    0 f=0.202349 test=0.201780 ||g||=7.36e-01 r=0.999 λ=5.00e-08 α=7.886e-02\n",
      "Epoch   10 f=0.014262 test=0.013811 ||g||=1.65e-01 r=0.935 λ=9.77e-11 α=7.432e-01\n",
      "Epoch   20 f=0.011538 test=0.011886 ||g||=1.77e-02 r=1.011 λ=9.54e-14 α=2.396e-01\n",
      "Epoch   30 f=0.010753 test=0.011879 ||g||=3.12e-02 r=1.030 λ=9.31e-17 α=1.702e-01\n",
      "Epoch   40 f=0.010343 test=0.011467 ||g||=1.66e-02 r=0.946 λ=9.09e-20 α=9.047e-02\n",
      "Epoch   49 f=0.010234 test=0.011434 ||g||=6.03e-03 r=1.020 λ=1.78e-22 α=1.370e-01\n",
      "Final - Train 0.010234 Test 0.011434\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.270820, Test Loss = 0.234151, Grad Norm = 2.010100, dt = 0.500500\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.315083, Test Loss = 0.356573, Grad Norm = 2.165867, dt = 0.500500\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.267425, Test Loss = 0.299784, Grad Norm = 1.744224, dt = 0.500500\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.270820, Test Loss = 0.234151, Grad Norm = 2.010100, dt = 0.500500\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.315083, Test Loss = 0.356573, Grad Norm = 2.165867, dt = 0.500500\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.267425, Test Loss = 0.299784, Grad Norm = 1.744224, dt = 0.500500\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.270820, Test Loss = 0.234151, Grad Norm = 2.010100, dt = 0.500500\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.315083, Test Loss = 0.356573, Grad Norm = 2.165867, dt = 0.500500\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.267425, Test Loss = 0.299784, Grad Norm = 1.744224, dt = 0.500500\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.270820, Test Loss = 0.234151, Grad Norm = 2.010100, dt = 0.500500\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.315083, Test Loss = 0.356573, Grad Norm = 2.165867, dt = 0.500500\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.267425, Test Loss = 0.299784, Grad Norm = 1.744224, dt = 0.500500\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.270820, Test Loss = 0.234151, Grad Norm = 2.010100, dt = 0.500500\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.315083, Test Loss = 0.356573, Grad Norm = 2.165867, dt = 0.500500\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.267425, Test Loss = 0.299784, Grad Norm = 1.744224, dt = 0.500500\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.270820, Test Loss = 0.234151, Grad Norm = 2.010100, dt = 0.500500\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.315083, Test Loss = 0.356573, Grad Norm = 2.165867, dt = 0.500500\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.267425, Test Loss = 0.299784, Grad Norm = 1.744224, dt = 0.500500\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.224684, Test Loss = 0.209957, Grad Norm = 0.849077, dt = 0.500000\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.274307, Test Loss = 0.247511, Grad Norm = 1.386369, dt = 0.500000\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.235055, Test Loss = 0.214817, Grad Norm = 1.007806, dt = 0.500000\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.224684, Test Loss = 0.209957, Grad Norm = 0.849077, dt = 0.500000\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.274307, Test Loss = 0.247511, Grad Norm = 1.386369, dt = 0.500000\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.235055, Test Loss = 0.214817, Grad Norm = 1.007806, dt = 0.500000\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.224684, Test Loss = 0.209957, Grad Norm = 0.849077, dt = 0.500000\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.274307, Test Loss = 0.247511, Grad Norm = 1.386369, dt = 0.500000\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.235055, Test Loss = 0.214817, Grad Norm = 1.007806, dt = 0.500000\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.279466, Test Loss = 0.275417, Grad Norm = 1.306199, dt = 0.200200\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.351577, Test Loss = 0.315170, Grad Norm = 1.955480, dt = 0.200200\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.306656, Test Loss = 0.273295, Grad Norm = 1.639049, dt = 0.200200\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.279466, Test Loss = 0.275417, Grad Norm = 1.306199, dt = 0.200200\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.351577, Test Loss = 0.315170, Grad Norm = 1.955480, dt = 0.200200\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.306656, Test Loss = 0.273295, Grad Norm = 1.639049, dt = 0.200200\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.279466, Test Loss = 0.275417, Grad Norm = 1.306199, dt = 0.200200\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.351577, Test Loss = 0.315170, Grad Norm = 1.955480, dt = 0.200200\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.306656, Test Loss = 0.273295, Grad Norm = 1.639049, dt = 0.200200\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.279466, Test Loss = 0.275417, Grad Norm = 1.306199, dt = 0.200200\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.351577, Test Loss = 0.315170, Grad Norm = 1.955480, dt = 0.200200\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.306656, Test Loss = 0.273295, Grad Norm = 1.639049, dt = 0.200200\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.279466, Test Loss = 0.275417, Grad Norm = 1.306199, dt = 0.200200\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.351577, Test Loss = 0.315170, Grad Norm = 1.955480, dt = 0.200200\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.306656, Test Loss = 0.273295, Grad Norm = 1.639049, dt = 0.200200\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.279466, Test Loss = 0.275417, Grad Norm = 1.306199, dt = 0.200200\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.351577, Test Loss = 0.315170, Grad Norm = 1.955480, dt = 0.200200\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.306656, Test Loss = 0.273295, Grad Norm = 1.639049, dt = 0.200200\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.279466, Test Loss = 0.275417, Grad Norm = 1.306199, dt = 0.200200\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.351577, Test Loss = 0.315170, Grad Norm = 1.955480, dt = 0.200200\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.306656, Test Loss = 0.273295, Grad Norm = 1.639049, dt = 0.200200\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.279466, Test Loss = 0.275417, Grad Norm = 1.306199, dt = 0.200200\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.351577, Test Loss = 0.315170, Grad Norm = 1.955480, dt = 0.200200\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.306656, Test Loss = 0.273295, Grad Norm = 1.639049, dt = 0.200200\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.279466, Test Loss = 0.275417, Grad Norm = 1.306199, dt = 0.200200\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.351577, Test Loss = 0.315170, Grad Norm = 1.955480, dt = 0.200200\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.306656, Test Loss = 0.273295, Grad Norm = 1.639049, dt = 0.200200\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.317732, Test Loss = 0.317735, Grad Norm = 1.634758, dt = 0.100100\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.438576, Test Loss = 0.395323, Grad Norm = 2.461746, dt = 0.100100\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.366738, Test Loss = 0.326668, Grad Norm = 2.043998, dt = 0.100100\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.317732, Test Loss = 0.317735, Grad Norm = 1.634758, dt = 0.100100\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.438576, Test Loss = 0.395323, Grad Norm = 2.461746, dt = 0.100100\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.366738, Test Loss = 0.326668, Grad Norm = 2.043998, dt = 0.100100\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.317732, Test Loss = 0.317735, Grad Norm = 1.634758, dt = 0.100100\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.438576, Test Loss = 0.395323, Grad Norm = 2.461746, dt = 0.100100\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.366738, Test Loss = 0.326668, Grad Norm = 2.043998, dt = 0.100100\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.317732, Test Loss = 0.317735, Grad Norm = 1.634758, dt = 0.100100\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.438576, Test Loss = 0.395323, Grad Norm = 2.461746, dt = 0.100100\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.366738, Test Loss = 0.326668, Grad Norm = 2.043998, dt = 0.100100\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.317732, Test Loss = 0.317735, Grad Norm = 1.634758, dt = 0.100100\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.438576, Test Loss = 0.395323, Grad Norm = 2.461746, dt = 0.100100\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.366738, Test Loss = 0.326668, Grad Norm = 2.043998, dt = 0.100100\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.317732, Test Loss = 0.317735, Grad Norm = 1.634758, dt = 0.100100\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.438576, Test Loss = 0.395323, Grad Norm = 2.461746, dt = 0.100100\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.366738, Test Loss = 0.326668, Grad Norm = 2.043998, dt = 0.100100\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.317732, Test Loss = 0.317735, Grad Norm = 1.634758, dt = 0.100100\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.438576, Test Loss = 0.395323, Grad Norm = 2.461746, dt = 0.100100\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.366738, Test Loss = 0.326668, Grad Norm = 2.043998, dt = 0.100100\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.317732, Test Loss = 0.317735, Grad Norm = 1.634758, dt = 0.100100\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.438576, Test Loss = 0.395323, Grad Norm = 2.461746, dt = 0.100100\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.366738, Test Loss = 0.326668, Grad Norm = 2.043998, dt = 0.100100\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.317732, Test Loss = 0.317735, Grad Norm = 1.634758, dt = 0.100100\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.438576, Test Loss = 0.395323, Grad Norm = 2.461746, dt = 0.100100\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.366738, Test Loss = 0.326668, Grad Norm = 2.043998, dt = 0.100100\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.328752, Test Loss = 0.329743, Grad Norm = 1.722041, dt = 0.050050\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.463524, Test Loss = 0.418728, Grad Norm = 2.590714, dt = 0.050050\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.383881, Test Loss = 0.342238, Grad Norm = 2.147228, dt = 0.050050\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.328752, Test Loss = 0.329743, Grad Norm = 1.722041, dt = 0.050050\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.463524, Test Loss = 0.418728, Grad Norm = 2.590714, dt = 0.050050\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.383881, Test Loss = 0.342238, Grad Norm = 2.147228, dt = 0.050050\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.328752, Test Loss = 0.329743, Grad Norm = 1.722041, dt = 0.050050\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.463524, Test Loss = 0.418728, Grad Norm = 2.590714, dt = 0.050050\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.383881, Test Loss = 0.342238, Grad Norm = 2.147228, dt = 0.050050\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.328752, Test Loss = 0.329743, Grad Norm = 1.722041, dt = 0.050050\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.463524, Test Loss = 0.418728, Grad Norm = 2.590714, dt = 0.050050\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.383881, Test Loss = 0.342238, Grad Norm = 2.147228, dt = 0.050050\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.328752, Test Loss = 0.329743, Grad Norm = 1.722041, dt = 0.050050\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.463524, Test Loss = 0.418728, Grad Norm = 2.590714, dt = 0.050050\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.383881, Test Loss = 0.342238, Grad Norm = 2.147228, dt = 0.050050\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.328752, Test Loss = 0.329743, Grad Norm = 1.722041, dt = 0.050050\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.463524, Test Loss = 0.418728, Grad Norm = 2.590714, dt = 0.050050\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.383881, Test Loss = 0.342238, Grad Norm = 2.147228, dt = 0.050050\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.328752, Test Loss = 0.329743, Grad Norm = 1.722041, dt = 0.050050\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.463524, Test Loss = 0.418728, Grad Norm = 2.590714, dt = 0.050050\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.383881, Test Loss = 0.342238, Grad Norm = 2.147228, dt = 0.050050\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.328752, Test Loss = 0.329743, Grad Norm = 1.722041, dt = 0.050050\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.463524, Test Loss = 0.418728, Grad Norm = 2.590714, dt = 0.050050\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.383881, Test Loss = 0.342238, Grad Norm = 2.147228, dt = 0.050050\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.328752, Test Loss = 0.329743, Grad Norm = 1.722041, dt = 0.050050\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.463524, Test Loss = 0.418728, Grad Norm = 2.590714, dt = 0.050050\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.383881, Test Loss = 0.342238, Grad Norm = 2.147228, dt = 0.050050\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.332400, Test Loss = 0.333706, Grad Norm = 1.750297, dt = 0.010010\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.471778, Test Loss = 0.426502, Grad Norm = 2.632143, dt = 0.010010\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.389547, Test Loss = 0.347408, Grad Norm = 2.180392, dt = 0.010010\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.332400, Test Loss = 0.333706, Grad Norm = 1.750297, dt = 0.010010\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.471778, Test Loss = 0.426502, Grad Norm = 2.632143, dt = 0.010010\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.389547, Test Loss = 0.347408, Grad Norm = 2.180392, dt = 0.010010\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.332400, Test Loss = 0.333706, Grad Norm = 1.750297, dt = 0.010010\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.471778, Test Loss = 0.426502, Grad Norm = 2.632143, dt = 0.010010\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.389547, Test Loss = 0.347408, Grad Norm = 2.180392, dt = 0.010010\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.332400, Test Loss = 0.333706, Grad Norm = 1.750297, dt = 0.010010\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.471778, Test Loss = 0.426502, Grad Norm = 2.632143, dt = 0.010010\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.389547, Test Loss = 0.347408, Grad Norm = 2.180392, dt = 0.010010\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.332400, Test Loss = 0.333706, Grad Norm = 1.750297, dt = 0.010010\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.471778, Test Loss = 0.426502, Grad Norm = 2.632143, dt = 0.010010\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.389547, Test Loss = 0.347408, Grad Norm = 2.180392, dt = 0.010010\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.332400, Test Loss = 0.333706, Grad Norm = 1.750297, dt = 0.010010\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.471778, Test Loss = 0.426502, Grad Norm = 2.632143, dt = 0.010010\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.389547, Test Loss = 0.347408, Grad Norm = 2.180392, dt = 0.010010\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.332400, Test Loss = 0.333706, Grad Norm = 1.750297, dt = 0.010010\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.471778, Test Loss = 0.426502, Grad Norm = 2.632143, dt = 0.010010\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.389547, Test Loss = 0.347408, Grad Norm = 2.180392, dt = 0.010010\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.332400, Test Loss = 0.333706, Grad Norm = 1.750297, dt = 0.010010\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.471778, Test Loss = 0.426502, Grad Norm = 2.632143, dt = 0.010010\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.389547, Test Loss = 0.347408, Grad Norm = 2.180392, dt = 0.010010\n",
      "Initial loss: 0.332554, Gradient norm: 1.751478\n",
      "Epoch 0: Train Loss = 0.332400, Test Loss = 0.333706, Grad Norm = 1.750297, dt = 0.010010\n",
      "Initial loss: 0.472124, Gradient norm: 2.633870\n",
      "Epoch 0: Train Loss = 0.471778, Test Loss = 0.426502, Grad Norm = 2.632143, dt = 0.010010\n",
      "Initial loss: 0.389784, Gradient norm: 2.181775\n",
      "Epoch 0: Train Loss = 0.389547, Test Loss = 0.347408, Grad Norm = 2.180392, dt = 0.010010\n",
      "Initial - Train 36.470943 Test 36.375965\n",
      "Epoch    0 f=7.029371 test=6.886209 ||g||=5.00e+00 r=1.175 λ=5.00e-05 α=2.112e-01\n",
      "Epoch   10 f=1.703305 test=1.642461 ||g||=4.23e+00 r=0.881 λ=1.64e+00 α=2.758e-02\n",
      "Epoch   20 f=0.951322 test=0.856992 ||g||=4.87e-01 r=-2.414 λ=4.10e-01 α=1.000e+00\n",
      "Epoch   30 f=0.951322 test=0.856992 ||g||=4.87e-01 r=-2.414 λ=4.19e+02 α=1.000e+00\n",
      "Epoch   40 f=0.951322 test=0.856992 ||g||=4.87e-01 r=-2.414 λ=4.29e+05 α=1.000e+00\n",
      "Epoch   49 f=0.951322 test=0.856992 ||g||=4.87e-01 r=-2.414 λ=2.20e+08 α=1.000e+00\n",
      "Final - Train 0.951322 Test 0.856992\n",
      "Initial - Train 34.247143 Test 34.215733\n",
      "Epoch    0 f=2.574341 test=2.676814 ||g||=5.00e+00 r=0.936 λ=5.00e-05 α=2.846e-01\n",
      "Epoch   10 f=1.087301 test=1.134480 ||g||=1.66e+00 r=1.559 λ=1.31e+01 α=7.629e-02\n",
      "Epoch   20 f=0.538806 test=0.512419 ||g||=2.37e-01 r=1.218 λ=1.28e-02 α=2.439e-01\n",
      "Epoch   30 f=0.526760 test=0.507465 ||g||=8.91e-02 r=0.927 λ=1.02e-01 α=2.435e-01\n",
      "Epoch   40 f=0.521833 test=0.512424 ||g||=8.52e-02 r=-0.418 λ=1.64e+00 α=6.154e-01\n",
      "Epoch   49 f=0.521833 test=0.512424 ||g||=8.52e-02 r=-0.418 λ=8.39e+02 α=6.154e-01\n",
      "Final - Train 0.521833 Test 0.512424\n",
      "Initial - Train 34.848946 Test 34.846596\n",
      "Epoch    0 f=6.774577 test=6.675555 ||g||=5.00e+00 r=1.163 λ=5.00e-05 α=2.133e-01\n",
      "Epoch   10 f=2.072775 test=1.735626 ||g||=4.76e+00 r=1.179 λ=1.64e+00 α=7.729e-02\n",
      "Epoch   20 f=0.544050 test=0.526919 ||g||=1.29e-01 r=1.030 λ=6.40e-03 α=1.608e-01\n",
      "Epoch   30 f=0.520834 test=0.513872 ||g||=7.84e-02 r=1.006 λ=1.25e-05 α=2.205e-01\n",
      "Epoch   40 f=0.514716 test=0.505695 ||g||=2.77e-02 r=1.066 λ=1.22e-08 α=1.000e+00\n",
      "Epoch   49 f=0.512256 test=0.501052 ||g||=4.89e-02 r=-0.048 λ=6.10e-09 α=6.872e-01\n",
      "Final - Train 0.512256 Test 0.501052\n",
      "Initial - Train 36.470943 Test 36.375965\n",
      "Epoch    0 f=7.029371 test=6.886209 ||g||=5.00e+00 r=1.175 λ=5.00e-06 α=2.112e-01\n",
      "Epoch   10 f=43.006508 test=42.112968 ||g||=5.00e+00 r=-3.772 λ=5.24e+00 α=7.629e-01\n",
      "Epoch   20 f=0.581240 test=0.564130 ||g||=2.03e-01 r=0.968 λ=2.05e-02 α=1.287e-01\n",
      "Epoch   30 f=0.526789 test=0.508316 ||g||=1.36e-01 r=1.029 λ=4.10e-02 α=7.935e-01\n",
      "Epoch   40 f=0.520247 test=0.504230 ||g||=3.66e-02 r=1.011 λ=4.00e-05 α=2.887e-01\n",
      "Epoch   49 f=0.507632 test=0.492167 ||g||=8.35e-02 r=0.871 λ=6.25e-07 α=3.708e-01\n",
      "Final - Train 0.507632 Test 0.492167\n",
      "Initial - Train 34.247143 Test 34.215733\n",
      "Epoch    0 f=2.574341 test=2.676814 ||g||=5.00e+00 r=0.936 λ=5.00e-06 α=2.846e-01\n",
      "Epoch   10 f=11.467779 test=11.470680 ||g||=5.00e+00 r=-0.933 λ=5.24e+00 α=7.629e-01\n",
      "Epoch   20 f=0.585048 test=0.566382 ||g||=2.51e-01 r=1.269 λ=2.05e-02 α=1.767e-01\n",
      "Epoch   30 f=0.540080 test=0.506685 ||g||=4.63e-01 r=-1.000 λ=5.12e-03 α=-6.481e-02\n",
      "Epoch   40 f=0.540080 test=0.506685 ||g||=4.63e-01 r=-1.000 λ=5.24e+00 α=-6.481e-02\n",
      "Epoch   49 f=0.540080 test=0.506685 ||g||=4.63e-01 r=-1.000 λ=2.68e+03 α=-6.481e-02\n",
      "Final - Train 0.540080 Test 0.506685\n",
      "Initial - Train 34.848946 Test 34.846596\n",
      "Epoch    0 f=6.774577 test=6.675555 ||g||=5.00e+00 r=1.163 λ=5.00e-06 α=2.133e-01\n",
      "Epoch   10 f=36.649212 test=33.897869 ||g||=5.00e+00 r=-3.133 λ=5.24e+00 α=7.629e-01\n",
      "Epoch   20 f=4.107551 test=3.621382 ||g||=2.68e+00 r=-0.759 λ=2.68e+03 α=7.829e-01\n",
      "Epoch   30 f=4.107551 test=3.621382 ||g||=2.68e+00 r=-0.759 λ=2.75e+06 α=7.829e-01\n",
      "Epoch   40 f=4.107551 test=3.621382 ||g||=2.68e+00 r=-0.759 λ=2.81e+09 α=7.829e-01\n",
      "Epoch   49 f=4.107551 test=3.621382 ||g||=2.68e+00 r=-0.759 λ=1.44e+12 α=7.829e-01\n",
      "Final - Train 4.107551 Test 3.621382\n",
      "Initial - Train 36.470943 Test 36.375965\n",
      "Epoch    0 f=7.029371 test=6.886209 ||g||=5.00e+00 r=1.175 λ=5.00e-07 α=2.112e-01\n",
      "Epoch   10 f=105.876968 test=103.600800 ||g||=5.00e+00 r=-4.231 λ=5.24e-01 α=1.000e+00\n",
      "Epoch   20 f=0.585084 test=0.566141 ||g||=4.99e-01 r=1.021 λ=2.62e-01 α=3.774e-01\n",
      "Epoch   30 f=0.524625 test=0.512844 ||g||=6.97e-02 r=0.961 λ=2.56e-04 α=3.345e-01\n",
      "Epoch   40 f=0.517983 test=0.508314 ||g||=8.12e-02 r=0.934 λ=1.00e-06 α=2.615e-01\n",
      "Epoch   49 f=0.501972 test=0.491300 ||g||=6.83e-02 r=1.022 λ=7.81e-09 α=2.749e-01\n",
      "Final - Train 0.501972 Test 0.491300\n",
      "Initial - Train 34.247143 Test 34.215733\n",
      "Epoch    0 f=2.574341 test=2.676814 ||g||=5.00e+00 r=0.936 λ=5.00e-07 α=2.846e-01\n",
      "Epoch   10 f=19.014975 test=18.994078 ||g||=5.00e+00 r=-0.704 λ=5.24e-01 α=1.000e+00\n",
      "Epoch   20 f=0.574590 test=0.544551 ||g||=4.07e-01 r=1.079 λ=1.05e+00 α=1.064e-01\n",
      "Epoch   30 f=0.524643 test=0.507233 ||g||=6.78e-02 r=1.060 λ=1.02e-03 α=3.691e-01\n",
      "Epoch   40 f=0.519186 test=0.504366 ||g||=5.19e-02 r=0.910 λ=4.00e-06 α=8.758e-01\n",
      "Epoch   49 f=0.517441 test=0.506315 ||g||=2.79e-02 r=1.392 λ=1.56e-08 α=9.304e-02\n",
      "Final - Train 0.517441 Test 0.506315\n",
      "Initial - Train 34.848946 Test 34.846596\n",
      "Epoch    0 f=6.774577 test=6.675555 ||g||=5.00e+00 r=1.163 λ=5.00e-07 α=2.133e-01\n",
      "Epoch   10 f=85.493111 test=79.899361 ||g||=5.00e+00 r=-3.370 λ=5.24e-01 α=1.000e+00\n",
      "Epoch   20 f=0.747702 test=0.696018 ||g||=4.38e-01 r=1.054 λ=1.05e+00 α=1.110e-01\n",
      "Epoch   30 f=0.531680 test=0.515995 ||g||=1.18e-01 r=0.961 λ=1.02e-03 α=2.729e-01\n",
      "Epoch   40 f=0.524072 test=0.509674 ||g||=6.42e-02 r=-0.060 λ=2.05e-03 α=8.794e-01\n",
      "Epoch   49 f=0.524072 test=0.509674 ||g||=6.42e-02 r=-0.060 λ=1.05e+00 α=8.794e-01\n",
      "Final - Train 0.524072 Test 0.509674\n",
      "Initial - Train 36.470943 Test 36.375965\n",
      "Epoch    0 f=7.029371 test=6.886209 ||g||=5.00e+00 r=1.175 λ=5.00e-08 α=2.112e-01\n",
      "Epoch   10 f=105.876968 test=103.600800 ||g||=5.00e+00 r=-3.980 λ=5.24e-02 α=1.000e+00\n",
      "Epoch   20 f=0.631097 test=0.597671 ||g||=1.02e+00 r=1.035 λ=1.05e-01 α=3.773e-02\n",
      "Epoch   30 f=0.542269 test=0.530161 ||g||=1.62e-01 r=-1.578 λ=8.19e-04 α=7.516e-01\n",
      "Epoch   40 f=0.542269 test=0.530161 ||g||=1.62e-01 r=-1.578 λ=8.39e-01 α=7.516e-01\n",
      "Epoch   49 f=0.542269 test=0.530161 ||g||=1.62e-01 r=-1.578 λ=4.29e+02 α=7.516e-01\n",
      "Final - Train 0.542269 Test 0.530161\n",
      "Initial - Train 34.247143 Test 34.215733\n",
      "Epoch    0 f=2.574341 test=2.676814 ||g||=5.00e+00 r=0.936 λ=5.00e-08 α=2.846e-01\n",
      "Epoch   10 f=19.014975 test=18.994078 ||g||=5.00e+00 r=-0.662 λ=5.24e-02 α=1.000e+00\n",
      "Epoch   20 f=0.614749 test=0.584730 ||g||=5.42e-01 r=1.619 λ=4.19e-01 α=3.783e-02\n",
      "Epoch   30 f=0.528593 test=0.510073 ||g||=1.55e-01 r=0.734 λ=3.28e-03 α=1.890e-01\n",
      "Epoch   40 f=0.517263 test=0.500576 ||g||=9.60e-02 r=1.062 λ=2.56e-05 α=2.305e-01\n",
      "Epoch   49 f=0.514915 test=0.501856 ||g||=3.65e-02 r=0.995 λ=1.00e-07 α=2.568e-01\n",
      "Final - Train 0.514915 Test 0.501856\n",
      "Initial - Train 34.848946 Test 34.846596\n",
      "Epoch    0 f=6.774577 test=6.675555 ||g||=5.00e+00 r=1.163 λ=5.00e-08 α=2.133e-01\n",
      "Epoch   10 f=85.493111 test=79.899361 ||g||=5.00e+00 r=-3.170 λ=5.24e-02 α=1.000e+00\n",
      "Epoch   20 f=0.656083 test=0.617409 ||g||=2.92e-01 r=1.559 λ=5.24e-02 α=1.737e-01\n",
      "Epoch   30 f=0.527565 test=0.508374 ||g||=1.02e-01 r=0.993 λ=1.02e-04 α=1.589e-01\n",
      "Epoch   40 f=0.517175 test=0.506875 ||g||=2.73e-02 r=1.055 λ=2.00e-07 α=2.636e-01\n",
      "Epoch   49 f=0.510709 test=0.498692 ||g||=6.16e-02 r=0.896 λ=3.12e-09 α=3.204e-01\n",
      "Final - Train 0.510709 Test 0.498692\n",
      "Initial - Train 36.470943 Test 36.375965\n",
      "Epoch    0 f=6.299362 test=6.151405 ||g||=5.00e+00 r=1.140 λ=5.00e-05 α=2.232e-01\n",
      "Epoch   10 f=1.827367 test=1.763831 ||g||=4.51e+00 r=1.324 λ=1.64e+00 α=-5.524e-03\n",
      "Epoch   20 f=0.544673 test=0.513071 ||g||=1.75e-01 r=0.920 λ=3.20e-03 α=1.682e-01\n",
      "Epoch   30 f=0.535086 test=0.517963 ||g||=1.44e-01 r=-0.108 λ=5.12e-02 α=8.133e-01\n",
      "Epoch   40 f=0.535086 test=0.517963 ||g||=1.44e-01 r=-0.108 λ=5.24e+01 α=8.133e-01\n",
      "Epoch   49 f=0.535086 test=0.517963 ||g||=1.44e-01 r=-0.108 λ=2.68e+04 α=8.133e-01\n",
      "Final - Train 0.535086 Test 0.517963\n",
      "Initial - Train 34.247143 Test 34.215733\n",
      "Epoch    0 f=2.568879 test=2.671597 ||g||=5.00e+00 r=0.937 λ=5.00e-05 α=2.845e-01\n",
      "Epoch   10 f=1.086397 test=1.133600 ||g||=1.65e+00 r=1.554 λ=1.31e+01 α=7.629e-02\n",
      "Epoch   20 f=0.557834 test=0.532064 ||g||=1.66e-01 r=1.400 λ=5.12e-02 α=2.259e-01\n",
      "Epoch   30 f=0.527806 test=0.507731 ||g||=8.18e-02 r=0.892 λ=4.10e-01 α=1.981e-01\n",
      "Epoch   40 f=0.518903 test=0.506320 ||g||=7.41e-02 r=0.578 λ=2.56e-02 α=3.313e-01\n",
      "Epoch   49 f=0.516423 test=0.502906 ||g||=2.53e-02 r=1.104 λ=2.00e-04 α=3.181e-01\n",
      "Final - Train 0.516423 Test 0.502906\n",
      "Initial - Train 34.848946 Test 34.846596\n",
      "Epoch    0 f=6.457509 test=6.344842 ||g||=5.00e+00 r=1.138 λ=5.00e-05 α=2.205e-01\n",
      "Epoch   10 f=1.928007 test=1.586611 ||g||=4.44e+00 r=1.155 λ=1.64e+00 α=1.097e-01\n",
      "Epoch   20 f=0.581725 test=0.566719 ||g||=2.21e-01 r=1.312 λ=1.64e+00 α=1.582e-01\n",
      "Epoch   30 f=0.522153 test=0.515732 ||g||=1.04e-01 r=0.586 λ=1.28e-02 α=2.837e-01\n",
      "Epoch   40 f=0.512467 test=0.502674 ||g||=8.26e-02 r=0.536 λ=5.00e-05 α=4.651e-01\n",
      "Epoch   49 f=0.508811 test=0.499298 ||g||=6.78e-02 r=0.492 λ=1.56e-06 α=5.081e-01\n",
      "Final - Train 0.508811 Test 0.499298\n",
      "Initial - Train 36.470943 Test 36.375965\n",
      "Epoch    0 f=6.299362 test=6.151405 ||g||=5.00e+00 r=1.140 λ=5.00e-06 α=2.232e-01\n",
      "Epoch   10 f=46.062176 test=44.857212 ||g||=5.00e+00 r=-4.169 λ=5.24e+00 α=7.629e-01\n",
      "Epoch   20 f=0.552899 test=0.520914 ||g||=2.83e-01 r=1.063 λ=2.05e-02 α=6.616e-01\n",
      "Epoch   30 f=0.525134 test=0.509783 ||g||=6.19e-02 r=0.987 λ=5.12e-03 α=2.345e-01\n",
      "Epoch   40 f=0.524109 test=0.508832 ||g||=7.22e-02 r=-0.218 λ=8.19e-02 α=9.371e-01\n",
      "Epoch   49 f=0.524109 test=0.508832 ||g||=7.22e-02 r=-0.218 λ=4.19e+01 α=9.371e-01\n",
      "Final - Train 0.524109 Test 0.508832\n",
      "Initial - Train 34.247143 Test 34.215733\n",
      "Epoch    0 f=2.568879 test=2.671597 ||g||=5.00e+00 r=0.937 λ=5.00e-06 α=2.845e-01\n",
      "Epoch   10 f=11.465245 test=11.467962 ||g||=5.00e+00 r=-0.933 λ=5.24e+00 α=7.629e-01\n",
      "Epoch   20 f=1.774827 test=1.990721 ||g||=3.56e+00 r=-0.234 λ=5.37e+03 α=-1.885e-01\n",
      "Epoch   30 f=1.774827 test=1.990721 ||g||=3.56e+00 r=-0.234 λ=5.50e+06 α=-1.885e-01\n",
      "Epoch   40 f=1.774827 test=1.990721 ||g||=3.56e+00 r=-0.234 λ=5.63e+09 α=-1.885e-01\n",
      "Epoch   49 f=1.774827 test=1.990721 ||g||=3.56e+00 r=-0.234 λ=2.88e+12 α=-1.885e-01\n",
      "Final - Train 1.774827 Test 1.990721\n",
      "Initial - Train 34.848946 Test 34.846596\n",
      "Epoch    0 f=6.457509 test=6.344842 ||g||=5.00e+00 r=1.138 λ=5.00e-06 α=2.205e-01\n",
      "Epoch   10 f=32.310284 test=29.614344 ||g||=5.00e+00 r=-2.711 λ=5.24e+00 α=7.629e-01\n",
      "Epoch   20 f=0.632757 test=0.567741 ||g||=2.66e-01 r=-0.591 λ=8.19e-02 α=1.000e+00\n",
      "Epoch   30 f=0.632757 test=0.567741 ||g||=2.66e-01 r=-0.591 λ=8.39e+01 α=1.000e+00\n",
      "Epoch   40 f=0.632757 test=0.567741 ||g||=2.66e-01 r=-0.591 λ=8.59e+04 α=1.000e+00\n",
      "Epoch   49 f=0.632757 test=0.567741 ||g||=2.66e-01 r=-0.591 λ=4.40e+07 α=1.000e+00\n",
      "Final - Train 0.632757 Test 0.567741\n",
      "Initial - Train 36.470943 Test 36.375965\n",
      "Epoch    0 f=6.299362 test=6.151405 ||g||=5.00e+00 r=1.140 λ=5.00e-07 α=2.232e-01\n",
      "Epoch   10 f=111.089142 test=108.127464 ||g||=5.00e+00 r=-4.486 λ=5.24e-01 α=1.000e+00\n",
      "Epoch   20 f=3.097171 test=2.903599 ||g||=4.89e+00 r=-0.090 λ=1.07e+03 α=2.331e-01\n",
      "Epoch   30 f=3.097171 test=2.903599 ||g||=4.89e+00 r=-0.090 λ=1.10e+06 α=2.331e-01\n",
      "Epoch   40 f=3.097171 test=2.903599 ||g||=4.89e+00 r=-0.090 λ=1.13e+09 α=2.331e-01\n",
      "Epoch   49 f=3.097171 test=2.903599 ||g||=4.89e+00 r=-0.090 λ=5.76e+11 α=2.331e-01\n",
      "Final - Train 3.097171 Test 2.903599\n",
      "Initial - Train 34.247143 Test 34.215733\n",
      "Epoch    0 f=2.568879 test=2.671597 ||g||=5.00e+00 r=0.937 λ=5.00e-07 α=2.845e-01\n",
      "Epoch   10 f=19.022608 test=19.001753 ||g||=5.00e+00 r=-0.704 λ=5.24e-01 α=1.000e+00\n",
      "Epoch   20 f=0.571031 test=0.545986 ||g||=2.31e-01 r=0.968 λ=1.31e-01 α=9.461e-02\n",
      "Epoch   30 f=0.538853 test=0.516003 ||g||=2.01e-01 r=0.921 λ=2.10e+00 α=7.982e-02\n",
      "Epoch   40 f=0.521391 test=0.506347 ||g||=8.51e-02 r=0.895 λ=3.28e-02 α=2.902e-01\n",
      "Epoch   49 f=0.518838 test=0.504343 ||g||=2.72e-02 r=1.031 λ=1.28e-04 α=2.341e-01\n",
      "Final - Train 0.518838 Test 0.504343\n",
      "Initial - Train 34.848946 Test 34.846596\n",
      "Epoch    0 f=6.457509 test=6.344842 ||g||=5.00e+00 r=1.138 λ=5.00e-07 α=2.205e-01\n",
      "Epoch   10 f=72.750793 test=67.469826 ||g||=5.00e+00 r=-2.838 λ=5.24e-01 α=1.000e+00\n",
      "Epoch   20 f=1.992913 test=1.828194 ||g||=2.19e+00 r=-0.045 λ=5.37e+02 α=5.988e-01\n",
      "Epoch   30 f=1.992913 test=1.828194 ||g||=2.19e+00 r=-0.045 λ=5.50e+05 α=5.988e-01\n",
      "Epoch   40 f=1.992913 test=1.828194 ||g||=2.19e+00 r=-0.045 λ=5.63e+08 α=5.988e-01\n",
      "Epoch   49 f=1.992913 test=1.828194 ||g||=2.19e+00 r=-0.045 λ=2.88e+11 α=5.988e-01\n",
      "Final - Train 1.992913 Test 1.828194\n",
      "Initial - Train 36.470943 Test 36.375965\n",
      "Epoch    0 f=6.299362 test=6.151405 ||g||=5.00e+00 r=1.140 λ=5.00e-08 α=2.232e-01\n",
      "Epoch   10 f=111.089142 test=108.127464 ||g||=5.00e+00 r=-4.219 λ=5.24e-02 α=1.000e+00\n",
      "Epoch   20 f=0.581738 test=0.562716 ||g||=2.75e-01 r=1.381 λ=5.24e-02 α=1.535e-01\n",
      "Epoch   30 f=0.527447 test=0.507390 ||g||=7.84e-02 r=1.210 λ=5.12e-05 α=2.442e-01\n",
      "Epoch   40 f=0.519372 test=0.504350 ||g||=5.27e-02 r=0.994 λ=1.00e-07 α=3.124e-01\n",
      "Epoch   49 f=0.518473 test=0.497808 ||g||=1.69e-01 r=-0.235 λ=6.40e-06 α=3.945e-01\n",
      "Final - Train 0.518473 Test 0.497808\n",
      "Initial - Train 34.247143 Test 34.215733\n",
      "Epoch    0 f=2.568879 test=2.671597 ||g||=5.00e+00 r=0.937 λ=5.00e-08 α=2.845e-01\n",
      "Epoch   10 f=19.022608 test=19.001753 ||g||=5.00e+00 r=-0.662 λ=5.24e-02 α=1.000e+00\n",
      "Epoch   20 f=0.599611 test=0.566131 ||g||=4.01e-01 r=1.072 λ=4.19e-01 α=1.385e-01\n",
      "Epoch   30 f=0.526451 test=0.510082 ||g||=8.41e-02 r=1.153 λ=6.55e-03 α=1.207e-01\n",
      "Epoch   40 f=0.523336 test=0.501542 ||g||=8.82e-02 r=-0.275 λ=2.10e-01 α=1.000e+00\n",
      "Epoch   49 f=0.519537 test=0.502003 ||g||=7.51e-02 r=-0.009 λ=2.68e+01 α=6.141e-01\n",
      "Final - Train 0.519537 Test 0.502003\n",
      "Initial - Train 34.848946 Test 34.846596\n",
      "Epoch    0 f=6.457509 test=6.344842 ||g||=5.00e+00 r=1.138 λ=5.00e-08 α=2.205e-01\n",
      "Epoch   10 f=72.750793 test=67.469826 ||g||=5.00e+00 r=-2.669 λ=5.24e-02 α=1.000e+00\n",
      "Epoch   20 f=0.630792 test=0.607111 ||g||=3.95e-01 r=1.044 λ=5.24e-02 α=4.495e-01\n",
      "Epoch   30 f=0.525935 test=0.510747 ||g||=8.50e-02 r=1.025 λ=5.12e-05 α=9.319e-01\n",
      "Epoch   40 f=0.516005 test=0.507600 ||g||=4.96e-02 r=0.952 λ=5.00e-08 α=3.807e-01\n",
      "Epoch   49 f=0.525146 test=0.517918 ||g||=8.78e-02 r=-2.218 λ=1.00e-07 α=1.000e+00\n",
      "Final - Train 0.525146 Test 0.517918\n",
      "Initial - Train 36.470943 Test 36.375965\n",
      "Epoch    0 f=7.824646 test=7.687785 ||g||=5.00e+00 r=1.210 λ=5.00e-05 α=1.995e-01\n",
      "Epoch   10 f=1.549386 test=1.489143 ||g||=3.74e+00 r=0.942 λ=1.64e+00 α=4.518e-02\n",
      "Epoch   20 f=0.533220 test=0.510243 ||g||=1.20e-01 r=1.007 λ=1.28e-02 α=1.405e-01\n",
      "Epoch   30 f=0.526959 test=0.508670 ||g||=6.91e-02 r=1.003 λ=4.10e-01 α=2.410e-01\n",
      "Epoch   40 f=0.519568 test=0.499060 ||g||=1.61e-01 r=0.789 λ=4.00e-04 α=9.775e-01\n",
      "Epoch   49 f=0.502123 test=0.488924 ||g||=6.64e-02 r=0.982 λ=7.81e-07 α=2.011e-01\n",
      "Final - Train 0.502123 Test 0.488924\n",
      "Initial - Train 34.247143 Test 34.215733\n",
      "Epoch    0 f=2.602145 test=2.703375 ||g||=5.00e+00 r=0.934 λ=5.00e-05 α=2.851e-01\n",
      "Epoch   10 f=1.092094 test=1.139166 ||g||=1.69e+00 r=1.583 λ=1.31e+01 α=7.629e-02\n",
      "Epoch   20 f=0.537115 test=0.512712 ||g||=1.71e-01 r=1.029 λ=5.12e-02 α=1.536e-01\n",
      "Epoch   30 f=0.521445 test=0.507575 ||g||=6.03e-02 r=1.094 λ=2.05e-01 α=1.655e-01\n",
      "Epoch   40 f=0.517636 test=0.505350 ||g||=8.13e-02 r=1.803 λ=1.02e-01 α=1.469e-02\n",
      "Epoch   49 f=0.515959 test=0.502356 ||g||=7.78e-02 r=0.616 λ=4.00e-04 α=5.189e-01\n",
      "Final - Train 0.515959 Test 0.502356\n",
      "Initial - Train 34.848946 Test 34.846596\n",
      "Epoch    0 f=6.702993 test=6.601128 ||g||=5.00e+00 r=1.157 λ=5.00e-05 α=2.148e-01\n",
      "Epoch   10 f=2.057645 test=1.721917 ||g||=4.68e+00 r=1.236 λ=1.64e+00 α=7.588e-02\n",
      "Epoch   20 f=0.564762 test=0.540317 ||g||=2.02e-01 r=1.066 λ=2.05e-01 α=2.503e-01\n",
      "Epoch   30 f=0.561696 test=0.531968 ||g||=4.05e-01 r=-1.613 λ=8.39e+02 α=-5.270e-02\n",
      "Epoch   40 f=0.561696 test=0.531968 ||g||=4.05e-01 r=-1.613 λ=8.59e+05 α=-5.270e-02\n",
      "Epoch   49 f=0.561696 test=0.531968 ||g||=4.05e-01 r=-1.613 λ=4.40e+08 α=-5.270e-02\n",
      "Final - Train 0.561696 Test 0.531968\n",
      "Initial - Train 36.470943 Test 36.375965\n",
      "Epoch    0 f=7.824646 test=7.687785 ||g||=5.00e+00 r=1.210 λ=5.00e-06 α=1.995e-01\n",
      "Epoch   10 f=40.305923 test=39.666218 ||g||=5.00e+00 r=-3.406 λ=5.24e+00 α=7.629e-01\n",
      "Epoch   20 f=0.552279 test=0.532155 ||g||=2.96e-01 r=1.026 λ=1.02e-02 α=2.986e-01\n",
      "Epoch   30 f=0.523192 test=0.503659 ||g||=7.53e-02 r=0.933 λ=1.00e-05 α=3.155e-01\n",
      "Epoch   40 f=0.510622 test=0.501025 ||g||=1.71e-01 r=0.918 λ=1.95e-08 α=6.609e-01\n",
      "Epoch   49 f=0.498383 test=0.488301 ||g||=3.82e-02 r=0.983 λ=3.81e-11 α=2.486e-01\n",
      "Final - Train 0.498383 Test 0.488301\n",
      "Initial - Train 34.247143 Test 34.215733\n",
      "Epoch    0 f=2.602145 test=2.703375 ||g||=5.00e+00 r=0.934 λ=5.00e-06 α=2.851e-01\n",
      "Epoch   10 f=11.471661 test=11.475974 ||g||=5.00e+00 r=-0.930 λ=5.24e+00 α=7.629e-01\n",
      "Epoch   20 f=1.915949 test=1.739843 ||g||=9.72e-01 r=-3.341 λ=1.05e+01 α=1.000e+00\n",
      "Epoch   30 f=1.915949 test=1.739843 ||g||=9.72e-01 r=-3.341 λ=1.07e+04 α=1.000e+00\n",
      "Epoch   40 f=1.915949 test=1.739843 ||g||=9.72e-01 r=-3.341 λ=1.10e+07 α=1.000e+00\n",
      "Epoch   49 f=1.915949 test=1.739843 ||g||=9.72e-01 r=-3.341 λ=5.63e+09 α=1.000e+00\n",
      "Final - Train 1.915949 Test 1.739843\n",
      "Initial - Train 34.848946 Test 34.846596\n",
      "Epoch    0 f=6.702993 test=6.601128 ||g||=5.00e+00 r=1.157 λ=5.00e-06 α=2.148e-01\n",
      "Epoch   10 f=35.835487 test=33.085049 ||g||=5.00e+00 r=-3.055 λ=5.24e+00 α=7.629e-01\n",
      "Epoch   20 f=0.895173 test=0.799007 ||g||=6.39e-01 r=-0.431 λ=4.19e+01 α=8.348e-01\n",
      "Epoch   30 f=0.895173 test=0.799007 ||g||=6.39e-01 r=-0.431 λ=4.29e+04 α=8.348e-01\n",
      "Epoch   40 f=0.895173 test=0.799007 ||g||=6.39e-01 r=-0.431 λ=4.40e+07 α=8.348e-01\n",
      "Epoch   49 f=0.895173 test=0.799007 ||g||=6.39e-01 r=-0.431 λ=2.25e+10 α=8.348e-01\n",
      "Final - Train 0.895173 Test 0.799007\n",
      "Initial - Train 36.470943 Test 36.375965\n",
      "Epoch    0 f=7.824646 test=7.687785 ||g||=5.00e+00 r=1.210 λ=5.00e-07 α=1.995e-01\n",
      "Epoch   10 f=101.220985 test=99.518234 ||g||=5.00e+00 r=-3.998 λ=5.24e-01 α=1.000e+00\n",
      "Epoch   20 f=0.594159 test=0.584449 ||g||=7.63e-01 r=0.368 λ=1.31e-01 α=1.000e+00\n",
      "Epoch   30 f=0.529330 test=0.512862 ||g||=6.97e-02 r=1.003 λ=1.28e-04 α=4.826e-01\n",
      "Epoch   40 f=0.518899 test=0.507315 ||g||=2.14e-01 r=0.850 λ=2.50e-07 α=-2.842e-02\n",
      "Epoch   49 f=0.505705 test=0.491945 ||g||=7.10e-02 r=1.009 λ=4.88e-10 α=4.003e-01\n",
      "Final - Train 0.505705 Test 0.491945\n",
      "Initial - Train 34.247143 Test 34.215733\n",
      "Epoch    0 f=2.602145 test=2.703375 ||g||=5.00e+00 r=0.934 λ=5.00e-07 α=2.851e-01\n",
      "Epoch   10 f=18.958954 test=18.939266 ||g||=5.00e+00 r=-0.700 λ=5.24e-01 α=1.000e+00\n",
      "Epoch   20 f=0.557492 test=0.527267 ||g||=2.04e-01 r=1.004 λ=6.55e-02 α=2.337e-01\n",
      "Epoch   30 f=0.532375 test=0.518464 ||g||=1.01e-01 r=-0.264 λ=6.55e-02 α=1.000e+00\n",
      "Epoch   40 f=0.532375 test=0.518464 ||g||=1.01e-01 r=-0.264 λ=6.71e+01 α=1.000e+00\n",
      "Epoch   49 f=0.532375 test=0.518464 ||g||=1.01e-01 r=-0.264 λ=3.44e+04 α=1.000e+00\n",
      "Final - Train 0.532375 Test 0.518464\n",
      "Initial - Train 34.848946 Test 34.846596\n",
      "Epoch    0 f=6.702993 test=6.601128 ||g||=5.00e+00 r=1.157 λ=5.00e-07 α=2.148e-01\n",
      "Epoch   10 f=83.040314 test=77.483475 ||g||=5.00e+00 r=-3.268 λ=5.24e-01 α=1.000e+00\n",
      "Epoch   20 f=0.637320 test=0.602166 ||g||=5.83e-01 r=1.101 λ=1.31e-01 α=3.640e-01\n",
      "Epoch   30 f=0.526934 test=0.515813 ||g||=1.43e-01 r=1.099 λ=1.28e-04 α=6.682e-01\n",
      "Epoch   40 f=0.515693 test=0.505819 ||g||=4.36e-02 r=1.765 λ=1.25e-07 α=8.022e-02\n",
      "Epoch   49 f=0.515906 test=0.497267 ||g||=6.38e-02 r=-0.471 λ=1.00e-06 α=1.000e+00\n",
      "Final - Train 0.515906 Test 0.497267\n",
      "Initial - Train 36.470943 Test 36.375965\n",
      "Epoch    0 f=7.824646 test=7.687785 ||g||=5.00e+00 r=1.210 λ=5.00e-08 α=1.995e-01\n",
      "Epoch   10 f=101.220985 test=99.518234 ||g||=5.00e+00 r=-3.760 λ=5.24e-02 α=1.000e+00\n",
      "Epoch   20 f=0.600668 test=0.586967 ||g||=3.46e-01 r=1.015 λ=5.24e-02 α=2.791e-01\n",
      "Epoch   30 f=0.541520 test=0.527759 ||g||=1.44e-01 r=-0.598 λ=2.10e-01 α=1.000e+00\n",
      "Epoch   40 f=0.519781 test=0.505031 ||g||=6.23e-02 r=1.014 λ=5.24e-02 α=1.290e-01\n",
      "Epoch   49 f=0.503293 test=0.486691 ||g||=7.82e-02 r=1.074 λ=1.02e-04 α=3.601e-01\n",
      "Final - Train 0.503293 Test 0.486691\n",
      "Initial - Train 34.247143 Test 34.215733\n",
      "Epoch    0 f=2.602145 test=2.703375 ||g||=5.00e+00 r=0.934 λ=5.00e-08 α=2.851e-01\n",
      "Epoch   10 f=18.958954 test=18.939266 ||g||=5.00e+00 r=-0.659 λ=5.24e-02 α=1.000e+00\n",
      "Epoch   20 f=2.398530 test=2.330563 ||g||=1.76e+00 r=-1.214 λ=4.29e+02 α=1.000e+00\n",
      "Epoch   30 f=2.398530 test=2.330563 ||g||=1.76e+00 r=-1.214 λ=4.40e+05 α=1.000e+00\n",
      "Epoch   40 f=2.398530 test=2.330563 ||g||=1.76e+00 r=-1.214 λ=4.50e+08 α=1.000e+00\n",
      "Epoch   49 f=2.398530 test=2.330563 ||g||=1.76e+00 r=-1.214 λ=2.31e+11 α=1.000e+00\n",
      "Final - Train 2.398530 Test 2.330563\n",
      "Initial - Train 34.848946 Test 34.846596\n",
      "Epoch    0 f=6.702993 test=6.601128 ||g||=5.00e+00 r=1.157 λ=5.00e-08 α=2.148e-01\n",
      "Epoch   10 f=83.040314 test=77.483475 ||g||=5.00e+00 r=-3.074 λ=5.24e-02 α=1.000e+00\n",
      "Epoch   20 f=0.644167 test=0.636140 ||g||=7.24e-01 r=0.806 λ=5.24e-02 α=8.346e-01\n",
      "Epoch   30 f=0.523918 test=0.511362 ||g||=7.61e-02 r=0.975 λ=5.12e-05 α=2.387e-01\n",
      "Epoch   40 f=0.514550 test=0.503131 ||g||=6.66e-02 r=1.696 λ=5.00e-08 α=8.949e-02\n",
      "Epoch   49 f=0.514517 test=0.495311 ||g||=5.51e-02 r=-0.696 λ=4.00e-07 α=1.000e+00\n",
      "Final - Train 0.514517 Test 0.495311\n",
      "Initial - Train 36.470943 Test 36.375965\n",
      "Epoch    0 f=7.826817 test=7.689972 ||g||=5.00e+00 r=1.210 λ=5.00e-05 α=1.995e-01\n",
      "Epoch   10 f=1.549032 test=1.488791 ||g||=3.74e+00 r=0.941 λ=1.64e+00 α=4.525e-02\n",
      "Epoch   20 f=0.618180 test=0.596263 ||g||=2.65e-01 r=-1.236 λ=1.02e-01 α=1.000e+00\n",
      "Epoch   30 f=0.522692 test=0.506561 ||g||=7.32e-02 r=0.959 λ=1.28e-02 α=2.295e-01\n",
      "Epoch   40 f=0.508669 test=0.490081 ||g||=8.40e-02 r=1.006 λ=2.50e-05 α=2.817e-01\n",
      "Epoch   49 f=0.501742 test=0.492796 ||g||=5.22e-02 r=-0.068 λ=3.13e-06 α=1.000e+00\n",
      "Final - Train 0.501742 Test 0.492796\n",
      "Initial - Train 34.247143 Test 34.215733\n",
      "Epoch    0 f=2.556200 test=2.659488 ||g||=5.00e+00 r=0.938 λ=5.00e-05 α=2.843e-01\n",
      "Epoch   10 f=1.084296 test=1.131537 ||g||=1.64e+00 r=1.543 λ=1.31e+01 α=7.629e-02\n",
      "Epoch   20 f=0.536098 test=0.509864 ||g||=1.81e-01 r=1.041 λ=1.28e-02 α=1.783e-01\n",
      "Epoch   30 f=0.519149 test=0.504869 ||g||=9.64e-02 r=0.601 λ=2.50e-05 α=2.388e-01\n",
      "Epoch   40 f=0.515299 test=0.501034 ||g||=4.93e-02 r=0.965 λ=2.44e-08 α=2.796e-01\n",
      "Epoch   49 f=0.513278 test=0.501449 ||g||=4.02e-02 r=0.917 λ=9.54e-11 α=1.732e-01\n",
      "Final - Train 0.513278 Test 0.501449\n",
      "Initial - Train 34.848946 Test 34.846596\n",
      "Epoch    0 f=6.658435 test=6.554737 ||g||=5.00e+00 r=1.154 λ=5.00e-05 α=2.158e-01\n",
      "Epoch   10 f=2.041204 test=1.705006 ||g||=4.64e+00 r=1.243 λ=1.64e+00 α=7.805e-02\n",
      "Epoch   20 f=0.543201 test=0.520157 ||g||=1.49e-01 r=1.726 λ=1.60e-03 α=2.643e-01\n",
      "Epoch   30 f=0.519886 test=0.509166 ||g||=4.13e-02 r=0.984 λ=1.56e-06 α=4.835e-01\n",
      "Epoch   40 f=0.511778 test=0.498095 ||g||=6.29e-02 r=0.997 λ=3.05e-09 α=5.897e-01\n",
      "Epoch   49 f=0.507417 test=0.498579 ||g||=4.67e-02 r=0.943 λ=2.38e-11 α=2.867e-01\n",
      "Final - Train 0.507417 Test 0.498579\n",
      "Initial - Train 36.470943 Test 36.375965\n",
      "Epoch    0 f=7.826817 test=7.689972 ||g||=5.00e+00 r=1.210 λ=5.00e-06 α=1.995e-01\n",
      "Epoch   10 f=40.299084 test=39.660042 ||g||=5.00e+00 r=-3.405 λ=5.24e+00 α=7.629e-01\n",
      "Epoch   20 f=0.615819 test=0.592583 ||g||=5.03e-01 r=1.044 λ=2.62e+00 α=1.833e-01\n",
      "Epoch   30 f=0.556523 test=0.530446 ||g||=3.02e-01 r=-0.166 λ=1.05e+01 α=7.016e-01\n",
      "Epoch   40 f=0.556523 test=0.530446 ||g||=3.02e-01 r=-0.166 λ=1.07e+04 α=7.016e-01\n",
      "Epoch   49 f=0.556523 test=0.530446 ||g||=3.02e-01 r=-0.166 λ=5.50e+06 α=7.016e-01\n",
      "Final - Train 0.556523 Test 0.530446\n",
      "Initial - Train 34.247143 Test 34.215733\n",
      "Epoch    0 f=2.556200 test=2.659488 ||g||=5.00e+00 r=0.938 λ=5.00e-06 α=2.843e-01\n",
      "Epoch   10 f=11.458899 test=11.461236 ||g||=5.00e+00 r=-0.934 λ=5.24e+00 α=7.629e-01\n",
      "Epoch   20 f=0.561491 test=0.541558 ||g||=2.02e-01 r=0.983 λ=2.05e-02 α=3.066e-01\n",
      "Epoch   30 f=0.540754 test=0.532725 ||g||=1.26e-01 r=-0.771 λ=8.19e-02 α=1.000e+00\n",
      "Epoch   40 f=0.520964 test=0.504858 ||g||=4.72e-02 r=0.947 λ=4.10e-02 α=1.767e-01\n",
      "Epoch   49 f=0.517793 test=0.503085 ||g||=5.69e-02 r=0.585 λ=1.28e-03 α=3.129e-01\n",
      "Final - Train 0.517793 Test 0.503085\n",
      "Initial - Train 34.848946 Test 34.846596\n",
      "Epoch    0 f=6.658435 test=6.554737 ||g||=5.00e+00 r=1.154 λ=5.00e-06 α=2.158e-01\n",
      "Epoch   10 f=35.275909 test=32.530369 ||g||=5.00e+00 r=-3.001 λ=5.24e+00 α=7.629e-01\n",
      "Epoch   20 f=7.318598 test=6.710066 ||g||=2.60e+00 r=-1.202 λ=2.68e+03 α=1.000e+00\n",
      "Epoch   30 f=7.318598 test=6.710066 ||g||=2.60e+00 r=-1.202 λ=2.75e+06 α=1.000e+00\n",
      "Epoch   40 f=7.318598 test=6.710066 ||g||=2.60e+00 r=-1.202 λ=2.81e+09 α=1.000e+00\n",
      "Epoch   49 f=7.318598 test=6.710066 ||g||=2.60e+00 r=-1.202 λ=1.44e+12 α=1.000e+00\n",
      "Final - Train 7.318598 Test 6.710066\n",
      "Initial - Train 36.470943 Test 36.375965\n",
      "Epoch    0 f=7.826817 test=7.689972 ||g||=5.00e+00 r=1.210 λ=5.00e-07 α=1.995e-01\n",
      "Epoch   10 f=101.209198 test=99.507950 ||g||=5.00e+00 r=-3.997 λ=5.24e-01 α=1.000e+00\n",
      "Epoch   20 f=0.563997 test=0.543176 ||g||=3.39e-01 r=1.044 λ=6.55e-02 α=1.000e+00\n",
      "Epoch   30 f=0.528479 test=0.510022 ||g||=2.21e-01 r=0.119 λ=1.31e-01 α=1.000e+00\n",
      "Epoch   40 f=0.539181 test=0.523704 ||g||=1.49e-01 r=-1.456 λ=8.19e-03 α=1.000e+00\n",
      "Epoch   49 f=0.539181 test=0.523704 ||g||=1.49e-01 r=-1.456 λ=4.19e+00 α=1.000e+00\n",
      "Final - Train 0.539181 Test 0.523704\n",
      "Initial - Train 34.247143 Test 34.215733\n",
      "Epoch    0 f=2.556200 test=2.659488 ||g||=5.00e+00 r=0.938 λ=5.00e-07 α=2.843e-01\n",
      "Epoch   10 f=19.040398 test=19.020035 ||g||=5.00e+00 r=-0.706 λ=5.24e-01 α=1.000e+00\n",
      "Epoch   20 f=1.142324 test=1.081549 ||g||=5.05e-01 r=-2.328 λ=5.24e-01 α=1.000e+00\n",
      "Epoch   30 f=0.530028 test=0.514386 ||g||=1.69e-01 r=0.851 λ=8.19e-03 α=1.839e-01\n",
      "Epoch   40 f=0.519191 test=0.506489 ||g||=8.21e-02 r=0.677 λ=6.40e-05 α=3.832e-01\n",
      "Epoch   49 f=0.517737 test=0.503855 ||g||=1.92e-02 r=0.910 λ=2.50e-07 α=2.273e-01\n",
      "Final - Train 0.517737 Test 0.503855\n",
      "Initial - Train 34.848946 Test 34.846596\n",
      "Epoch    0 f=6.658435 test=6.554737 ||g||=5.00e+00 r=1.154 λ=5.00e-07 α=2.158e-01\n",
      "Epoch   10 f=81.379875 test=75.859634 ||g||=5.00e+00 r=-3.198 λ=5.24e-01 α=1.000e+00\n",
      "Epoch   20 f=0.615886 test=0.592743 ||g||=4.28e-01 r=1.203 λ=6.55e-02 α=5.545e-01\n",
      "Epoch   30 f=0.528305 test=0.514878 ||g||=7.73e-02 r=1.003 λ=6.40e-05 α=3.462e-01\n",
      "Epoch   40 f=0.516434 test=0.505542 ||g||=6.72e-02 r=0.816 λ=6.25e-08 α=5.311e-01\n",
      "Epoch   49 f=0.517826 test=0.512933 ||g||=6.80e-02 r=-1.105 λ=8.00e-06 α=7.967e-01\n",
      "Final - Train 0.517826 Test 0.512933\n",
      "Initial - Train 36.470943 Test 36.375965\n",
      "Epoch    0 f=7.826817 test=7.689972 ||g||=5.00e+00 r=1.210 λ=5.00e-08 α=1.995e-01\n",
      "Epoch   10 f=101.209198 test=99.507950 ||g||=5.00e+00 r=-3.760 λ=5.24e-02 α=1.000e+00\n",
      "Epoch   20 f=0.611402 test=0.604698 ||g||=4.68e-01 r=1.105 λ=5.24e-02 α=2.405e-01\n",
      "Epoch   30 f=0.527080 test=0.508571 ||g||=8.53e-02 r=1.737 λ=5.12e-05 α=6.870e-02\n",
      "Epoch   40 f=0.513106 test=0.495981 ||g||=1.24e-01 r=0.904 λ=1.00e-07 α=5.764e-01\n",
      "Epoch   49 f=0.504915 test=0.489531 ||g||=4.74e-02 r=1.020 λ=1.95e-10 α=2.374e-01\n",
      "Final - Train 0.504915 Test 0.489531\n",
      "Initial - Train 34.247143 Test 34.215733\n",
      "Epoch    0 f=2.556200 test=2.659488 ||g||=5.00e+00 r=0.938 λ=5.00e-08 α=2.843e-01\n",
      "Epoch   10 f=19.040398 test=19.020035 ||g||=5.00e+00 r=-0.664 λ=5.24e-02 α=1.000e+00\n",
      "Epoch   20 f=0.600491 test=0.566305 ||g||=3.99e-01 r=1.047 λ=4.19e-01 α=1.285e-01\n",
      "Epoch   30 f=0.522389 test=0.514020 ||g||=9.60e-02 r=1.029 λ=4.10e-04 α=2.200e-01\n",
      "Epoch   40 f=0.515125 test=0.502916 ||g||=6.49e-02 r=0.716 λ=1.60e-06 α=3.889e-01\n",
      "Epoch   49 f=0.513119 test=0.501251 ||g||=3.76e-02 r=0.771 λ=6.25e-09 α=4.829e-01\n",
      "Final - Train 0.513119 Test 0.501251\n",
      "Initial - Train 34.848946 Test 34.846596\n",
      "Epoch    0 f=6.658435 test=6.554737 ||g||=5.00e+00 r=1.154 λ=5.00e-08 α=2.158e-01\n",
      "Epoch   10 f=81.379875 test=75.859634 ||g||=5.00e+00 r=-3.009 λ=5.24e-02 α=1.000e+00\n",
      "Epoch   20 f=0.629129 test=0.592157 ||g||=5.78e-01 r=1.021 λ=5.24e-02 α=1.000e+00\n",
      "Epoch   30 f=0.525096 test=0.512136 ||g||=8.35e-02 r=1.001 λ=1.02e-04 α=2.681e-01\n",
      "Epoch   40 f=0.515052 test=0.506792 ||g||=3.34e-02 r=0.984 λ=4.00e-07 α=2.969e-01\n",
      "Epoch   49 f=0.507811 test=0.496347 ||g||=3.27e-02 r=1.076 λ=1.56e-09 α=1.000e+00\n",
      "Final - Train 0.507811 Test 0.496347\n",
      "Initial - Train 36.470943 Test 36.375965\n",
      "Epoch    0 f=7.620495 test=7.481933 ||g||=5.00e+00 r=1.201 λ=5.00e-05 α=2.024e-01\n",
      "Epoch   10 f=1.584637 test=1.524207 ||g||=3.86e+00 r=0.935 λ=1.64e+00 α=4.173e-02\n",
      "Epoch   20 f=0.530818 test=0.507209 ||g||=1.13e-01 r=0.992 λ=1.60e-03 α=6.122e-01\n",
      "Epoch   30 f=0.518607 test=0.500153 ||g||=1.41e-01 r=0.903 λ=3.13e-06 α=2.849e-01\n",
      "Epoch   40 f=0.501353 test=0.492598 ||g||=7.55e-02 r=1.135 λ=3.05e-09 α=4.921e-01\n",
      "Epoch   49 f=0.496313 test=0.493372 ||g||=5.80e-02 r=0.847 λ=5.96e-12 α=2.921e-01\n",
      "Final - Train 0.496313 Test 0.493372\n",
      "Initial - Train 34.247143 Test 34.215733\n",
      "Epoch    0 f=2.150346 test=2.273282 ||g||=5.00e+00 r=0.975 λ=5.00e-05 α=2.769e-01\n",
      "Epoch   10 f=1.034968 test=1.081355 ||g||=1.29e+00 r=1.170 λ=1.31e+01 α=7.629e-02\n",
      "Epoch   20 f=0.533425 test=0.509552 ||g||=1.75e-01 r=1.106 λ=1.28e-02 α=1.843e-01\n",
      "Epoch   30 f=0.517405 test=0.505052 ||g||=8.24e-02 r=0.841 λ=1.25e-05 α=3.731e-01\n",
      "Epoch   40 f=0.525595 test=0.512518 ||g||=5.66e-02 r=-2.894 λ=2.56e-02 α=1.000e+00\n",
      "Epoch   49 f=0.515588 test=0.503670 ||g||=4.41e-02 r=0.956 λ=4.10e-01 α=2.038e-01\n",
      "Final - Train 0.515588 Test 0.503670\n",
      "Initial - Train 34.848946 Test 34.846596\n",
      "Epoch    0 f=6.764953 test=6.665557 ||g||=5.00e+00 r=1.162 λ=5.00e-05 α=2.135e-01\n",
      "Epoch   10 f=2.076851 test=1.741988 ||g||=4.73e+00 r=1.222 λ=1.64e+00 α=7.355e-02\n",
      "Epoch   20 f=0.536175 test=0.507883 ||g||=1.80e-01 r=0.905 λ=1.60e-03 α=1.000e+00\n",
      "Epoch   30 f=0.517039 test=0.504764 ||g||=4.70e-02 r=0.947 λ=1.56e-06 α=8.864e-01\n",
      "Epoch   40 f=0.511302 test=0.498889 ||g||=6.18e-02 r=1.008 λ=1.53e-09 α=6.394e-01\n",
      "Epoch   49 f=0.508187 test=0.496613 ||g||=5.57e-02 r=0.729 λ=1.19e-11 α=7.940e-01\n",
      "Final - Train 0.508187 Test 0.496613\n",
      "Initial - Train 36.470943 Test 36.375965\n",
      "Epoch    0 f=7.620495 test=7.481933 ||g||=5.00e+00 r=1.201 λ=5.00e-06 α=2.024e-01\n",
      "Epoch   10 f=40.952896 test=40.254166 ||g||=5.00e+00 r=-3.495 λ=5.24e+00 α=7.629e-01\n",
      "Epoch   20 f=0.550442 test=0.531008 ||g||=2.90e-01 r=1.007 λ=1.02e-02 α=3.850e-01\n",
      "Epoch   30 f=0.522551 test=0.505510 ||g||=6.67e-02 r=0.964 λ=1.00e-05 α=2.017e-01\n",
      "Epoch   40 f=0.507255 test=0.492230 ||g||=4.57e-02 r=1.018 λ=1.95e-08 α=1.527e-01\n",
      "Epoch   49 f=0.575187 test=0.552002 ||g||=1.69e-01 r=-2.885 λ=1.00e-05 α=1.000e+00\n",
      "Final - Train 0.575187 Test 0.552002\n",
      "Initial - Train 34.247143 Test 34.215733\n",
      "Epoch    0 f=2.150346 test=2.273282 ||g||=5.00e+00 r=0.975 λ=5.00e-06 α=2.769e-01\n",
      "Epoch   10 f=10.990462 test=10.977108 ||g||=5.00e+00 r=-0.927 λ=5.24e+00 α=7.629e-01\n",
      "Epoch   20 f=0.564178 test=0.543804 ||g||=2.57e-01 r=0.957 λ=4.10e-02 α=2.913e-01\n",
      "Epoch   30 f=0.520792 test=0.504875 ||g||=1.16e-01 r=0.960 λ=4.00e-05 α=3.372e-01\n",
      "Epoch   40 f=0.515524 test=0.502419 ||g||=3.90e-02 r=0.989 λ=1.56e-07 α=2.060e-01\n",
      "Epoch   49 f=0.513597 test=0.500167 ||g||=2.47e-02 r=0.974 λ=1.22e-09 α=2.058e-01\n",
      "Final - Train 0.513597 Test 0.500167\n",
      "Initial - Train 34.848946 Test 34.846596\n",
      "Epoch    0 f=6.764953 test=6.665557 ||g||=5.00e+00 r=1.162 λ=5.00e-06 α=2.135e-01\n",
      "Epoch   10 f=36.541477 test=33.790897 ||g||=5.00e+00 r=-3.122 λ=5.24e+00 α=7.629e-01\n",
      "Epoch   20 f=0.548413 test=0.519102 ||g||=1.48e-01 r=0.927 λ=1.02e-02 α=4.246e-01\n",
      "Epoch   30 f=0.518947 test=0.507597 ||g||=6.36e-02 r=0.912 λ=1.00e-05 α=3.172e-01\n",
      "Epoch   40 f=0.512171 test=0.500102 ||g||=4.67e-02 r=0.985 λ=3.91e-08 α=2.019e-01\n",
      "Epoch   49 f=0.509567 test=0.499405 ||g||=3.55e-02 r=1.033 λ=3.05e-10 α=5.149e-01\n",
      "Final - Train 0.509567 Test 0.499405\n",
      "Initial - Train 36.470943 Test 36.375965\n",
      "Epoch    0 f=7.620495 test=7.481933 ||g||=5.00e+00 r=1.201 λ=5.00e-07 α=2.024e-01\n",
      "Epoch   10 f=102.344406 test=100.514465 ||g||=5.00e+00 r=-4.055 λ=5.24e-01 α=1.000e+00\n",
      "Epoch   20 f=0.560647 test=0.539441 ||g||=3.01e-01 r=1.051 λ=6.55e-02 α=1.000e+00\n",
      "Epoch   30 f=0.541421 test=0.541738 ||g||=1.15e-01 r=-1.328 λ=2.62e-01 α=1.000e+00\n",
      "Epoch   40 f=0.518215 test=0.503482 ||g||=2.23e-01 r=0.229 λ=3.28e-02 α=6.959e-01\n",
      "Epoch   49 f=0.511431 test=0.499649 ||g||=8.95e-02 r=-0.280 λ=2.62e-01 α=1.000e+00\n",
      "Final - Train 0.511431 Test 0.499649\n",
      "Initial - Train 34.247143 Test 34.215733\n",
      "Epoch    0 f=2.150346 test=2.273282 ||g||=5.00e+00 r=0.975 λ=5.00e-07 α=2.769e-01\n",
      "Epoch   10 f=19.336231 test=19.314980 ||g||=5.00e+00 r=-0.736 λ=5.24e-01 α=1.000e+00\n",
      "Epoch   20 f=0.568672 test=0.537045 ||g||=2.13e-01 r=1.018 λ=1.31e-01 α=1.173e-01\n",
      "Epoch   30 f=0.524719 test=0.512349 ||g||=1.11e-01 r=0.903 λ=1.28e-04 α=3.525e-01\n",
      "Epoch   40 f=0.521687 test=0.507247 ||g||=8.30e-02 r=0.927 λ=5.24e-01 α=2.712e-01\n",
      "Epoch   49 f=0.519253 test=0.502975 ||g||=6.06e-02 r=0.974 λ=1.02e-03 α=3.353e-01\n",
      "Final - Train 0.519253 Test 0.502975\n",
      "Initial - Train 34.848946 Test 34.846596\n",
      "Epoch    0 f=6.764953 test=6.665557 ||g||=5.00e+00 r=1.162 λ=5.00e-07 α=2.135e-01\n",
      "Epoch   10 f=85.161827 test=79.574371 ||g||=5.00e+00 r=-3.356 λ=5.24e-01 α=1.000e+00\n",
      "Epoch   20 f=0.620537 test=0.597163 ||g||=4.38e-01 r=1.230 λ=6.55e-02 α=5.293e-01\n",
      "Epoch   30 f=0.525227 test=0.513036 ||g||=6.18e-02 r=1.039 λ=6.40e-05 α=2.442e-01\n",
      "Epoch   40 f=0.515088 test=0.507040 ||g||=6.20e-02 r=0.987 λ=6.25e-08 α=8.473e-01\n",
      "Epoch   49 f=0.508439 test=0.500618 ||g||=6.91e-02 r=0.989 λ=1.22e-10 α=4.835e-01\n",
      "Final - Train 0.508439 Test 0.500618\n",
      "Initial - Train 36.470943 Test 36.375965\n",
      "Epoch    0 f=7.620495 test=7.481933 ||g||=5.00e+00 r=1.201 λ=5.00e-08 α=2.024e-01\n",
      "Epoch   10 f=102.344406 test=100.514465 ||g||=5.00e+00 r=-3.814 λ=5.24e-02 α=1.000e+00\n",
      "Epoch   20 f=0.598987 test=0.585180 ||g||=3.56e-01 r=1.033 λ=5.24e-02 α=2.889e-01\n",
      "Epoch   30 f=0.524588 test=0.512529 ||g||=8.23e-02 r=1.009 λ=5.12e-05 α=2.984e-01\n",
      "Epoch   40 f=0.506220 test=0.493833 ||g||=9.24e-02 r=1.054 λ=5.00e-08 α=1.518e-01\n",
      "Epoch   49 f=0.499630 test=0.488733 ||g||=4.80e-02 r=0.887 λ=1.95e-10 α=5.605e-01\n",
      "Final - Train 0.499630 Test 0.488733\n",
      "Initial - Train 34.247143 Test 34.215733\n",
      "Epoch    0 f=2.150346 test=2.273282 ||g||=5.00e+00 r=0.975 λ=5.00e-08 α=2.769e-01\n",
      "Epoch   10 f=19.336231 test=19.314980 ||g||=5.00e+00 r=-0.692 λ=5.24e-02 α=1.000e+00\n",
      "Epoch   20 f=0.583996 test=0.553104 ||g||=3.65e-01 r=1.052 λ=4.19e-01 α=1.319e-01\n",
      "Epoch   30 f=0.522715 test=0.509783 ||g||=1.21e-01 r=0.884 λ=4.10e-04 α=4.761e-01\n",
      "Epoch   40 f=0.515285 test=0.503989 ||g||=4.90e-02 r=0.900 λ=4.00e-07 α=3.974e-01\n",
      "Epoch   49 f=0.513127 test=0.500933 ||g||=3.22e-02 r=0.776 λ=1.56e-09 α=1.390e-01\n",
      "Final - Train 0.513127 Test 0.500933\n",
      "Initial - Train 34.848946 Test 34.846596\n",
      "Epoch    0 f=6.764953 test=6.665557 ||g||=5.00e+00 r=1.162 λ=5.00e-08 α=2.135e-01\n",
      "Epoch   10 f=85.161827 test=79.574371 ||g||=5.00e+00 r=-3.157 λ=5.24e-02 α=1.000e+00\n",
      "Epoch   20 f=0.638852 test=0.618492 ||g||=4.54e-01 r=1.121 λ=5.24e-02 α=4.712e-01\n",
      "Epoch   30 f=0.543483 test=0.516858 ||g||=1.11e-01 r=-1.263 λ=3.28e-03 α=1.000e+00\n",
      "Epoch   40 f=0.518663 test=0.510389 ||g||=6.12e-02 r=1.007 λ=2.62e-02 α=2.124e-01\n",
      "Epoch   49 f=0.511435 test=0.499279 ||g||=7.02e-02 r=1.068 λ=5.12e-05 α=8.701e-01\n",
      "Final - Train 0.511435 Test 0.499279\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 23.402880, Test Loss = 23.331703, Grad Norm = 11.778470, dt = 0.500000\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 20.558006, Test Loss = 20.718374, Grad Norm = 12.989841, dt = 0.500000\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 22.002089, Test Loss = 22.028389, Grad Norm = 11.552635, dt = 0.500000\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 23.402880, Test Loss = 23.331703, Grad Norm = 11.778470, dt = 0.500000\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 20.558006, Test Loss = 20.718374, Grad Norm = 12.989841, dt = 0.500000\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 22.002089, Test Loss = 22.028389, Grad Norm = 11.552635, dt = 0.500000\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 23.402880, Test Loss = 23.331703, Grad Norm = 11.778470, dt = 0.500000\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 20.558006, Test Loss = 20.718374, Grad Norm = 12.989841, dt = 0.500000\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 22.002089, Test Loss = 22.028389, Grad Norm = 11.552635, dt = 0.500000\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 29.376585, Test Loss = 29.301785, Grad Norm = 13.271094, dt = 0.500000\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 26.973223, Test Loss = 27.052242, Grad Norm = 13.997532, dt = 0.500000\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 27.893440, Test Loss = 27.913403, Grad Norm = 13.041268, dt = 0.500000\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 29.376585, Test Loss = 29.301785, Grad Norm = 13.271094, dt = 0.500000\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 26.973223, Test Loss = 27.052242, Grad Norm = 13.997532, dt = 0.500000\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 27.893440, Test Loss = 27.913403, Grad Norm = 13.041268, dt = 0.500000\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 29.376585, Test Loss = 29.301785, Grad Norm = 13.271094, dt = 0.500000\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 26.973223, Test Loss = 27.052242, Grad Norm = 13.997532, dt = 0.500000\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 27.893440, Test Loss = 27.913403, Grad Norm = 13.041268, dt = 0.500000\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 34.955570, Test Loss = 34.865692, Grad Norm = 14.915675, dt = 0.500000\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 32.722893, Test Loss = 32.715534, Grad Norm = 15.083247, dt = 0.500000\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 33.367611, Test Loss = 33.370743, Grad Norm = 14.592171, dt = 0.500000\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 34.955570, Test Loss = 34.865692, Grad Norm = 14.915675, dt = 0.500000\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 32.722893, Test Loss = 32.715534, Grad Norm = 15.083247, dt = 0.500000\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 33.367611, Test Loss = 33.370743, Grad Norm = 14.592171, dt = 0.500000\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 34.955570, Test Loss = 34.865692, Grad Norm = 14.915675, dt = 0.500000\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 32.722893, Test Loss = 32.715534, Grad Norm = 15.083247, dt = 0.500000\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 33.367611, Test Loss = 33.370743, Grad Norm = 14.592171, dt = 0.500000\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 31.959301, Test Loss = 31.878517, Grad Norm = 14.002445, dt = 0.200200\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 29.658972, Test Loss = 29.698364, Grad Norm = 14.470092, dt = 0.200200\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 30.529694, Test Loss = 30.542269, Grad Norm = 13.763682, dt = 0.200200\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 31.959301, Test Loss = 31.878517, Grad Norm = 14.002445, dt = 0.200200\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 29.658972, Test Loss = 29.698364, Grad Norm = 14.470092, dt = 0.200200\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 30.529694, Test Loss = 30.542269, Grad Norm = 13.763682, dt = 0.200200\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 31.959301, Test Loss = 31.878517, Grad Norm = 14.002445, dt = 0.200200\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 29.658972, Test Loss = 29.698364, Grad Norm = 14.470092, dt = 0.200200\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 30.529694, Test Loss = 30.542269, Grad Norm = 13.763682, dt = 0.200200\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 31.959301, Test Loss = 31.878517, Grad Norm = 14.002445, dt = 0.200200\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 29.658972, Test Loss = 29.698364, Grad Norm = 14.470092, dt = 0.200200\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 30.529694, Test Loss = 30.542269, Grad Norm = 13.763682, dt = 0.200200\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 31.959301, Test Loss = 31.878517, Grad Norm = 14.002445, dt = 0.200200\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 29.658972, Test Loss = 29.698364, Grad Norm = 14.470092, dt = 0.200200\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 30.529694, Test Loss = 30.542269, Grad Norm = 13.763682, dt = 0.200200\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 31.959301, Test Loss = 31.878517, Grad Norm = 14.002445, dt = 0.200200\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 29.658972, Test Loss = 29.698364, Grad Norm = 14.470092, dt = 0.200200\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 30.529694, Test Loss = 30.542269, Grad Norm = 13.763682, dt = 0.200200\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 34.955570, Test Loss = 34.865692, Grad Norm = 14.915675, dt = 0.200000\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 32.722893, Test Loss = 32.715534, Grad Norm = 15.083246, dt = 0.200000\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 33.367611, Test Loss = 33.370743, Grad Norm = 14.592171, dt = 0.200000\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 34.955570, Test Loss = 34.865692, Grad Norm = 14.915675, dt = 0.200000\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 32.722893, Test Loss = 32.715534, Grad Norm = 15.083246, dt = 0.200000\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 33.367611, Test Loss = 33.370743, Grad Norm = 14.592171, dt = 0.200000\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 34.955570, Test Loss = 34.865692, Grad Norm = 14.915675, dt = 0.200000\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 32.722893, Test Loss = 32.715534, Grad Norm = 15.083246, dt = 0.200000\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 33.367611, Test Loss = 33.370743, Grad Norm = 14.592171, dt = 0.200000\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 35.299385, Test Loss = 35.208382, Grad Norm = 15.025667, dt = 0.100100\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 33.069065, Test Loss = 33.056232, Grad Norm = 15.156363, dt = 0.100100\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 33.730236, Test Loss = 33.732075, Grad Norm = 14.702180, dt = 0.100100\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 35.299385, Test Loss = 35.208382, Grad Norm = 15.025667, dt = 0.100100\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 33.069065, Test Loss = 33.056232, Grad Norm = 15.156363, dt = 0.100100\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 33.730236, Test Loss = 33.732075, Grad Norm = 14.702180, dt = 0.100100\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 35.299385, Test Loss = 35.208382, Grad Norm = 15.025667, dt = 0.100100\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 33.069065, Test Loss = 33.056232, Grad Norm = 15.156363, dt = 0.100100\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 33.730236, Test Loss = 33.732075, Grad Norm = 14.702180, dt = 0.100100\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 35.299385, Test Loss = 35.208382, Grad Norm = 15.025667, dt = 0.100100\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 33.069065, Test Loss = 33.056232, Grad Norm = 15.156363, dt = 0.100100\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 33.730236, Test Loss = 33.732075, Grad Norm = 14.702180, dt = 0.100100\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 35.299385, Test Loss = 35.208382, Grad Norm = 15.025667, dt = 0.100100\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 33.069065, Test Loss = 33.056232, Grad Norm = 15.156363, dt = 0.100100\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 33.730236, Test Loss = 33.732075, Grad Norm = 14.702180, dt = 0.100100\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 35.299385, Test Loss = 35.208382, Grad Norm = 15.025667, dt = 0.100100\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 33.069065, Test Loss = 33.056232, Grad Norm = 15.156363, dt = 0.100100\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 33.730236, Test Loss = 33.732075, Grad Norm = 14.702180, dt = 0.100100\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 35.299385, Test Loss = 35.208382, Grad Norm = 15.025667, dt = 0.100100\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 33.069065, Test Loss = 33.056232, Grad Norm = 15.156363, dt = 0.100100\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 33.730236, Test Loss = 33.732075, Grad Norm = 14.702180, dt = 0.100100\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 35.299385, Test Loss = 35.208382, Grad Norm = 15.025667, dt = 0.100100\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 33.069065, Test Loss = 33.056232, Grad Norm = 15.156363, dt = 0.100100\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 33.730236, Test Loss = 33.732075, Grad Norm = 14.702180, dt = 0.100100\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 35.299385, Test Loss = 35.208382, Grad Norm = 15.025667, dt = 0.100100\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 33.069065, Test Loss = 33.056232, Grad Norm = 15.156363, dt = 0.100100\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 33.730236, Test Loss = 33.732075, Grad Norm = 14.702180, dt = 0.100100\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 36.175259, Test Loss = 36.081306, Grad Norm = 15.308257, dt = 0.050050\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 33.950676, Test Loss = 33.923958, Grad Norm = 15.349122, dt = 0.050050\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 34.566799, Test Loss = 34.565533, Grad Norm = 14.958095, dt = 0.050050\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 36.175259, Test Loss = 36.081306, Grad Norm = 15.308257, dt = 0.050050\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 33.950676, Test Loss = 33.923958, Grad Norm = 15.349122, dt = 0.050050\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 34.566799, Test Loss = 34.565533, Grad Norm = 14.958095, dt = 0.050050\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 36.175259, Test Loss = 36.081306, Grad Norm = 15.308257, dt = 0.050050\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 33.950676, Test Loss = 33.923958, Grad Norm = 15.349122, dt = 0.050050\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 34.566799, Test Loss = 34.565533, Grad Norm = 14.958095, dt = 0.050050\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 36.175259, Test Loss = 36.081306, Grad Norm = 15.308257, dt = 0.050050\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 33.950676, Test Loss = 33.923958, Grad Norm = 15.349122, dt = 0.050050\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 34.566799, Test Loss = 34.565533, Grad Norm = 14.958095, dt = 0.050050\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 36.175259, Test Loss = 36.081306, Grad Norm = 15.308257, dt = 0.050050\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 33.950676, Test Loss = 33.923958, Grad Norm = 15.349122, dt = 0.050050\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 34.566799, Test Loss = 34.565533, Grad Norm = 14.958095, dt = 0.050050\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 36.175259, Test Loss = 36.081306, Grad Norm = 15.308257, dt = 0.050050\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 33.950676, Test Loss = 33.923958, Grad Norm = 15.349122, dt = 0.050050\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 34.566799, Test Loss = 34.565533, Grad Norm = 14.958095, dt = 0.050050\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 36.175259, Test Loss = 36.081306, Grad Norm = 15.308257, dt = 0.050050\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 33.950676, Test Loss = 33.923958, Grad Norm = 15.349122, dt = 0.050050\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 34.566799, Test Loss = 34.565533, Grad Norm = 14.958095, dt = 0.050050\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 36.175259, Test Loss = 36.081306, Grad Norm = 15.308257, dt = 0.050050\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 33.950676, Test Loss = 33.923958, Grad Norm = 15.349122, dt = 0.050050\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 34.566799, Test Loss = 34.565533, Grad Norm = 14.958095, dt = 0.050050\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 36.175259, Test Loss = 36.081306, Grad Norm = 15.308257, dt = 0.050050\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 33.950676, Test Loss = 33.923958, Grad Norm = 15.349122, dt = 0.050050\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 34.566799, Test Loss = 34.565533, Grad Norm = 14.958095, dt = 0.050050\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 36.459080, Test Loss = 36.364143, Grad Norm = 15.400520, dt = 0.010010\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 34.235260, Test Loss = 34.204037, Grad Norm = 15.414886, dt = 0.010010\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 34.837627, Test Loss = 34.835320, Grad Norm = 15.042277, dt = 0.010010\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 36.459080, Test Loss = 36.364143, Grad Norm = 15.400520, dt = 0.010010\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 34.235260, Test Loss = 34.204037, Grad Norm = 15.414886, dt = 0.010010\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 34.837627, Test Loss = 34.835320, Grad Norm = 15.042277, dt = 0.010010\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 36.459080, Test Loss = 36.364143, Grad Norm = 15.400520, dt = 0.010010\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 34.235260, Test Loss = 34.204037, Grad Norm = 15.414886, dt = 0.010010\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 34.837627, Test Loss = 34.835320, Grad Norm = 15.042277, dt = 0.010010\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 36.459080, Test Loss = 36.364143, Grad Norm = 15.400520, dt = 0.010010\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 34.235260, Test Loss = 34.204037, Grad Norm = 15.414886, dt = 0.010010\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 34.837627, Test Loss = 34.835320, Grad Norm = 15.042277, dt = 0.010010\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 36.459080, Test Loss = 36.364143, Grad Norm = 15.400520, dt = 0.010010\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 34.235260, Test Loss = 34.204037, Grad Norm = 15.414886, dt = 0.010010\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 34.837627, Test Loss = 34.835320, Grad Norm = 15.042277, dt = 0.010010\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 36.459080, Test Loss = 36.364143, Grad Norm = 15.400520, dt = 0.010010\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 34.235260, Test Loss = 34.204037, Grad Norm = 15.414886, dt = 0.010010\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 34.837627, Test Loss = 34.835320, Grad Norm = 15.042277, dt = 0.010010\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 36.459080, Test Loss = 36.364143, Grad Norm = 15.400520, dt = 0.010010\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 34.235260, Test Loss = 34.204037, Grad Norm = 15.414886, dt = 0.010010\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 34.837627, Test Loss = 34.835320, Grad Norm = 15.042277, dt = 0.010010\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 36.459080, Test Loss = 36.364143, Grad Norm = 15.400520, dt = 0.010010\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 34.235260, Test Loss = 34.204037, Grad Norm = 15.414886, dt = 0.010010\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 34.837627, Test Loss = 34.835320, Grad Norm = 15.042277, dt = 0.010010\n",
      "Initial loss: 36.470943, Gradient norm: 15.404110\n",
      "Epoch 0: Train Loss = 36.459080, Test Loss = 36.364143, Grad Norm = 15.400520, dt = 0.010010\n",
      "Initial loss: 34.247143, Gradient norm: 15.417592\n",
      "Epoch 0: Train Loss = 34.235260, Test Loss = 34.204037, Grad Norm = 15.414886, dt = 0.010010\n",
      "Initial loss: 34.848946, Gradient norm: 15.045812\n",
      "Epoch 0: Train Loss = 34.837627, Test Loss = 34.835320, Grad Norm = 15.042277, dt = 0.010010\n",
      "Initial - Train 0.472132 Test 0.471422\n",
      "Epoch    0 f=0.251975 test=0.250907 ||g||=2.51e-01 r=1.054 λ=5.00e-05 α=1.000e+00\n",
      "Epoch   10 f=0.168561 test=0.171245 ||g||=3.66e-02 r=1.196 λ=4.88e-08 α=1.000e+00\n",
      "Epoch   20 f=0.160998 test=0.166879 ||g||=1.97e-02 r=0.989 λ=9.54e-11 α=1.000e+00\n",
      "Epoch   30 f=0.158537 test=0.167061 ||g||=1.24e-02 r=0.995 λ=1.86e-13 α=1.000e+00\n",
      "Epoch   40 f=0.157510 test=0.167548 ||g||=8.83e-03 r=0.990 λ=7.28e-16 α=1.000e+00\n",
      "Epoch   49 f=0.157027 test=0.167646 ||g||=5.95e-03 r=1.140 λ=1.42e-18 α=1.000e+00\n",
      "Final - Train 0.157027 Test 0.167646\n",
      "Initial - Train 0.714481 Test 0.717420\n",
      "Epoch    0 f=0.332699 test=0.336665 ||g||=3.53e-01 r=0.993 λ=5.00e-05 α=1.000e+00\n",
      "Epoch   10 f=0.171753 test=0.177392 ||g||=4.14e-02 r=0.949 λ=1.95e-07 α=1.000e+00\n",
      "Epoch   20 f=0.162680 test=0.170512 ||g||=2.30e-02 r=0.984 λ=7.63e-10 α=1.000e+00\n",
      "Epoch   30 f=0.159446 test=0.169084 ||g||=1.36e-02 r=0.996 λ=5.96e-12 α=1.000e+00\n",
      "Epoch   40 f=0.158197 test=0.168355 ||g||=9.22e-03 r=0.976 λ=5.82e-15 α=1.000e+00\n",
      "Epoch   49 f=0.157564 test=0.167914 ||g||=7.13e-03 r=0.999 λ=2.27e-17 α=1.000e+00\n",
      "Final - Train 0.157564 Test 0.167914\n",
      "Initial - Train 0.999561 Test 0.994798\n",
      "Epoch    0 f=0.424066 test=0.420512 ||g||=4.12e-01 r=1.072 λ=5.00e-05 α=1.000e+00\n",
      "Epoch   10 f=0.187534 test=0.184603 ||g||=5.01e-02 r=1.004 λ=4.88e-08 α=1.000e+00\n",
      "Epoch   20 f=0.173800 test=0.172349 ||g||=2.97e-02 r=0.996 λ=1.91e-10 α=1.000e+00\n",
      "Epoch   30 f=0.167081 test=0.167393 ||g||=2.27e-02 r=1.002 λ=3.73e-13 α=1.000e+00\n",
      "Epoch   40 f=0.163197 test=0.164946 ||g||=1.80e-02 r=1.011 λ=3.64e-16 α=1.000e+00\n",
      "Epoch   49 f=0.161040 test=0.164139 ||g||=1.38e-02 r=1.033 λ=5.68e-18 α=1.000e+00\n",
      "Final - Train 0.161040 Test 0.164139\n",
      "Initial - Train 0.472132 Test 0.471422\n",
      "Epoch    0 f=0.251975 test=0.250907 ||g||=2.51e-01 r=1.054 λ=5.00e-06 α=1.000e+00\n",
      "Epoch   10 f=0.168561 test=0.171245 ||g||=3.66e-02 r=1.196 λ=4.88e-09 α=1.000e+00\n",
      "Epoch   20 f=0.160998 test=0.166879 ||g||=1.97e-02 r=0.989 λ=9.54e-12 α=1.000e+00\n",
      "Epoch   30 f=0.158537 test=0.167061 ||g||=1.24e-02 r=0.995 λ=1.86e-14 α=1.000e+00\n",
      "Epoch   40 f=0.157510 test=0.167548 ||g||=8.83e-03 r=0.990 λ=7.28e-17 α=1.000e+00\n",
      "Epoch   49 f=0.157027 test=0.167646 ||g||=5.95e-03 r=1.140 λ=1.42e-19 α=1.000e+00\n",
      "Final - Train 0.157027 Test 0.167646\n",
      "Initial - Train 0.714481 Test 0.717420\n",
      "Epoch    0 f=0.332699 test=0.336665 ||g||=3.53e-01 r=0.993 λ=5.00e-06 α=1.000e+00\n",
      "Epoch   10 f=0.171753 test=0.177392 ||g||=4.14e-02 r=0.949 λ=1.95e-08 α=1.000e+00\n",
      "Epoch   20 f=0.162680 test=0.170512 ||g||=2.30e-02 r=0.984 λ=7.63e-11 α=1.000e+00\n",
      "Epoch   30 f=0.159446 test=0.169084 ||g||=1.36e-02 r=0.996 λ=5.96e-13 α=1.000e+00\n",
      "Epoch   40 f=0.158197 test=0.168355 ||g||=9.22e-03 r=0.976 λ=5.82e-16 α=1.000e+00\n",
      "Epoch   49 f=0.157564 test=0.167914 ||g||=7.13e-03 r=0.999 λ=2.27e-18 α=1.000e+00\n",
      "Final - Train 0.157564 Test 0.167914\n",
      "Initial - Train 0.999561 Test 0.994798\n",
      "Epoch    0 f=0.424066 test=0.420512 ||g||=4.12e-01 r=1.072 λ=5.00e-06 α=1.000e+00\n",
      "Epoch   10 f=0.187534 test=0.184603 ||g||=5.01e-02 r=1.004 λ=4.88e-09 α=1.000e+00\n",
      "Epoch   20 f=0.173800 test=0.172349 ||g||=2.97e-02 r=0.996 λ=1.91e-11 α=1.000e+00\n",
      "Epoch   30 f=0.167081 test=0.167393 ||g||=2.27e-02 r=1.002 λ=3.73e-14 α=1.000e+00\n",
      "Epoch   40 f=0.163197 test=0.164946 ||g||=1.80e-02 r=1.011 λ=3.64e-17 α=1.000e+00\n",
      "Epoch   49 f=0.161040 test=0.164139 ||g||=1.38e-02 r=1.033 λ=5.68e-19 α=1.000e+00\n",
      "Final - Train 0.161040 Test 0.164139\n",
      "Initial - Train 0.472132 Test 0.471422\n",
      "Epoch    0 f=0.251975 test=0.250907 ||g||=2.51e-01 r=1.054 λ=5.00e-07 α=1.000e+00\n",
      "Epoch   10 f=0.168561 test=0.171245 ||g||=3.66e-02 r=1.196 λ=4.88e-10 α=1.000e+00\n",
      "Epoch   20 f=0.160998 test=0.166879 ||g||=1.97e-02 r=0.989 λ=9.54e-13 α=1.000e+00\n",
      "Epoch   30 f=0.158537 test=0.167061 ||g||=1.24e-02 r=0.995 λ=1.86e-15 α=1.000e+00\n",
      "Epoch   40 f=0.157510 test=0.167548 ||g||=8.83e-03 r=0.990 λ=7.28e-18 α=1.000e+00\n",
      "Epoch   49 f=0.157027 test=0.167646 ||g||=5.95e-03 r=1.140 λ=1.42e-20 α=1.000e+00\n",
      "Final - Train 0.157027 Test 0.167646\n",
      "Initial - Train 0.714481 Test 0.717420\n",
      "Epoch    0 f=0.332699 test=0.336665 ||g||=3.53e-01 r=0.993 λ=5.00e-07 α=1.000e+00\n",
      "Epoch   10 f=0.171753 test=0.177392 ||g||=4.14e-02 r=0.949 λ=1.95e-09 α=1.000e+00\n",
      "Epoch   20 f=0.162680 test=0.170512 ||g||=2.30e-02 r=0.984 λ=7.63e-12 α=1.000e+00\n",
      "Epoch   30 f=0.159446 test=0.169084 ||g||=1.36e-02 r=0.996 λ=5.96e-14 α=1.000e+00\n",
      "Epoch   40 f=0.158197 test=0.168355 ||g||=9.22e-03 r=0.976 λ=5.82e-17 α=1.000e+00\n",
      "Epoch   49 f=0.157564 test=0.167914 ||g||=7.13e-03 r=0.999 λ=2.27e-19 α=1.000e+00\n",
      "Final - Train 0.157564 Test 0.167914\n",
      "Initial - Train 0.999561 Test 0.994798\n",
      "Epoch    0 f=0.424066 test=0.420512 ||g||=4.12e-01 r=1.072 λ=5.00e-07 α=1.000e+00\n",
      "Epoch   10 f=0.187534 test=0.184603 ||g||=5.01e-02 r=1.004 λ=4.88e-10 α=1.000e+00\n",
      "Epoch   20 f=0.173800 test=0.172349 ||g||=2.97e-02 r=0.996 λ=1.91e-12 α=1.000e+00\n",
      "Epoch   30 f=0.167081 test=0.167393 ||g||=2.27e-02 r=1.002 λ=3.73e-15 α=1.000e+00\n",
      "Epoch   40 f=0.163197 test=0.164946 ||g||=1.80e-02 r=1.011 λ=3.64e-18 α=1.000e+00\n",
      "Epoch   49 f=0.161040 test=0.164139 ||g||=1.38e-02 r=1.033 λ=5.68e-20 α=1.000e+00\n",
      "Final - Train 0.161040 Test 0.164139\n",
      "Initial - Train 0.472132 Test 0.471422\n",
      "Epoch    0 f=0.251975 test=0.250907 ||g||=2.51e-01 r=1.054 λ=5.00e-08 α=1.000e+00\n",
      "Epoch   10 f=0.168561 test=0.171245 ||g||=3.66e-02 r=1.196 λ=4.88e-11 α=1.000e+00\n",
      "Epoch   20 f=0.160998 test=0.166879 ||g||=1.97e-02 r=0.989 λ=9.54e-14 α=1.000e+00\n",
      "Epoch   30 f=0.158537 test=0.167061 ||g||=1.24e-02 r=0.995 λ=1.86e-16 α=1.000e+00\n",
      "Epoch   40 f=0.157510 test=0.167548 ||g||=8.83e-03 r=0.990 λ=7.28e-19 α=1.000e+00\n",
      "Epoch   49 f=0.157027 test=0.167646 ||g||=5.95e-03 r=1.140 λ=1.42e-21 α=1.000e+00\n",
      "Final - Train 0.157027 Test 0.167646\n",
      "Initial - Train 0.714481 Test 0.717420\n",
      "Epoch    0 f=0.332699 test=0.336665 ||g||=3.53e-01 r=0.993 λ=5.00e-08 α=1.000e+00\n",
      "Epoch   10 f=0.171753 test=0.177392 ||g||=4.14e-02 r=0.949 λ=1.95e-10 α=1.000e+00\n",
      "Epoch   20 f=0.162680 test=0.170512 ||g||=2.30e-02 r=0.984 λ=7.63e-13 α=1.000e+00\n",
      "Epoch   30 f=0.159446 test=0.169084 ||g||=1.36e-02 r=0.996 λ=5.96e-15 α=1.000e+00\n",
      "Epoch   40 f=0.158197 test=0.168355 ||g||=9.22e-03 r=0.976 λ=5.82e-18 α=1.000e+00\n",
      "Epoch   49 f=0.157564 test=0.167914 ||g||=7.13e-03 r=0.999 λ=2.27e-20 α=1.000e+00\n",
      "Final - Train 0.157564 Test 0.167914\n",
      "Initial - Train 0.999561 Test 0.994798\n",
      "Epoch    0 f=0.424066 test=0.420512 ||g||=4.12e-01 r=1.072 λ=5.00e-08 α=1.000e+00\n",
      "Epoch   10 f=0.187534 test=0.184603 ||g||=5.01e-02 r=1.004 λ=4.88e-11 α=1.000e+00\n",
      "Epoch   20 f=0.173800 test=0.172349 ||g||=2.97e-02 r=0.996 λ=1.91e-13 α=1.000e+00\n",
      "Epoch   30 f=0.167081 test=0.167393 ||g||=2.27e-02 r=1.002 λ=3.73e-16 α=1.000e+00\n",
      "Epoch   40 f=0.163197 test=0.164946 ||g||=1.80e-02 r=1.011 λ=3.64e-19 α=1.000e+00\n",
      "Epoch   49 f=0.161040 test=0.164139 ||g||=1.38e-02 r=1.033 λ=5.68e-21 α=1.000e+00\n",
      "Final - Train 0.161040 Test 0.164139\n",
      "Initial - Train 0.472132 Test 0.471422\n",
      "Epoch    0 f=0.251975 test=0.250907 ||g||=2.51e-01 r=1.054 λ=5.00e-05 α=1.000e+00\n",
      "Epoch   10 f=0.168561 test=0.171245 ||g||=3.66e-02 r=1.494 λ=4.88e-08 α=1.000e+00\n",
      "Epoch   20 f=0.160998 test=0.166879 ||g||=1.97e-02 r=0.989 λ=9.54e-11 α=1.000e+00\n",
      "Epoch   30 f=0.158537 test=0.167061 ||g||=1.24e-02 r=0.995 λ=1.86e-13 α=1.000e+00\n",
      "Epoch   40 f=0.157510 test=0.167548 ||g||=8.83e-03 r=0.990 λ=1.82e-16 α=1.000e+00\n",
      "Epoch   49 f=0.157027 test=0.167646 ||g||=5.95e-03 r=1.410 λ=3.55e-19 α=1.000e+00\n",
      "Final - Train 0.157027 Test 0.167646\n",
      "Initial - Train 0.714481 Test 0.717420\n",
      "Epoch    0 f=0.332699 test=0.336665 ||g||=3.53e-01 r=0.993 λ=5.00e-05 α=1.000e+00\n",
      "Epoch   10 f=0.171753 test=0.177392 ||g||=4.14e-02 r=0.983 λ=9.77e-08 α=1.000e+00\n",
      "Epoch   20 f=0.162680 test=0.170512 ||g||=2.30e-02 r=0.984 λ=3.81e-10 α=1.000e+00\n",
      "Epoch   30 f=0.159446 test=0.169084 ||g||=1.36e-02 r=0.996 λ=7.45e-13 α=1.000e+00\n",
      "Epoch   40 f=0.158197 test=0.168355 ||g||=9.22e-03 r=0.976 λ=7.28e-16 α=1.000e+00\n",
      "Epoch   49 f=0.157564 test=0.167914 ||g||=7.13e-03 r=0.999 λ=1.42e-18 α=1.000e+00\n",
      "Final - Train 0.157564 Test 0.167914\n",
      "Initial - Train 0.999561 Test 0.994798\n",
      "Epoch    0 f=0.424066 test=0.420512 ||g||=4.12e-01 r=1.100 λ=5.00e-05 α=1.000e+00\n",
      "Epoch   10 f=0.187534 test=0.184603 ||g||=5.01e-02 r=1.004 λ=4.88e-08 α=1.000e+00\n",
      "Epoch   20 f=0.173800 test=0.172349 ||g||=2.97e-02 r=0.996 λ=9.54e-11 α=1.000e+00\n",
      "Epoch   30 f=0.167081 test=0.167393 ||g||=2.27e-02 r=1.002 λ=9.31e-14 α=1.000e+00\n",
      "Epoch   40 f=0.163197 test=0.164946 ||g||=1.80e-02 r=1.011 λ=9.09e-17 α=1.000e+00\n",
      "Epoch   49 f=0.161040 test=0.164139 ||g||=1.38e-02 r=0.998 λ=1.42e-18 α=1.000e+00\n",
      "Final - Train 0.161040 Test 0.164139\n",
      "Initial - Train 0.472132 Test 0.471422\n",
      "Epoch    0 f=0.251975 test=0.250907 ||g||=2.51e-01 r=1.054 λ=5.00e-06 α=1.000e+00\n",
      "Epoch   10 f=0.168561 test=0.171245 ||g||=3.66e-02 r=1.494 λ=4.88e-09 α=1.000e+00\n",
      "Epoch   20 f=0.160998 test=0.166879 ||g||=1.97e-02 r=0.989 λ=9.54e-12 α=1.000e+00\n",
      "Epoch   30 f=0.158537 test=0.167061 ||g||=1.24e-02 r=0.995 λ=1.86e-14 α=1.000e+00\n",
      "Epoch   40 f=0.157510 test=0.167548 ||g||=8.83e-03 r=0.990 λ=1.82e-17 α=1.000e+00\n",
      "Epoch   49 f=0.157027 test=0.167646 ||g||=5.95e-03 r=1.410 λ=3.55e-20 α=1.000e+00\n",
      "Final - Train 0.157027 Test 0.167646\n",
      "Initial - Train 0.714481 Test 0.717420\n",
      "Epoch    0 f=0.332699 test=0.336665 ||g||=3.53e-01 r=0.993 λ=5.00e-06 α=1.000e+00\n",
      "Epoch   10 f=0.171753 test=0.177392 ||g||=4.14e-02 r=0.983 λ=9.77e-09 α=1.000e+00\n",
      "Epoch   20 f=0.162680 test=0.170512 ||g||=2.30e-02 r=0.984 λ=3.81e-11 α=1.000e+00\n",
      "Epoch   30 f=0.159446 test=0.169084 ||g||=1.36e-02 r=0.996 λ=7.45e-14 α=1.000e+00\n",
      "Epoch   40 f=0.158197 test=0.168355 ||g||=9.22e-03 r=0.976 λ=7.28e-17 α=1.000e+00\n",
      "Epoch   49 f=0.157564 test=0.167914 ||g||=7.13e-03 r=0.999 λ=1.42e-19 α=1.000e+00\n",
      "Final - Train 0.157564 Test 0.167914\n",
      "Initial - Train 0.999561 Test 0.994798\n",
      "Epoch    0 f=0.424066 test=0.420512 ||g||=4.12e-01 r=1.100 λ=5.00e-06 α=1.000e+00\n",
      "Epoch   10 f=0.187534 test=0.184603 ||g||=5.01e-02 r=1.004 λ=4.88e-09 α=1.000e+00\n",
      "Epoch   20 f=0.173800 test=0.172349 ||g||=2.97e-02 r=0.996 λ=9.54e-12 α=1.000e+00\n",
      "Epoch   30 f=0.167081 test=0.167393 ||g||=2.27e-02 r=1.002 λ=9.31e-15 α=1.000e+00\n",
      "Epoch   40 f=0.163197 test=0.164946 ||g||=1.80e-02 r=1.011 λ=9.09e-18 α=1.000e+00\n",
      "Epoch   49 f=0.161040 test=0.164139 ||g||=1.38e-02 r=0.998 λ=1.42e-19 α=1.000e+00\n",
      "Final - Train 0.161040 Test 0.164139\n",
      "Initial - Train 0.472132 Test 0.471422\n",
      "Epoch    0 f=0.251975 test=0.250907 ||g||=2.51e-01 r=1.054 λ=5.00e-07 α=1.000e+00\n",
      "Epoch   10 f=0.168561 test=0.171245 ||g||=3.66e-02 r=1.494 λ=4.88e-10 α=1.000e+00\n",
      "Epoch   20 f=0.160998 test=0.166879 ||g||=1.97e-02 r=0.989 λ=9.54e-13 α=1.000e+00\n",
      "Epoch   30 f=0.158537 test=0.167061 ||g||=1.24e-02 r=0.995 λ=1.86e-15 α=1.000e+00\n",
      "Epoch   40 f=0.157510 test=0.167548 ||g||=8.83e-03 r=0.990 λ=1.82e-18 α=1.000e+00\n",
      "Epoch   49 f=0.157027 test=0.167646 ||g||=5.95e-03 r=1.410 λ=3.55e-21 α=1.000e+00\n",
      "Final - Train 0.157027 Test 0.167646\n",
      "Initial - Train 0.714481 Test 0.717420\n",
      "Epoch    0 f=0.332699 test=0.336665 ||g||=3.53e-01 r=0.993 λ=5.00e-07 α=1.000e+00\n",
      "Epoch   10 f=0.171753 test=0.177392 ||g||=4.14e-02 r=0.983 λ=9.77e-10 α=1.000e+00\n",
      "Epoch   20 f=0.162680 test=0.170512 ||g||=2.30e-02 r=0.984 λ=3.81e-12 α=1.000e+00\n",
      "Epoch   30 f=0.159446 test=0.169084 ||g||=1.36e-02 r=0.996 λ=7.45e-15 α=1.000e+00\n",
      "Epoch   40 f=0.158197 test=0.168355 ||g||=9.22e-03 r=0.976 λ=7.28e-18 α=1.000e+00\n",
      "Epoch   49 f=0.157564 test=0.167914 ||g||=7.13e-03 r=0.999 λ=1.42e-20 α=1.000e+00\n",
      "Final - Train 0.157564 Test 0.167914\n",
      "Initial - Train 0.999561 Test 0.994798\n",
      "Epoch    0 f=0.424066 test=0.420512 ||g||=4.12e-01 r=1.100 λ=5.00e-07 α=1.000e+00\n",
      "Epoch   10 f=0.187534 test=0.184603 ||g||=5.01e-02 r=1.004 λ=4.88e-10 α=1.000e+00\n",
      "Epoch   20 f=0.173800 test=0.172349 ||g||=2.97e-02 r=0.996 λ=9.54e-13 α=1.000e+00\n",
      "Epoch   30 f=0.167081 test=0.167393 ||g||=2.27e-02 r=1.002 λ=9.31e-16 α=1.000e+00\n",
      "Epoch   40 f=0.163197 test=0.164946 ||g||=1.80e-02 r=1.011 λ=9.09e-19 α=1.000e+00\n",
      "Epoch   49 f=0.161040 test=0.164139 ||g||=1.38e-02 r=0.998 λ=1.42e-20 α=1.000e+00\n",
      "Final - Train 0.161040 Test 0.164139\n",
      "Initial - Train 0.472132 Test 0.471422\n",
      "Epoch    0 f=0.251975 test=0.250907 ||g||=2.51e-01 r=1.054 λ=5.00e-08 α=1.000e+00\n",
      "Epoch   10 f=0.168561 test=0.171245 ||g||=3.66e-02 r=1.494 λ=4.88e-11 α=1.000e+00\n",
      "Epoch   20 f=0.160998 test=0.166879 ||g||=1.97e-02 r=0.989 λ=9.54e-14 α=1.000e+00\n",
      "Epoch   30 f=0.158537 test=0.167061 ||g||=1.24e-02 r=0.995 λ=1.86e-16 α=1.000e+00\n",
      "Epoch   40 f=0.157510 test=0.167548 ||g||=8.83e-03 r=0.990 λ=1.82e-19 α=1.000e+00\n",
      "Epoch   49 f=0.157027 test=0.167646 ||g||=5.95e-03 r=1.410 λ=3.55e-22 α=1.000e+00\n",
      "Final - Train 0.157027 Test 0.167646\n",
      "Initial - Train 0.714481 Test 0.717420\n",
      "Epoch    0 f=0.332699 test=0.336665 ||g||=3.53e-01 r=0.993 λ=5.00e-08 α=1.000e+00\n",
      "Epoch   10 f=0.171753 test=0.177392 ||g||=4.14e-02 r=0.983 λ=9.77e-11 α=1.000e+00\n",
      "Epoch   20 f=0.162680 test=0.170512 ||g||=2.30e-02 r=0.984 λ=3.81e-13 α=1.000e+00\n",
      "Epoch   30 f=0.159446 test=0.169084 ||g||=1.36e-02 r=0.996 λ=7.45e-16 α=1.000e+00\n",
      "Epoch   40 f=0.158197 test=0.168355 ||g||=9.22e-03 r=0.976 λ=7.28e-19 α=1.000e+00\n",
      "Epoch   49 f=0.157564 test=0.167914 ||g||=7.13e-03 r=0.999 λ=1.42e-21 α=1.000e+00\n",
      "Final - Train 0.157564 Test 0.167914\n",
      "Initial - Train 0.999561 Test 0.994798\n",
      "Epoch    0 f=0.424066 test=0.420512 ||g||=4.12e-01 r=1.100 λ=5.00e-08 α=1.000e+00\n",
      "Epoch   10 f=0.187534 test=0.184603 ||g||=5.01e-02 r=1.004 λ=4.88e-11 α=1.000e+00\n",
      "Epoch   20 f=0.173800 test=0.172349 ||g||=2.97e-02 r=0.996 λ=9.54e-14 α=1.000e+00\n",
      "Epoch   30 f=0.167081 test=0.167393 ||g||=2.27e-02 r=1.002 λ=9.31e-17 α=1.000e+00\n",
      "Epoch   40 f=0.163197 test=0.164946 ||g||=1.80e-02 r=1.011 λ=9.09e-20 α=1.000e+00\n",
      "Epoch   49 f=0.161040 test=0.164139 ||g||=1.38e-02 r=0.998 λ=1.42e-21 α=1.000e+00\n",
      "Final - Train 0.161040 Test 0.164139\n",
      "Initial - Train 0.472132 Test 0.471422\n",
      "Epoch    0 f=0.251975 test=0.250907 ||g||=2.51e-01 r=1.054 λ=5.00e-05 α=1.000e+00\n",
      "Epoch   10 f=0.168561 test=0.171245 ||g||=3.66e-02 r=0.997 λ=4.88e-08 α=1.000e+00\n",
      "Epoch   20 f=0.160998 test=0.166879 ||g||=1.97e-02 r=0.989 λ=4.77e-11 α=1.000e+00\n",
      "Epoch   30 f=0.158537 test=0.167061 ||g||=1.24e-02 r=1.016 λ=4.66e-14 α=1.000e+00\n",
      "Epoch   40 f=0.157510 test=0.167548 ||g||=8.83e-03 r=0.990 λ=4.55e-17 α=1.000e+00\n",
      "Epoch   49 f=0.157056 test=0.167676 ||g||=6.09e-03 r=0.974 λ=8.88e-20 α=1.000e+00\n",
      "Final - Train 0.157056 Test 0.167676\n",
      "Initial - Train 0.714481 Test 0.717420\n",
      "Epoch    0 f=0.332699 test=0.336665 ||g||=3.53e-01 r=0.992 λ=5.00e-05 α=1.000e+00\n",
      "Epoch   10 f=0.171753 test=0.177392 ||g||=4.14e-02 r=0.983 λ=4.88e-08 α=1.000e+00\n",
      "Epoch   20 f=0.162680 test=0.170512 ||g||=2.30e-02 r=0.984 λ=4.77e-11 α=1.000e+00\n",
      "Epoch   30 f=0.159446 test=0.169084 ||g||=1.36e-02 r=0.996 λ=4.66e-14 α=1.000e+00\n",
      "Epoch   40 f=0.158250 test=0.168380 ||g||=9.31e-03 r=1.008 λ=4.55e-17 α=1.000e+00\n",
      "Epoch   49 f=0.157629 test=0.167942 ||g||=7.26e-03 r=0.969 λ=8.88e-20 α=1.000e+00\n",
      "Final - Train 0.157629 Test 0.167942\n",
      "Initial - Train 0.999561 Test 0.994798\n",
      "Epoch    0 f=0.424066 test=0.420512 ||g||=4.12e-01 r=0.995 λ=5.00e-05 α=1.000e+00\n",
      "Epoch   10 f=0.187534 test=0.184603 ||g||=5.01e-02 r=1.005 λ=4.88e-08 α=1.000e+00\n",
      "Epoch   20 f=0.173800 test=0.172349 ||g||=2.97e-02 r=0.996 λ=4.77e-11 α=1.000e+00\n",
      "Epoch   30 f=0.167081 test=0.167393 ||g||=2.27e-02 r=1.002 λ=4.66e-14 α=1.000e+00\n",
      "Epoch   40 f=0.163197 test=0.164946 ||g||=1.80e-02 r=1.011 λ=4.55e-17 α=1.000e+00\n",
      "Epoch   49 f=0.161040 test=0.164139 ||g||=1.38e-02 r=1.009 λ=1.78e-19 α=1.000e+00\n",
      "Final - Train 0.161040 Test 0.164139\n",
      "Initial - Train 0.472132 Test 0.471422\n",
      "Epoch    0 f=0.251975 test=0.250907 ||g||=2.51e-01 r=1.054 λ=5.00e-06 α=1.000e+00\n",
      "Epoch   10 f=0.168561 test=0.171245 ||g||=3.66e-02 r=0.997 λ=4.88e-09 α=1.000e+00\n",
      "Epoch   20 f=0.160998 test=0.166879 ||g||=1.97e-02 r=0.989 λ=4.77e-12 α=1.000e+00\n",
      "Epoch   30 f=0.158537 test=0.167061 ||g||=1.24e-02 r=1.016 λ=4.66e-15 α=1.000e+00\n",
      "Epoch   40 f=0.157510 test=0.167548 ||g||=8.83e-03 r=0.990 λ=4.55e-18 α=1.000e+00\n",
      "Epoch   49 f=0.157056 test=0.167676 ||g||=6.09e-03 r=0.974 λ=8.88e-21 α=1.000e+00\n",
      "Final - Train 0.157056 Test 0.167676\n",
      "Initial - Train 0.714481 Test 0.717420\n",
      "Epoch    0 f=0.332699 test=0.336665 ||g||=3.53e-01 r=0.992 λ=5.00e-06 α=1.000e+00\n",
      "Epoch   10 f=0.171753 test=0.177392 ||g||=4.14e-02 r=0.983 λ=4.88e-09 α=1.000e+00\n",
      "Epoch   20 f=0.162680 test=0.170512 ||g||=2.30e-02 r=0.984 λ=4.77e-12 α=1.000e+00\n",
      "Epoch   30 f=0.159446 test=0.169084 ||g||=1.36e-02 r=0.996 λ=4.66e-15 α=1.000e+00\n",
      "Epoch   40 f=0.158250 test=0.168380 ||g||=9.31e-03 r=1.008 λ=4.55e-18 α=1.000e+00\n",
      "Epoch   49 f=0.157629 test=0.167942 ||g||=7.26e-03 r=0.969 λ=8.88e-21 α=1.000e+00\n",
      "Final - Train 0.157629 Test 0.167942\n",
      "Initial - Train 0.999561 Test 0.994798\n",
      "Epoch    0 f=0.424066 test=0.420512 ||g||=4.12e-01 r=0.995 λ=5.00e-06 α=1.000e+00\n",
      "Epoch   10 f=0.187534 test=0.184603 ||g||=5.01e-02 r=1.005 λ=4.88e-09 α=1.000e+00\n",
      "Epoch   20 f=0.173800 test=0.172349 ||g||=2.97e-02 r=0.996 λ=4.77e-12 α=1.000e+00\n",
      "Epoch   30 f=0.167081 test=0.167393 ||g||=2.27e-02 r=1.002 λ=4.66e-15 α=1.000e+00\n",
      "Epoch   40 f=0.163197 test=0.164946 ||g||=1.80e-02 r=1.011 λ=4.55e-18 α=1.000e+00\n",
      "Epoch   49 f=0.161040 test=0.164139 ||g||=1.38e-02 r=1.009 λ=1.78e-20 α=1.000e+00\n",
      "Final - Train 0.161040 Test 0.164139\n",
      "Initial - Train 0.472132 Test 0.471422\n",
      "Epoch    0 f=0.251975 test=0.250907 ||g||=2.51e-01 r=1.054 λ=5.00e-07 α=1.000e+00\n",
      "Epoch   10 f=0.168561 test=0.171245 ||g||=3.66e-02 r=0.997 λ=4.88e-10 α=1.000e+00\n",
      "Epoch   20 f=0.160998 test=0.166879 ||g||=1.97e-02 r=0.989 λ=4.77e-13 α=1.000e+00\n",
      "Epoch   30 f=0.158537 test=0.167061 ||g||=1.24e-02 r=1.016 λ=4.66e-16 α=1.000e+00\n",
      "Epoch   40 f=0.157510 test=0.167548 ||g||=8.83e-03 r=0.990 λ=4.55e-19 α=1.000e+00\n",
      "Epoch   49 f=0.157056 test=0.167676 ||g||=6.09e-03 r=0.974 λ=8.88e-22 α=1.000e+00\n",
      "Final - Train 0.157056 Test 0.167676\n",
      "Initial - Train 0.714481 Test 0.717420\n",
      "Epoch    0 f=0.332699 test=0.336665 ||g||=3.53e-01 r=0.992 λ=5.00e-07 α=1.000e+00\n",
      "Epoch   10 f=0.171753 test=0.177392 ||g||=4.14e-02 r=0.983 λ=4.88e-10 α=1.000e+00\n",
      "Epoch   20 f=0.162680 test=0.170512 ||g||=2.30e-02 r=0.984 λ=4.77e-13 α=1.000e+00\n",
      "Epoch   30 f=0.159446 test=0.169084 ||g||=1.36e-02 r=0.996 λ=4.66e-16 α=1.000e+00\n",
      "Epoch   40 f=0.158250 test=0.168380 ||g||=9.31e-03 r=1.008 λ=4.55e-19 α=1.000e+00\n",
      "Epoch   49 f=0.157629 test=0.167942 ||g||=7.26e-03 r=0.969 λ=8.88e-22 α=1.000e+00\n",
      "Final - Train 0.157629 Test 0.167942\n",
      "Initial - Train 0.999561 Test 0.994798\n",
      "Epoch    0 f=0.424066 test=0.420512 ||g||=4.12e-01 r=0.995 λ=5.00e-07 α=1.000e+00\n",
      "Epoch   10 f=0.187534 test=0.184603 ||g||=5.01e-02 r=1.005 λ=4.88e-10 α=1.000e+00\n",
      "Epoch   20 f=0.173800 test=0.172349 ||g||=2.97e-02 r=0.996 λ=4.77e-13 α=1.000e+00\n",
      "Epoch   30 f=0.167081 test=0.167393 ||g||=2.27e-02 r=1.002 λ=4.66e-16 α=1.000e+00\n",
      "Epoch   40 f=0.163197 test=0.164946 ||g||=1.80e-02 r=1.011 λ=4.55e-19 α=1.000e+00\n",
      "Epoch   49 f=0.161040 test=0.164139 ||g||=1.38e-02 r=1.009 λ=1.78e-21 α=1.000e+00\n",
      "Final - Train 0.161040 Test 0.164139\n",
      "Initial - Train 0.472132 Test 0.471422\n",
      "Epoch    0 f=0.251975 test=0.250907 ||g||=2.51e-01 r=1.054 λ=5.00e-08 α=1.000e+00\n",
      "Epoch   10 f=0.168561 test=0.171245 ||g||=3.66e-02 r=0.997 λ=4.88e-11 α=1.000e+00\n",
      "Epoch   20 f=0.160998 test=0.166879 ||g||=1.97e-02 r=0.989 λ=4.77e-14 α=1.000e+00\n",
      "Epoch   30 f=0.158537 test=0.167061 ||g||=1.24e-02 r=1.016 λ=4.66e-17 α=1.000e+00\n",
      "Epoch   40 f=0.157510 test=0.167548 ||g||=8.83e-03 r=0.990 λ=4.55e-20 α=1.000e+00\n",
      "Epoch   49 f=0.157056 test=0.167676 ||g||=6.09e-03 r=0.974 λ=8.88e-23 α=1.000e+00\n",
      "Final - Train 0.157056 Test 0.167676\n",
      "Initial - Train 0.714481 Test 0.717420\n",
      "Epoch    0 f=0.332699 test=0.336665 ||g||=3.53e-01 r=0.992 λ=5.00e-08 α=1.000e+00\n",
      "Epoch   10 f=0.171753 test=0.177392 ||g||=4.14e-02 r=0.983 λ=4.88e-11 α=1.000e+00\n",
      "Epoch   20 f=0.162680 test=0.170512 ||g||=2.30e-02 r=0.984 λ=4.77e-14 α=1.000e+00\n",
      "Epoch   30 f=0.159446 test=0.169084 ||g||=1.36e-02 r=0.996 λ=4.66e-17 α=1.000e+00\n",
      "Epoch   40 f=0.158250 test=0.168380 ||g||=9.31e-03 r=1.008 λ=4.55e-20 α=1.000e+00\n",
      "Epoch   49 f=0.157629 test=0.167942 ||g||=7.26e-03 r=0.969 λ=8.88e-23 α=1.000e+00\n",
      "Final - Train 0.157629 Test 0.167942\n",
      "Initial - Train 0.999561 Test 0.994798\n",
      "Epoch    0 f=0.424066 test=0.420512 ||g||=4.12e-01 r=0.995 λ=5.00e-08 α=1.000e+00\n",
      "Epoch   10 f=0.187534 test=0.184603 ||g||=5.01e-02 r=1.005 λ=4.88e-11 α=1.000e+00\n",
      "Epoch   20 f=0.173800 test=0.172349 ||g||=2.97e-02 r=0.996 λ=4.77e-14 α=1.000e+00\n",
      "Epoch   30 f=0.167081 test=0.167393 ||g||=2.27e-02 r=1.002 λ=4.66e-17 α=1.000e+00\n",
      "Epoch   40 f=0.163197 test=0.164946 ||g||=1.80e-02 r=1.011 λ=4.55e-20 α=1.000e+00\n",
      "Epoch   49 f=0.161040 test=0.164139 ||g||=1.38e-02 r=1.009 λ=1.78e-22 α=1.000e+00\n",
      "Final - Train 0.161040 Test 0.164139\n",
      "Initial - Train 0.472132 Test 0.471422\n",
      "Epoch    0 f=0.251975 test=0.250907 ||g||=2.51e-01 r=1.054 λ=5.00e-05 α=1.000e+00\n",
      "Epoch   10 f=0.168561 test=0.171245 ||g||=3.66e-02 r=0.997 λ=4.88e-08 α=1.000e+00\n",
      "Epoch   20 f=0.160998 test=0.166879 ||g||=1.97e-02 r=0.989 λ=4.77e-11 α=1.000e+00\n",
      "Epoch   30 f=0.158537 test=0.167061 ||g||=1.24e-02 r=1.016 λ=4.66e-14 α=1.000e+00\n",
      "Epoch   40 f=0.157510 test=0.167548 ||g||=8.83e-03 r=0.990 λ=4.55e-17 α=1.000e+00\n",
      "Epoch   49 f=0.157027 test=0.167646 ||g||=5.95e-03 r=0.957 λ=8.88e-20 α=1.000e+00\n",
      "Final - Train 0.157027 Test 0.167646\n",
      "Initial - Train 0.714481 Test 0.717420\n",
      "Epoch    0 f=0.332699 test=0.336665 ||g||=3.53e-01 r=0.992 λ=5.00e-05 α=1.000e+00\n",
      "Epoch   10 f=0.171753 test=0.177392 ||g||=4.14e-02 r=0.983 λ=4.88e-08 α=1.000e+00\n",
      "Epoch   20 f=0.162680 test=0.170512 ||g||=2.30e-02 r=0.984 λ=4.77e-11 α=1.000e+00\n",
      "Epoch   30 f=0.159446 test=0.169084 ||g||=1.36e-02 r=0.996 λ=4.66e-14 α=1.000e+00\n",
      "Epoch   40 f=0.158265 test=0.168388 ||g||=9.44e-03 r=1.007 λ=4.55e-17 α=1.000e+00\n",
      "Epoch   49 f=0.157606 test=0.167921 ||g||=7.15e-03 r=0.994 λ=8.88e-20 α=1.000e+00\n",
      "Final - Train 0.157606 Test 0.167921\n",
      "Initial - Train 0.999561 Test 0.994798\n",
      "Epoch    0 f=0.424066 test=0.420512 ||g||=4.12e-01 r=0.996 λ=5.00e-05 α=1.000e+00\n",
      "Epoch   10 f=0.187534 test=0.184603 ||g||=5.01e-02 r=1.004 λ=4.88e-08 α=1.000e+00\n",
      "Epoch   20 f=0.173800 test=0.172349 ||g||=2.97e-02 r=0.997 λ=4.77e-11 α=1.000e+00\n",
      "Epoch   30 f=0.167081 test=0.167393 ||g||=2.27e-02 r=1.002 λ=4.66e-14 α=1.000e+00\n",
      "Epoch   40 f=0.163197 test=0.164946 ||g||=1.80e-02 r=1.011 λ=4.55e-17 α=1.000e+00\n",
      "Epoch   49 f=0.161040 test=0.164139 ||g||=1.38e-02 r=1.009 λ=8.88e-20 α=1.000e+00\n",
      "Final - Train 0.161040 Test 0.164139\n",
      "Initial - Train 0.472132 Test 0.471422\n",
      "Epoch    0 f=0.251975 test=0.250907 ||g||=2.51e-01 r=1.054 λ=5.00e-06 α=1.000e+00\n",
      "Epoch   10 f=0.168561 test=0.171245 ||g||=3.66e-02 r=0.997 λ=4.88e-09 α=1.000e+00\n",
      "Epoch   20 f=0.160998 test=0.166879 ||g||=1.97e-02 r=0.989 λ=4.77e-12 α=1.000e+00\n",
      "Epoch   30 f=0.158537 test=0.167061 ||g||=1.24e-02 r=1.016 λ=4.66e-15 α=1.000e+00\n",
      "Epoch   40 f=0.157510 test=0.167548 ||g||=8.83e-03 r=0.990 λ=4.55e-18 α=1.000e+00\n",
      "Epoch   49 f=0.157027 test=0.167646 ||g||=5.95e-03 r=0.957 λ=8.88e-21 α=1.000e+00\n",
      "Final - Train 0.157027 Test 0.167646\n",
      "Initial - Train 0.714481 Test 0.717420\n",
      "Epoch    0 f=0.332699 test=0.336665 ||g||=3.53e-01 r=0.992 λ=5.00e-06 α=1.000e+00\n",
      "Epoch   10 f=0.171753 test=0.177392 ||g||=4.14e-02 r=0.983 λ=4.88e-09 α=1.000e+00\n",
      "Epoch   20 f=0.162680 test=0.170512 ||g||=2.30e-02 r=0.984 λ=4.77e-12 α=1.000e+00\n",
      "Epoch   30 f=0.159446 test=0.169084 ||g||=1.36e-02 r=0.996 λ=4.66e-15 α=1.000e+00\n",
      "Epoch   40 f=0.158265 test=0.168388 ||g||=9.44e-03 r=1.007 λ=4.55e-18 α=1.000e+00\n",
      "Epoch   49 f=0.157606 test=0.167921 ||g||=7.15e-03 r=0.994 λ=8.88e-21 α=1.000e+00\n",
      "Final - Train 0.157606 Test 0.167921\n",
      "Initial - Train 0.999561 Test 0.994798\n",
      "Epoch    0 f=0.424066 test=0.420512 ||g||=4.12e-01 r=0.996 λ=5.00e-06 α=1.000e+00\n",
      "Epoch   10 f=0.187534 test=0.184603 ||g||=5.01e-02 r=1.004 λ=4.88e-09 α=1.000e+00\n",
      "Epoch   20 f=0.173800 test=0.172349 ||g||=2.97e-02 r=0.997 λ=4.77e-12 α=1.000e+00\n",
      "Epoch   30 f=0.167081 test=0.167393 ||g||=2.27e-02 r=1.002 λ=4.66e-15 α=1.000e+00\n",
      "Epoch   40 f=0.163197 test=0.164946 ||g||=1.80e-02 r=1.011 λ=4.55e-18 α=1.000e+00\n",
      "Epoch   49 f=0.161040 test=0.164139 ||g||=1.38e-02 r=1.009 λ=8.88e-21 α=1.000e+00\n",
      "Final - Train 0.161040 Test 0.164139\n",
      "Initial - Train 0.472132 Test 0.471422\n",
      "Epoch    0 f=0.251975 test=0.250907 ||g||=2.51e-01 r=1.054 λ=5.00e-07 α=1.000e+00\n",
      "Epoch   10 f=0.168561 test=0.171245 ||g||=3.66e-02 r=0.997 λ=4.88e-10 α=1.000e+00\n",
      "Epoch   20 f=0.160998 test=0.166879 ||g||=1.97e-02 r=0.989 λ=4.77e-13 α=1.000e+00\n",
      "Epoch   30 f=0.158537 test=0.167061 ||g||=1.24e-02 r=1.016 λ=4.66e-16 α=1.000e+00\n",
      "Epoch   40 f=0.157510 test=0.167548 ||g||=8.83e-03 r=0.990 λ=4.55e-19 α=1.000e+00\n",
      "Epoch   49 f=0.157027 test=0.167646 ||g||=5.95e-03 r=0.957 λ=8.88e-22 α=1.000e+00\n",
      "Final - Train 0.157027 Test 0.167646\n",
      "Initial - Train 0.714481 Test 0.717420\n",
      "Epoch    0 f=0.332699 test=0.336665 ||g||=3.53e-01 r=0.992 λ=5.00e-07 α=1.000e+00\n",
      "Epoch   10 f=0.171753 test=0.177392 ||g||=4.14e-02 r=0.983 λ=4.88e-10 α=1.000e+00\n",
      "Epoch   20 f=0.162680 test=0.170512 ||g||=2.30e-02 r=0.984 λ=4.77e-13 α=1.000e+00\n",
      "Epoch   30 f=0.159446 test=0.169084 ||g||=1.36e-02 r=0.996 λ=4.66e-16 α=1.000e+00\n",
      "Epoch   40 f=0.158265 test=0.168388 ||g||=9.44e-03 r=1.007 λ=4.55e-19 α=1.000e+00\n",
      "Epoch   49 f=0.157606 test=0.167921 ||g||=7.15e-03 r=0.994 λ=8.88e-22 α=1.000e+00\n",
      "Final - Train 0.157606 Test 0.167921\n",
      "Initial - Train 0.999561 Test 0.994798\n",
      "Epoch    0 f=0.424066 test=0.420512 ||g||=4.12e-01 r=0.996 λ=5.00e-07 α=1.000e+00\n",
      "Epoch   10 f=0.187534 test=0.184603 ||g||=5.01e-02 r=1.004 λ=4.88e-10 α=1.000e+00\n",
      "Epoch   20 f=0.173800 test=0.172349 ||g||=2.97e-02 r=0.997 λ=4.77e-13 α=1.000e+00\n",
      "Epoch   30 f=0.167081 test=0.167393 ||g||=2.27e-02 r=1.002 λ=4.66e-16 α=1.000e+00\n",
      "Epoch   40 f=0.163197 test=0.164946 ||g||=1.80e-02 r=1.011 λ=4.55e-19 α=1.000e+00\n",
      "Epoch   49 f=0.161040 test=0.164139 ||g||=1.38e-02 r=1.009 λ=8.88e-22 α=1.000e+00\n",
      "Final - Train 0.161040 Test 0.164139\n",
      "Initial - Train 0.472132 Test 0.471422\n",
      "Epoch    0 f=0.251975 test=0.250907 ||g||=2.51e-01 r=1.054 λ=5.00e-08 α=1.000e+00\n",
      "Epoch   10 f=0.168561 test=0.171245 ||g||=3.66e-02 r=0.997 λ=4.88e-11 α=1.000e+00\n",
      "Epoch   20 f=0.160998 test=0.166879 ||g||=1.97e-02 r=0.989 λ=4.77e-14 α=1.000e+00\n",
      "Epoch   30 f=0.158537 test=0.167061 ||g||=1.24e-02 r=1.016 λ=4.66e-17 α=1.000e+00\n",
      "Epoch   40 f=0.157510 test=0.167548 ||g||=8.83e-03 r=0.990 λ=4.55e-20 α=1.000e+00\n",
      "Epoch   49 f=0.157027 test=0.167646 ||g||=5.95e-03 r=0.957 λ=8.88e-23 α=1.000e+00\n",
      "Final - Train 0.157027 Test 0.167646\n",
      "Initial - Train 0.714481 Test 0.717420\n",
      "Epoch    0 f=0.332699 test=0.336665 ||g||=3.53e-01 r=0.992 λ=5.00e-08 α=1.000e+00\n",
      "Epoch   10 f=0.171753 test=0.177392 ||g||=4.14e-02 r=0.983 λ=4.88e-11 α=1.000e+00\n",
      "Epoch   20 f=0.162680 test=0.170512 ||g||=2.30e-02 r=0.984 λ=4.77e-14 α=1.000e+00\n",
      "Epoch   30 f=0.159446 test=0.169084 ||g||=1.36e-02 r=0.996 λ=4.66e-17 α=1.000e+00\n",
      "Epoch   40 f=0.158265 test=0.168388 ||g||=9.44e-03 r=1.007 λ=4.55e-20 α=1.000e+00\n",
      "Epoch   49 f=0.157606 test=0.167921 ||g||=7.15e-03 r=0.994 λ=8.88e-23 α=1.000e+00\n",
      "Final - Train 0.157606 Test 0.167921\n",
      "Initial - Train 0.999561 Test 0.994798\n",
      "Epoch    0 f=0.424066 test=0.420512 ||g||=4.12e-01 r=0.996 λ=5.00e-08 α=1.000e+00\n",
      "Epoch   10 f=0.187534 test=0.184603 ||g||=5.01e-02 r=1.004 λ=4.88e-11 α=1.000e+00\n",
      "Epoch   20 f=0.173800 test=0.172349 ||g||=2.97e-02 r=0.997 λ=4.77e-14 α=1.000e+00\n",
      "Epoch   30 f=0.167081 test=0.167393 ||g||=2.27e-02 r=1.002 λ=4.66e-17 α=1.000e+00\n",
      "Epoch   40 f=0.163197 test=0.164946 ||g||=1.80e-02 r=1.011 λ=4.55e-20 α=1.000e+00\n",
      "Epoch   49 f=0.161040 test=0.164139 ||g||=1.38e-02 r=1.009 λ=8.88e-23 α=1.000e+00\n",
      "Final - Train 0.161040 Test 0.164139\n",
      "Initial - Train 0.472132 Test 0.471422\n",
      "Epoch    0 f=0.251975 test=0.250907 ||g||=2.51e-01 r=1.052 λ=5.00e-05 α=1.000e+00\n",
      "Epoch   10 f=0.168561 test=0.171245 ||g||=3.66e-02 r=0.996 λ=4.88e-08 α=1.000e+00\n",
      "Epoch   20 f=0.160998 test=0.166879 ||g||=1.97e-02 r=0.989 λ=4.77e-11 α=1.000e+00\n",
      "Epoch   30 f=0.158537 test=0.167061 ||g||=1.24e-02 r=1.015 λ=4.66e-14 α=1.000e+00\n",
      "Epoch   40 f=0.157510 test=0.167548 ||g||=8.83e-03 r=0.990 λ=4.55e-17 α=1.000e+00\n",
      "Epoch   49 f=0.157027 test=0.167646 ||g||=5.95e-03 r=0.957 λ=8.88e-20 α=1.000e+00\n",
      "Final - Train 0.157027 Test 0.167646\n",
      "Initial - Train 0.714481 Test 0.717420\n",
      "Epoch    0 f=0.332699 test=0.336665 ||g||=3.53e-01 r=0.992 λ=5.00e-05 α=1.000e+00\n",
      "Epoch   10 f=0.171753 test=0.177392 ||g||=4.14e-02 r=0.984 λ=4.88e-08 α=1.000e+00\n",
      "Epoch   20 f=0.162680 test=0.170512 ||g||=2.30e-02 r=0.983 λ=4.77e-11 α=1.000e+00\n",
      "Epoch   30 f=0.159446 test=0.169084 ||g||=1.36e-02 r=0.995 λ=4.66e-14 α=1.000e+00\n",
      "Epoch   40 f=0.158279 test=0.168391 ||g||=9.49e-03 r=1.000 λ=4.55e-17 α=1.000e+00\n",
      "Epoch   49 f=0.157601 test=0.167913 ||g||=7.14e-03 r=1.007 λ=8.88e-20 α=1.000e+00\n",
      "Final - Train 0.157601 Test 0.167913\n",
      "Initial - Train 0.999561 Test 0.994798\n",
      "Epoch    0 f=0.424066 test=0.420512 ||g||=4.12e-01 r=0.997 λ=5.00e-05 α=1.000e+00\n",
      "Epoch   10 f=0.187534 test=0.184603 ||g||=5.01e-02 r=1.003 λ=4.88e-08 α=1.000e+00\n",
      "Epoch   20 f=0.173800 test=0.172349 ||g||=2.97e-02 r=0.998 λ=4.77e-11 α=1.000e+00\n",
      "Epoch   30 f=0.167081 test=0.167393 ||g||=2.27e-02 r=1.003 λ=4.66e-14 α=1.000e+00\n",
      "Epoch   40 f=0.163197 test=0.164946 ||g||=1.80e-02 r=1.011 λ=4.55e-17 α=1.000e+00\n",
      "Epoch   49 f=0.161040 test=0.164139 ||g||=1.38e-02 r=1.009 λ=8.88e-20 α=1.000e+00\n",
      "Final - Train 0.161040 Test 0.164139\n",
      "Initial - Train 0.472132 Test 0.471422\n",
      "Epoch    0 f=0.251975 test=0.250907 ||g||=2.51e-01 r=1.052 λ=5.00e-06 α=1.000e+00\n",
      "Epoch   10 f=0.168561 test=0.171245 ||g||=3.66e-02 r=0.996 λ=4.88e-09 α=1.000e+00\n",
      "Epoch   20 f=0.160998 test=0.166879 ||g||=1.97e-02 r=0.989 λ=4.77e-12 α=1.000e+00\n",
      "Epoch   30 f=0.158537 test=0.167061 ||g||=1.24e-02 r=1.015 λ=4.66e-15 α=1.000e+00\n",
      "Epoch   40 f=0.157510 test=0.167548 ||g||=8.83e-03 r=0.990 λ=4.55e-18 α=1.000e+00\n",
      "Epoch   49 f=0.157027 test=0.167646 ||g||=5.95e-03 r=0.957 λ=8.88e-21 α=1.000e+00\n",
      "Final - Train 0.157027 Test 0.167646\n",
      "Initial - Train 0.714481 Test 0.717420\n",
      "Epoch    0 f=0.332699 test=0.336665 ||g||=3.53e-01 r=0.992 λ=5.00e-06 α=1.000e+00\n",
      "Epoch   10 f=0.171753 test=0.177392 ||g||=4.14e-02 r=0.984 λ=4.88e-09 α=1.000e+00\n",
      "Epoch   20 f=0.162680 test=0.170512 ||g||=2.30e-02 r=0.983 λ=4.77e-12 α=1.000e+00\n",
      "Epoch   30 f=0.159446 test=0.169084 ||g||=1.36e-02 r=0.995 λ=4.66e-15 α=1.000e+00\n",
      "Epoch   40 f=0.158279 test=0.168391 ||g||=9.49e-03 r=1.000 λ=4.55e-18 α=1.000e+00\n",
      "Epoch   49 f=0.157601 test=0.167913 ||g||=7.14e-03 r=1.007 λ=8.88e-21 α=1.000e+00\n",
      "Final - Train 0.157601 Test 0.167913\n",
      "Initial - Train 0.999561 Test 0.994798\n",
      "Epoch    0 f=0.424066 test=0.420512 ||g||=4.12e-01 r=0.997 λ=5.00e-06 α=1.000e+00\n",
      "Epoch   10 f=0.187534 test=0.184603 ||g||=5.01e-02 r=1.003 λ=4.88e-09 α=1.000e+00\n",
      "Epoch   20 f=0.173800 test=0.172349 ||g||=2.97e-02 r=0.998 λ=4.77e-12 α=1.000e+00\n",
      "Epoch   30 f=0.167081 test=0.167393 ||g||=2.27e-02 r=1.003 λ=4.66e-15 α=1.000e+00\n",
      "Epoch   40 f=0.163197 test=0.164946 ||g||=1.80e-02 r=1.011 λ=4.55e-18 α=1.000e+00\n",
      "Epoch   49 f=0.161040 test=0.164139 ||g||=1.38e-02 r=1.009 λ=8.88e-21 α=1.000e+00\n",
      "Final - Train 0.161040 Test 0.164139\n",
      "Initial - Train 0.472132 Test 0.471422\n",
      "Epoch    0 f=0.251975 test=0.250907 ||g||=2.51e-01 r=1.052 λ=5.00e-07 α=1.000e+00\n",
      "Epoch   10 f=0.168561 test=0.171245 ||g||=3.66e-02 r=0.996 λ=4.88e-10 α=1.000e+00\n",
      "Epoch   20 f=0.160998 test=0.166879 ||g||=1.97e-02 r=0.989 λ=4.77e-13 α=1.000e+00\n",
      "Epoch   30 f=0.158537 test=0.167061 ||g||=1.24e-02 r=1.015 λ=4.66e-16 α=1.000e+00\n",
      "Epoch   40 f=0.157510 test=0.167548 ||g||=8.83e-03 r=0.990 λ=4.55e-19 α=1.000e+00\n",
      "Epoch   49 f=0.157027 test=0.167646 ||g||=5.95e-03 r=0.957 λ=8.88e-22 α=1.000e+00\n",
      "Final - Train 0.157027 Test 0.167646\n",
      "Initial - Train 0.714481 Test 0.717420\n",
      "Epoch    0 f=0.332699 test=0.336665 ||g||=3.53e-01 r=0.992 λ=5.00e-07 α=1.000e+00\n",
      "Epoch   10 f=0.171753 test=0.177392 ||g||=4.14e-02 r=0.984 λ=4.88e-10 α=1.000e+00\n",
      "Epoch   20 f=0.162680 test=0.170512 ||g||=2.30e-02 r=0.983 λ=4.77e-13 α=1.000e+00\n",
      "Epoch   30 f=0.159446 test=0.169084 ||g||=1.36e-02 r=0.995 λ=4.66e-16 α=1.000e+00\n",
      "Epoch   40 f=0.158279 test=0.168391 ||g||=9.49e-03 r=1.000 λ=4.55e-19 α=1.000e+00\n",
      "Epoch   49 f=0.157601 test=0.167913 ||g||=7.14e-03 r=1.007 λ=8.88e-22 α=1.000e+00\n",
      "Final - Train 0.157601 Test 0.167913\n",
      "Initial - Train 0.999561 Test 0.994798\n",
      "Epoch    0 f=0.424066 test=0.420512 ||g||=4.12e-01 r=0.997 λ=5.00e-07 α=1.000e+00\n",
      "Epoch   10 f=0.187534 test=0.184603 ||g||=5.01e-02 r=1.003 λ=4.88e-10 α=1.000e+00\n",
      "Epoch   20 f=0.173800 test=0.172349 ||g||=2.97e-02 r=0.998 λ=4.77e-13 α=1.000e+00\n",
      "Epoch   30 f=0.167081 test=0.167393 ||g||=2.27e-02 r=1.003 λ=4.66e-16 α=1.000e+00\n",
      "Epoch   40 f=0.163197 test=0.164946 ||g||=1.80e-02 r=1.011 λ=4.55e-19 α=1.000e+00\n",
      "Epoch   49 f=0.161040 test=0.164139 ||g||=1.38e-02 r=1.009 λ=8.88e-22 α=1.000e+00\n",
      "Final - Train 0.161040 Test 0.164139\n",
      "Initial - Train 0.472132 Test 0.471422\n",
      "Epoch    0 f=0.251975 test=0.250907 ||g||=2.51e-01 r=1.052 λ=5.00e-08 α=1.000e+00\n",
      "Epoch   10 f=0.168561 test=0.171245 ||g||=3.66e-02 r=0.996 λ=4.88e-11 α=1.000e+00\n",
      "Epoch   20 f=0.160998 test=0.166879 ||g||=1.97e-02 r=0.989 λ=4.77e-14 α=1.000e+00\n",
      "Epoch   30 f=0.158537 test=0.167061 ||g||=1.24e-02 r=1.015 λ=4.66e-17 α=1.000e+00\n",
      "Epoch   40 f=0.157510 test=0.167548 ||g||=8.83e-03 r=0.990 λ=4.55e-20 α=1.000e+00\n",
      "Epoch   49 f=0.157027 test=0.167646 ||g||=5.95e-03 r=0.957 λ=8.88e-23 α=1.000e+00\n",
      "Final - Train 0.157027 Test 0.167646\n",
      "Initial - Train 0.714481 Test 0.717420\n",
      "Epoch    0 f=0.332699 test=0.336665 ||g||=3.53e-01 r=0.992 λ=5.00e-08 α=1.000e+00\n",
      "Epoch   10 f=0.171753 test=0.177392 ||g||=4.14e-02 r=0.984 λ=4.88e-11 α=1.000e+00\n",
      "Epoch   20 f=0.162680 test=0.170512 ||g||=2.30e-02 r=0.983 λ=4.77e-14 α=1.000e+00\n",
      "Epoch   30 f=0.159446 test=0.169084 ||g||=1.36e-02 r=0.995 λ=4.66e-17 α=1.000e+00\n",
      "Epoch   40 f=0.158279 test=0.168391 ||g||=9.49e-03 r=1.000 λ=4.55e-20 α=1.000e+00\n",
      "Epoch   49 f=0.157601 test=0.167913 ||g||=7.14e-03 r=1.007 λ=8.88e-23 α=1.000e+00\n",
      "Final - Train 0.157601 Test 0.167913\n",
      "Initial - Train 0.999561 Test 0.994798\n",
      "Epoch    0 f=0.424066 test=0.420512 ||g||=4.12e-01 r=0.997 λ=5.00e-08 α=1.000e+00\n",
      "Epoch   10 f=0.187534 test=0.184603 ||g||=5.01e-02 r=1.003 λ=4.88e-11 α=1.000e+00\n",
      "Epoch   20 f=0.173800 test=0.172349 ||g||=2.97e-02 r=0.998 λ=4.77e-14 α=1.000e+00\n",
      "Epoch   30 f=0.167081 test=0.167393 ||g||=2.27e-02 r=1.003 λ=4.66e-17 α=1.000e+00\n",
      "Epoch   40 f=0.163197 test=0.164946 ||g||=1.80e-02 r=1.011 λ=4.55e-20 α=1.000e+00\n",
      "Epoch   49 f=0.161040 test=0.164139 ||g||=1.38e-02 r=1.009 λ=8.88e-23 α=1.000e+00\n",
      "Final - Train 0.161040 Test 0.164139\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.434957, Test Loss = 0.434412, Grad Norm = 0.519657, dt = 0.500500\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.651665, Test Loss = 0.654865, Grad Norm = 0.677072, dt = 0.500500\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.900593, Test Loss = 0.896165, Grad Norm = 0.837523, dt = 0.500500\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.434957, Test Loss = 0.434412, Grad Norm = 0.519657, dt = 0.500500\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.651665, Test Loss = 0.654865, Grad Norm = 0.677072, dt = 0.500500\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.900593, Test Loss = 0.896165, Grad Norm = 0.837523, dt = 0.500500\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.434957, Test Loss = 0.434412, Grad Norm = 0.519657, dt = 0.500500\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.651665, Test Loss = 0.654865, Grad Norm = 0.677072, dt = 0.500500\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.900593, Test Loss = 0.896165, Grad Norm = 0.837523, dt = 0.500500\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.434957, Test Loss = 0.434412, Grad Norm = 0.519657, dt = 0.500500\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.651665, Test Loss = 0.654865, Grad Norm = 0.677072, dt = 0.500500\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.900593, Test Loss = 0.896165, Grad Norm = 0.837523, dt = 0.500500\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.434957, Test Loss = 0.434412, Grad Norm = 0.519657, dt = 0.500500\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.651665, Test Loss = 0.654865, Grad Norm = 0.677072, dt = 0.500500\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.900593, Test Loss = 0.896165, Grad Norm = 0.837523, dt = 0.500500\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.434957, Test Loss = 0.434412, Grad Norm = 0.519657, dt = 0.500500\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.651665, Test Loss = 0.654865, Grad Norm = 0.677072, dt = 0.500500\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.900593, Test Loss = 0.896165, Grad Norm = 0.837523, dt = 0.500500\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.434957, Test Loss = 0.434412, Grad Norm = 0.519657, dt = 0.500500\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.651665, Test Loss = 0.654865, Grad Norm = 0.677072, dt = 0.500500\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.911916, Test Loss = 0.907460, Grad Norm = 0.845933, dt = 0.500000\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.434957, Test Loss = 0.434412, Grad Norm = 0.519657, dt = 0.500500\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.651665, Test Loss = 0.654865, Grad Norm = 0.677072, dt = 0.500500\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.911916, Test Loss = 0.907460, Grad Norm = 0.845933, dt = 0.500000\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.434957, Test Loss = 0.434412, Grad Norm = 0.519657, dt = 0.500500\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.651665, Test Loss = 0.654865, Grad Norm = 0.677072, dt = 0.500500\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.911916, Test Loss = 0.907460, Grad Norm = 0.845933, dt = 0.500000\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.466023, Test Loss = 0.465337, Grad Norm = 0.548771, dt = 0.200200\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.704170, Test Loss = 0.707146, Grad Norm = 0.712963, dt = 0.200200\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.983184, Test Loss = 0.978479, Grad Norm = 0.896473, dt = 0.200200\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.466023, Test Loss = 0.465337, Grad Norm = 0.548771, dt = 0.200200\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.704170, Test Loss = 0.707146, Grad Norm = 0.712963, dt = 0.200200\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.983184, Test Loss = 0.978479, Grad Norm = 0.896473, dt = 0.200200\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.466023, Test Loss = 0.465337, Grad Norm = 0.548771, dt = 0.200200\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.704170, Test Loss = 0.707146, Grad Norm = 0.712963, dt = 0.200200\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.983184, Test Loss = 0.978479, Grad Norm = 0.896473, dt = 0.200200\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.466023, Test Loss = 0.465337, Grad Norm = 0.548771, dt = 0.200200\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.704170, Test Loss = 0.707146, Grad Norm = 0.712963, dt = 0.200200\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.983184, Test Loss = 0.978479, Grad Norm = 0.896473, dt = 0.200200\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.466023, Test Loss = 0.465337, Grad Norm = 0.548771, dt = 0.200200\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.704170, Test Loss = 0.707146, Grad Norm = 0.712963, dt = 0.200200\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.983184, Test Loss = 0.978479, Grad Norm = 0.896473, dt = 0.200200\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.466023, Test Loss = 0.465337, Grad Norm = 0.548771, dt = 0.200200\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.704170, Test Loss = 0.707146, Grad Norm = 0.712963, dt = 0.200200\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.983184, Test Loss = 0.978479, Grad Norm = 0.896473, dt = 0.200200\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.466023, Test Loss = 0.465337, Grad Norm = 0.548771, dt = 0.200200\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.704170, Test Loss = 0.707146, Grad Norm = 0.712963, dt = 0.200200\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.983184, Test Loss = 0.978479, Grad Norm = 0.896473, dt = 0.200200\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.466023, Test Loss = 0.465337, Grad Norm = 0.548771, dt = 0.200200\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.704170, Test Loss = 0.707146, Grad Norm = 0.712963, dt = 0.200200\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.983184, Test Loss = 0.978479, Grad Norm = 0.896473, dt = 0.200200\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.466023, Test Loss = 0.465337, Grad Norm = 0.548771, dt = 0.200200\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.704170, Test Loss = 0.707146, Grad Norm = 0.712963, dt = 0.200200\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.983184, Test Loss = 0.978479, Grad Norm = 0.896473, dt = 0.200200\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.470599, Test Loss = 0.469896, Grad Norm = 0.552518, dt = 0.100100\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.711894, Test Loss = 0.714842, Grad Norm = 0.717937, dt = 0.100100\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.995448, Test Loss = 0.990699, Grad Norm = 0.904841, dt = 0.100100\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.470599, Test Loss = 0.469896, Grad Norm = 0.552518, dt = 0.100100\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.711894, Test Loss = 0.714842, Grad Norm = 0.717937, dt = 0.100100\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.995448, Test Loss = 0.990699, Grad Norm = 0.904841, dt = 0.100100\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.470599, Test Loss = 0.469896, Grad Norm = 0.552518, dt = 0.100100\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.711894, Test Loss = 0.714842, Grad Norm = 0.717937, dt = 0.100100\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.995448, Test Loss = 0.990699, Grad Norm = 0.904841, dt = 0.100100\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.470599, Test Loss = 0.469896, Grad Norm = 0.552518, dt = 0.100100\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.711894, Test Loss = 0.714842, Grad Norm = 0.717937, dt = 0.100100\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.995448, Test Loss = 0.990699, Grad Norm = 0.904841, dt = 0.100100\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.470599, Test Loss = 0.469896, Grad Norm = 0.552518, dt = 0.100100\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.711894, Test Loss = 0.714842, Grad Norm = 0.717937, dt = 0.100100\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.995448, Test Loss = 0.990699, Grad Norm = 0.904841, dt = 0.100100\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.470599, Test Loss = 0.469896, Grad Norm = 0.552518, dt = 0.100100\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.711894, Test Loss = 0.714842, Grad Norm = 0.717937, dt = 0.100100\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.995448, Test Loss = 0.990699, Grad Norm = 0.904841, dt = 0.100100\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.470599, Test Loss = 0.469896, Grad Norm = 0.552518, dt = 0.100100\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.711894, Test Loss = 0.714842, Grad Norm = 0.717937, dt = 0.100100\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.995448, Test Loss = 0.990699, Grad Norm = 0.904841, dt = 0.100100\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.470599, Test Loss = 0.469896, Grad Norm = 0.552518, dt = 0.100100\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.711894, Test Loss = 0.714842, Grad Norm = 0.717937, dt = 0.100100\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.995448, Test Loss = 0.990699, Grad Norm = 0.904841, dt = 0.100100\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.470599, Test Loss = 0.469896, Grad Norm = 0.552518, dt = 0.100100\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.711894, Test Loss = 0.714842, Grad Norm = 0.717937, dt = 0.100100\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.995448, Test Loss = 0.990699, Grad Norm = 0.904841, dt = 0.100100\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.471748, Test Loss = 0.471040, Grad Norm = 0.553658, dt = 0.050050\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.713833, Test Loss = 0.716775, Grad Norm = 0.719264, dt = 0.050050\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.998531, Test Loss = 0.993772, Grad Norm = 0.906983, dt = 0.050050\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.471748, Test Loss = 0.471040, Grad Norm = 0.553658, dt = 0.050050\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.713833, Test Loss = 0.716775, Grad Norm = 0.719264, dt = 0.050050\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.998531, Test Loss = 0.993772, Grad Norm = 0.906983, dt = 0.050050\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.471748, Test Loss = 0.471040, Grad Norm = 0.553658, dt = 0.050050\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.713833, Test Loss = 0.716775, Grad Norm = 0.719264, dt = 0.050050\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.998531, Test Loss = 0.993772, Grad Norm = 0.906983, dt = 0.050050\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.471748, Test Loss = 0.471040, Grad Norm = 0.553658, dt = 0.050050\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.713833, Test Loss = 0.716775, Grad Norm = 0.719264, dt = 0.050050\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.998531, Test Loss = 0.993772, Grad Norm = 0.906983, dt = 0.050050\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.471748, Test Loss = 0.471040, Grad Norm = 0.553658, dt = 0.050050\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.713833, Test Loss = 0.716775, Grad Norm = 0.719264, dt = 0.050050\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.998531, Test Loss = 0.993772, Grad Norm = 0.906983, dt = 0.050050\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.471748, Test Loss = 0.471040, Grad Norm = 0.553658, dt = 0.050050\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.713833, Test Loss = 0.716775, Grad Norm = 0.719264, dt = 0.050050\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.998531, Test Loss = 0.993772, Grad Norm = 0.906983, dt = 0.050050\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.471748, Test Loss = 0.471040, Grad Norm = 0.553658, dt = 0.050050\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.713833, Test Loss = 0.716775, Grad Norm = 0.719264, dt = 0.050050\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.998531, Test Loss = 0.993772, Grad Norm = 0.906983, dt = 0.050050\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.471748, Test Loss = 0.471040, Grad Norm = 0.553658, dt = 0.050050\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.713833, Test Loss = 0.716775, Grad Norm = 0.719264, dt = 0.050050\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.998531, Test Loss = 0.993772, Grad Norm = 0.906983, dt = 0.050050\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.471748, Test Loss = 0.471040, Grad Norm = 0.553658, dt = 0.050050\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.713833, Test Loss = 0.716775, Grad Norm = 0.719264, dt = 0.050050\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.998531, Test Loss = 0.993772, Grad Norm = 0.906983, dt = 0.050050\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.472116, Test Loss = 0.471407, Grad Norm = 0.554083, dt = 0.010010\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.714455, Test Loss = 0.717394, Grad Norm = 0.719709, dt = 0.010010\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.999520, Test Loss = 0.994757, Grad Norm = 0.907732, dt = 0.010010\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.472116, Test Loss = 0.471407, Grad Norm = 0.554083, dt = 0.010010\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.714455, Test Loss = 0.717394, Grad Norm = 0.719709, dt = 0.010010\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.999520, Test Loss = 0.994757, Grad Norm = 0.907732, dt = 0.010010\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.472116, Test Loss = 0.471407, Grad Norm = 0.554083, dt = 0.010010\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.714455, Test Loss = 0.717394, Grad Norm = 0.719709, dt = 0.010010\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.999520, Test Loss = 0.994757, Grad Norm = 0.907732, dt = 0.010010\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.472116, Test Loss = 0.471407, Grad Norm = 0.554083, dt = 0.010010\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.714455, Test Loss = 0.717394, Grad Norm = 0.719709, dt = 0.010010\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.999520, Test Loss = 0.994757, Grad Norm = 0.907732, dt = 0.010010\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.472116, Test Loss = 0.471407, Grad Norm = 0.554083, dt = 0.010010\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.714455, Test Loss = 0.717394, Grad Norm = 0.719709, dt = 0.010010\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.999520, Test Loss = 0.994757, Grad Norm = 0.907732, dt = 0.010010\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.472116, Test Loss = 0.471407, Grad Norm = 0.554083, dt = 0.010010\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.714455, Test Loss = 0.717394, Grad Norm = 0.719709, dt = 0.010010\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.999520, Test Loss = 0.994757, Grad Norm = 0.907732, dt = 0.010010\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.472116, Test Loss = 0.471407, Grad Norm = 0.554083, dt = 0.010010\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.714455, Test Loss = 0.717394, Grad Norm = 0.719709, dt = 0.010010\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.999520, Test Loss = 0.994757, Grad Norm = 0.907732, dt = 0.010010\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.472116, Test Loss = 0.471407, Grad Norm = 0.554083, dt = 0.010010\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.714455, Test Loss = 0.717394, Grad Norm = 0.719709, dt = 0.010010\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.999520, Test Loss = 0.994757, Grad Norm = 0.907732, dt = 0.010010\n",
      "Initial loss: 0.472132, Gradient norm: 0.554101\n",
      "Epoch 0: Train Loss = 0.472116, Test Loss = 0.471407, Grad Norm = 0.554083, dt = 0.010010\n",
      "Initial loss: 0.714481, Gradient norm: 0.719727\n",
      "Epoch 0: Train Loss = 0.714455, Test Loss = 0.717394, Grad Norm = 0.719709, dt = 0.010010\n",
      "Initial loss: 0.999561, Gradient norm: 0.907759\n",
      "Epoch 0: Train Loss = 0.999520, Test Loss = 0.994757, Grad Norm = 0.907732, dt = 0.010010\n",
      "All parameter tuning results saved to results/param_tuning_results_20251001_185306.json\n"
     ]
    }
   ],
   "source": [
    "# best_hidden = {ds: search_results[ds][\"selected\"][\"hidden_dim\"] for ds in search_results}\n",
    "best_hidden = {\n",
    "    \"sine\": 32,\n",
    "    \"iris\": 16,\n",
    "    \"stroke\": 2,\n",
    "    \"wine\": 8,\n",
    "    \"california\": 64,\n",
    "    \"mnist\": 64\n",
    "}\n",
    "\n",
    "\n",
    "param_grid_sgd = [\n",
    "    {\"lr\": lr, \"momentum\": m}\n",
    "    for lr, m in product(\n",
    "        [0.1, 0.05, 0.01, 0.005, 0.001, 0.0005],\n",
    "        [0.0, 0.5, 0.9]\n",
    "    )\n",
    "]\n",
    "\n",
    "param_grid_scg = [\n",
    "    {\"sigma\": sigma, \"lambda_init\": lam}\n",
    "    for sigma, lam in product(\n",
    "        [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "        [1e-4, 1e-5, 1e-6, 1e-7]\n",
    "    )\n",
    "]\n",
    "\n",
    "param_grid_lfrog = [\n",
    "    {\"dt\": dt, \"max_step\": ms, \"convergence_tol\": tol}\n",
    "    for dt, ms, tol in product(\n",
    "        [0.5, 0.2, 0.1, 0.05, 0.01],\n",
    "        [1.0, 0.5, 0.1],\n",
    "        [1e-4, 1e-5, 1e-6]\n",
    "    )\n",
    "]\n",
    "\n",
    "# --- Run tuning ---\n",
    "all_results = {}\n",
    "\n",
    "# # Iris (classification)\n",
    "# all_results[\"iris\"] = {\n",
    "#     \"sgd\": parameter_tuning(\"iris\", param_grid_sgd, hidden_dim=best_hidden[\"iris\"], optimizer_name=\"sgd\", classification=True),\n",
    "#     \"scg\": parameter_tuning(\"iris\", param_grid_scg, hidden_dim=best_hidden[\"iris\"], optimizer_name=\"scg\", classification=True),\n",
    "#     \"lfrog\": parameter_tuning(\"iris\", param_grid_lfrog, hidden_dim=best_hidden[\"iris\"], optimizer_name=\"lfrog\", classification=True)\n",
    "# }\n",
    "\n",
    "# Sine (regression)\n",
    "all_results[\"sine\"] = {\n",
    "    \"sgd\": parameter_tuning(\"sine\", param_grid_sgd, hidden_dim=best_hidden[\"sine\"], optimizer_name=\"sgd\", classification=False),\n",
    "    \"scg\": parameter_tuning(\"sine\", param_grid_scg, hidden_dim=best_hidden[\"sine\"], optimizer_name=\"scg\", classification=False),\n",
    "    \"lfrog\": parameter_tuning(\"sine\", param_grid_lfrog, hidden_dim=best_hidden[\"sine\"], optimizer_name=\"lfrog\", classification=False)\n",
    "}\n",
    "\n",
    "# Wine (regression)\n",
    "all_results[\"wine\"] = {\n",
    "    \"sgd\": parameter_tuning(\"wine\", param_grid_sgd, hidden_dim=best_hidden[\"wine\"], optimizer_name=\"sgd\", classification=False),\n",
    "    \"scg\": parameter_tuning(\"wine\", param_grid_scg, hidden_dim=best_hidden[\"wine\"], optimizer_name=\"scg\", classification=False),\n",
    "    \"lfrog\": parameter_tuning(\"wine\", param_grid_lfrog, hidden_dim=best_hidden[\"wine\"], optimizer_name=\"lfrog\", classification=False)\n",
    "}\n",
    "\n",
    "# Stroke (classification)\n",
    "all_results[\"stroke\"] = {\n",
    "    \"sgd\": parameter_tuning(\"stroke\", param_grid_sgd, hidden_dim=best_hidden[\"stroke\"], optimizer_name=\"sgd\", classification=True),\n",
    "    \"scg\": parameter_tuning(\"stroke\", param_grid_scg, hidden_dim=best_hidden[\"stroke\"], optimizer_name=\"scg\", classification=True),\n",
    "    \"lfrog\": parameter_tuning(\"stroke\", param_grid_lfrog, hidden_dim=best_hidden[\"stroke\"], optimizer_name=\"lfrog\", classification=True)\n",
    "}\n",
    "\n",
    "# --- Save results ---\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_path = f\"results/param_tuning_results_{timestamp}.json\"\n",
    "\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(f\"All parameter tuning results saved to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "3fbeddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_one_se(results, minimization=True):\n",
    "    \"\"\"\n",
    "    Apply the one-standard-error rule to a list of results for one dataset × optimizer.\n",
    "\n",
    "    Args:\n",
    "        results: list of dicts, each containing \"mean_val_loss\" and \"std_val_loss\"\n",
    "        minimization: True if smaller metric is better (e.g., loss, MSE),\n",
    "                      False if larger metric is better (e.g., accuracy, F1, R2).\n",
    "\n",
    "    Returns:\n",
    "        dict with:\n",
    "          - best_config: the config with smallest mean_val_loss (or largest if maximization)\n",
    "          - selected_config: the parsimonious config chosen via one-SE rule\n",
    "          - threshold: cutoff value\n",
    "          - candidates: all configs within threshold\n",
    "    \"\"\"\n",
    "    if minimization:\n",
    "        # 1. Find best (lowest mean)\n",
    "        best = min(results, key=lambda x: x[\"mean_val_loss\"])\n",
    "        se_best = best[\"std_val_loss\"]  # already averaged per seed → std across runs\n",
    "        threshold = best[\"mean_val_loss\"] + se_best\n",
    "\n",
    "        # 2. All configs within one-SE\n",
    "        candidates = [r for r in results if r[\"mean_val_loss\"] <= threshold]\n",
    "\n",
    "        # 3. Parsimony: choose simplest candidate (here = lowest learning rate if SGD, lowest dt if LFROG)\n",
    "        # Fallback: pick the one with lowest mean_time if multiple remain\n",
    "        selected = min(candidates, key=lambda x: (x[\"params\"].get(\"lr\", x[\"mean_time\"])))\n",
    "    else:\n",
    "        # Same logic but for maximization\n",
    "        best = max(results, key=lambda x: x[\"mean_val_loss\"])\n",
    "        se_best = best[\"std_val_loss\"]\n",
    "        threshold = best[\"mean_val_loss\"] - se_best\n",
    "        candidates = [r for r in results if r[\"mean_val_loss\"] >= threshold]\n",
    "        selected = min(candidates, key=lambda x: (x[\"params\"].get(\"lr\", x[\"mean_time\"])))\n",
    "\n",
    "    return {\n",
    "        \"best_config\": best,\n",
    "        \"selected_config\": selected,\n",
    "        \"threshold\": threshold,\n",
    "        \"candidates\": candidates\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "9bad95d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parsimony_key(result):\n",
    "    p = result[\"params\"]\n",
    "    if \"lr\" in p:\n",
    "        return (p[\"lr\"], result[\"mean_time\"])\n",
    "    if \"dt\" in p:\n",
    "        return (p[\"dt\"], result[\"mean_time\"])\n",
    "    if \"sigma\" in p:\n",
    "        return (p[\"sigma\"], result[\"mean_time\"])\n",
    "    return (result[\"mean_time\"],)\n",
    "\n",
    "def select_best_one_se_generic(results, n_runs, minimization=True):\n",
    "    if minimization:\n",
    "        best = min(results, key=lambda x: x[\"mean_val_loss\"])\n",
    "        se_best = best[\"std_val_loss\"] / np.sqrt(n_runs)\n",
    "        threshold = best[\"mean_val_loss\"] + se_best\n",
    "        candidates = [r for r in results if r[\"mean_val_loss\"] <= threshold]\n",
    "    else:\n",
    "        best = max(results, key=lambda x: x[\"mean_val_loss\"])\n",
    "        se_best = best[\"std_val_loss\"] / np.sqrt(n_runs)\n",
    "        threshold = best[\"mean_val_loss\"] - se_best\n",
    "        candidates = [r for r in results if r[\"mean_val_loss\"] >= threshold]\n",
    "    selected = min(candidates, key=_parsimony_key)\n",
    "    return {\"best_config\": best,\n",
    "            \"selected_config\": selected,\n",
    "            \"threshold\": threshold,\n",
    "            \"candidates\": candidates,\n",
    "            \"se_best\": se_best}\n",
    "\n",
    "def build_selection_summary(all_results, n_runs, minimization=True):\n",
    "    rows = []\n",
    "    for dataset, optim_dict in all_results.items():\n",
    "        for optimizer, results in optim_dict.items():\n",
    "            if not results:\n",
    "                continue\n",
    "            sel = select_best_one_se_generic(results, n_runs=n_runs, minimization=minimization)\n",
    "            best = sel[\"best_config\"]; chosen = sel[\"selected_config\"]\n",
    "            rows.append({\n",
    "                \"dataset\": dataset,\n",
    "                \"optimizer\": optimizer,\n",
    "                \"best_params\": json.dumps(best[\"params\"], sort_keys=True),\n",
    "                \"best_mean_val_loss\": best[\"mean_val_loss\"],\n",
    "                \"best_std_val_loss\": best[\"std_val_loss\"],\n",
    "                \"best_mean_time\": best.get(\"mean_time\"),\n",
    "                \"best_std_time\": best.get(\"std_time\"),\n",
    "                \"selected_params\": json.dumps(chosen[\"params\"], sort_keys=True),\n",
    "                \"selected_mean_val_loss\": chosen[\"mean_val_loss\"],\n",
    "                \"selected_std_val_loss\": chosen[\"std_val_loss\"],\n",
    "                \"selected_mean_time\": chosen.get(\"mean_time\"),\n",
    "                \"selected_std_time\": chosen.get(\"std_time\"),\n",
    "                \"time_ratio_selected_over_best\": (\n",
    "                    (chosen.get(\"mean_time\") / best.get(\"mean_time\"))\n",
    "                    if best.get(\"mean_time\") and chosen.get(\"mean_time\") else None\n",
    "                ),\n",
    "                \"threshold\": sel[\"threshold\"],\n",
    "                \"se_best\": sel[\"se_best\"],\n",
    "                \"n_candidates\": len(sel[\"candidates\"])\n",
    "            })\n",
    "    return pd.DataFrame(rows).sort_values([\"dataset\",\"optimizer\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "21c1866a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dataset optimizer                                        best_params  \\\n",
      "0    sine     lfrog  {\"convergence_tol\": 0.0001, \"dt\": 0.5, \"max_st...   \n",
      "1    sine       scg            {\"lambda_init\": 1e-05, \"sigma\": 0.0005}   \n",
      "2    sine       sgd                       {\"lr\": 0.1, \"momentum\": 0.5}   \n",
      "3  stroke     lfrog  {\"convergence_tol\": 0.0001, \"dt\": 0.5, \"max_st...   \n",
      "4  stroke       scg            {\"lambda_init\": 0.0001, \"sigma\": 1e-05}   \n",
      "5  stroke       sgd                      {\"lr\": 0.01, \"momentum\": 0.9}   \n",
      "6    wine     lfrog  {\"convergence_tol\": 0.0001, \"dt\": 0.5, \"max_st...   \n",
      "7    wine       scg             {\"lambda_init\": 1e-07, \"sigma\": 5e-05}   \n",
      "8    wine       sgd                      {\"lr\": 0.01, \"momentum\": 0.5}   \n",
      "\n",
      "   best_mean_val_loss  best_std_val_loss  best_mean_time  best_std_time  \\\n",
      "0            0.014306           0.000499        0.008822       0.000257   \n",
      "1            0.011611           0.000162        0.015140       0.000720   \n",
      "2            0.012094           0.000746        0.222898       0.006588   \n",
      "3            0.167127           0.002127        0.020008       0.000108   \n",
      "4            0.166566           0.001720        0.033332       0.000129   \n",
      "5            0.164714           0.000736        1.391722       0.012512   \n",
      "6            0.491741           0.003532        0.015438       0.001068   \n",
      "7            0.495710           0.004806        0.029718       0.001619   \n",
      "8            0.482105           0.005875        1.418367       0.009882   \n",
      "\n",
      "                                     selected_params  selected_mean_val_loss  \\\n",
      "0  {\"convergence_tol\": 1e-05, \"dt\": 0.5, \"max_ste...                0.014306   \n",
      "1            {\"lambda_init\": 1e-07, \"sigma\": 0.0005}                0.011611   \n",
      "2                      {\"lr\": 0.01, \"momentum\": 0.9}                0.012122   \n",
      "3  {\"convergence_tol\": 0.0001, \"dt\": 0.5, \"max_st...                0.167127   \n",
      "4             {\"lambda_init\": 1e-06, \"sigma\": 1e-05}                0.166566   \n",
      "5                     {\"lr\": 0.005, \"momentum\": 0.9}                0.164951   \n",
      "6  {\"convergence_tol\": 0.0001, \"dt\": 0.5, \"max_st...                0.491741   \n",
      "7            {\"lambda_init\": 0.0001, \"sigma\": 1e-05}                0.497885   \n",
      "8                     {\"lr\": 0.005, \"momentum\": 0.9}                0.482888   \n",
      "\n",
      "   selected_std_val_loss  selected_mean_time  selected_std_time  \\\n",
      "0               0.000499            0.008709           0.000354   \n",
      "1               0.000162            0.014200           0.000348   \n",
      "2               0.000417            0.219885           0.002043   \n",
      "3               0.002127            0.020008           0.000108   \n",
      "4               0.001720            0.033318           0.000134   \n",
      "5               0.001013            1.401573           0.021143   \n",
      "6               0.003532            0.015438           0.001068   \n",
      "7               0.004299            0.026611           0.000423   \n",
      "8               0.011153            1.431484           0.009275   \n",
      "\n",
      "   time_ratio_selected_over_best  threshold   se_best  n_candidates  \n",
      "0                       0.987154   0.014594  0.000288             3  \n",
      "1                       0.937921   0.011704  0.000094             3  \n",
      "2                       0.986485   0.012525  0.000431             2  \n",
      "3                       1.000000   0.168355  0.001228             3  \n",
      "4                       0.999583   0.167559  0.000993            20  \n",
      "5                       1.007079   0.165139  0.000425             2  \n",
      "6                       1.000000   0.493780  0.002039             3  \n",
      "7                       0.895431   0.498484  0.002774             4  \n",
      "8                       1.009248   0.485497  0.003392             3  \n",
      "Saved summary to:\n",
      "  results/param_tuning_selection_summary_20251001_185517.csv\n",
      "  results/param_tuning_selection_summary_20251001_185517.json\n"
     ]
    }
   ],
   "source": [
    "# Load results\n",
    "with open(\"results/param_tuning_results_20251001_185306.json\", \"r\") as f:\n",
    "    all_results = json.load(f)\n",
    "\n",
    "# Example: sine → SGD results\n",
    "sine_sgd_results = all_results[\"sine\"][\"sgd\"]\n",
    "\n",
    "selection = select_best_one_se(sine_sgd_results, minimization=True)\n",
    "\n",
    "# Ensure results JSON loaded (reuse all_results if already in scope)\n",
    "# If not loaded yet, uncomment:\n",
    "# with open(\"results/param_tuning_results_20251001_173314.json\", \"r\") as f:\n",
    "#     all_results = json.load(f)\n",
    "\n",
    "summary_df = build_selection_summary(all_results, n_runs=3, minimization=True)\n",
    "print(summary_df)\n",
    "\n",
    "# Save\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_path = f\"results/param_tuning_selection_summary_{timestamp}.csv\"\n",
    "json_path = f\"results/param_tuning_selection_summary_{timestamp}.json\"\n",
    "summary_df.to_csv(csv_path, index=False)\n",
    "summary_df.to_json(json_path, orient=\"records\", indent=2)\n",
    "print(f\"Saved summary to:\\n  {csv_path}\\n  {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "e197d7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global hyperparameter choices:\n",
      "sgd: {'lr': 0.01, 'momentum': 0.9} mean_rel_gap=2.813e-03 worst_rel_gap=6.152e-03 avg_rank=2.00\n",
      "scg: {'lambda_init': 1e-07, 'sigma': 0.0005} mean_rel_gap=6.861e-03 worst_rel_gap=2.058e-02 avg_rank=7.33\n",
      "lfrog: {'convergence_tol': 0.0001, 'dt': 0.2, 'max_step': 0.5} mean_rel_gap=1.482e-01 worst_rel_gap=3.894e-01 avg_rank=6.00\n"
     ]
    }
   ],
   "source": [
    "def aggregate_global_best(all_results, optimizers=(\"sgd\",\"scg\",\"lfrog\"), require_full_coverage=False):\n",
    "    \"\"\"\n",
    "    Choose one global hyperparameter set per optimizer across datasets.\n",
    "    Ranks by mean_relative_gap, worst_relative_gap, avg_rank, mean_time_avg.\n",
    "    \"\"\"\n",
    "    per_optimizer = {}\n",
    "    for opt in optimizers:\n",
    "        # Collect runs per dataset for this optimizer\n",
    "        per_dataset = {\n",
    "            ds: opt_dict[opt]\n",
    "            for ds, opt_dict in all_results.items()\n",
    "            if opt in opt_dict and opt_dict[opt]\n",
    "        }\n",
    "        if not per_dataset:\n",
    "            continue\n",
    "\n",
    "        # Best loss per dataset (scale-free normalization)\n",
    "        best_loss = {\n",
    "            ds: min(r[\"mean_val_loss\"] for r in runs)\n",
    "            for ds, runs in per_dataset.items()\n",
    "        }\n",
    "\n",
    "        metrics = defaultdict(lambda: {\"losses\":{}, \"rel\":{}, \"ranks\":{}, \"times\":{}})\n",
    "\n",
    "        # Accumulate stats\n",
    "        for ds, runs in per_dataset.items():\n",
    "            sorted_runs = sorted(runs, key=lambda r: r[\"mean_val_loss\"])\n",
    "            for rank, r in enumerate(sorted_runs, start=1):\n",
    "                k = tuple(sorted(r[\"params\"].items()))\n",
    "                loss = r[\"mean_val_loss\"]\n",
    "                rel_gap = (loss - best_loss[ds]) / (best_loss[ds] + 1e-12)\n",
    "                m = metrics[k]\n",
    "                m[\"losses\"][ds] = loss\n",
    "                m[\"rel\"][ds] = rel_gap\n",
    "                m[\"ranks\"][ds] = rank\n",
    "                m[\"times\"][ds] = r.get(\"mean_time\", math.nan)\n",
    "\n",
    "        rows = []\n",
    "        full_cov = len(per_dataset)\n",
    "        for k, d in metrics.items():\n",
    "            cov = len(d[\"losses\"])\n",
    "            if require_full_coverage and cov < full_cov:\n",
    "                continue\n",
    "            rels = list(d[\"rel\"].values())\n",
    "            ranks = list(d[\"ranks\"].values())\n",
    "            times = list(d[\"times\"].values())\n",
    "            rows.append({\n",
    "                \"optimizer\": opt,\n",
    "                \"params\": dict(k),\n",
    "                \"datasets_covered\": cov,\n",
    "                \"mean_relative_gap\": float(np.mean(rels)),\n",
    "                \"worst_relative_gap\": float(np.max(rels)),\n",
    "                \"avg_rank\": float(np.mean(ranks)),\n",
    "                \"mean_time_avg\": float(np.nanmean(times)),\n",
    "            })\n",
    "\n",
    "        if not rows:\n",
    "            continue\n",
    "\n",
    "        df = pd.DataFrame(rows).sort_values(\n",
    "            [\"mean_relative_gap\",\"worst_relative_gap\",\"avg_rank\",\"mean_time_avg\"]\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        per_optimizer[opt] = {\n",
    "            \"candidates\": df.to_dict(orient=\"records\"),\n",
    "            \"selected\": df.iloc[0].to_dict()\n",
    "        }\n",
    "\n",
    "    return per_optimizer\n",
    "\n",
    "global_hparam_choices = aggregate_global_best(all_results, require_full_coverage=False)\n",
    "\n",
    "print(\"Global hyperparameter choices:\")\n",
    "for opt, info in global_hparam_choices.items():\n",
    "    sel = info[\"selected\"]\n",
    "    print(f\"{opt}: {sel['params']} \"\n",
    "          f\"mean_rel_gap={sel['mean_relative_gap']:.3e} \"\n",
    "          f\"worst_rel_gap={sel['worst_relative_gap']:.3e} \"\n",
    "          f\"avg_rank={sel['avg_rank']:.2f}\")\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "with open(\"results/global_hparam_choices.json\",\"w\") as f:\n",
    "    json.dump(global_hparam_choices, f, indent=2)\n",
    "\n",
    "# Optional: flat CSV of selected\n",
    "selected_rows = [\n",
    "    {\n",
    "        \"optimizer\": opt,\n",
    "        **info[\"selected\"][\"params\"],\n",
    "        **{k: v for k, v in info[\"selected\"].items() if k not in (\"params\",\"optimizer\")}\n",
    "    }\n",
    "    for opt, info in global_hparam_choices.items()\n",
    "]\n",
    "pd.DataFrame(selected_rows).to_csv(\"results/global_hparam_choices_selected.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2860a7c1",
   "metadata": {},
   "source": [
    "### Final Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e31de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris x SGD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef9d413",
   "metadata": {},
   "source": [
    "### Visualisation and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591ce1ea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
