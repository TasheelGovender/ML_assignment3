{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cab3a8a",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf4ce5",
   "metadata": {},
   "source": [
    "This project compares three feedforward neural network training algorithms: Stochastic Gradient Descent (SGD), Scaled Conjugate Gradient (SCG), and LeapFrog. Using six datasets—three for classification and three for regression—the study evaluates convergence speed, stability, and predictive accuracy. Each network has a single hidden layer, with experiments across different hidden layer sizes and hyperparameters. Performance is measured using accuracy and F1-score for classification, and MSE, RMSE, and R² for regression, alongside training time and convergence behavior. The results highlight the strengths and weaknesses of each optimizer across problems of varying complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e30bd8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d9f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "72ddc923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.datasets import load_iris, fetch_california_housing, fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import List, Tuple, Optional, Callable\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994246ee",
   "metadata": {},
   "source": [
    "## Data and Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ca2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preprocessing_pipeline(X, classification=True):\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    numeric_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot',  OneHotEncoder(drop='first'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7c5bc1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mnist(X, y, test_size=0.2, random_state=42):\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X_norm = X / 255.0\n",
    "\n",
    "    # Flatten images (if not already flat)\n",
    "    if len(X_norm.shape) > 2:\n",
    "        X_flat = X_norm.reshape(X_norm.shape[0], -1)\n",
    "    else:\n",
    "        X_flat = X_norm\n",
    "\n",
    "    # Encode target labels\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    # Stratified split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_flat, y_encoded, test_size=test_size, random_state=random_state, stratify=y_encoded\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "de2af212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_california_housing(X, y, skewed_features=['MedInc'], log_target=True, test_size=0.2, random_state=42):\n",
    "    # Log-transform skewed features\n",
    "    X_processed = X.copy()\n",
    "    for col in skewed_features:\n",
    "        X_processed[col] = np.log1p(X_processed[col])\n",
    "\n",
    "    # Log-transform target if needed\n",
    "    if log_target:\n",
    "        y_processed = np.log1p(y)\n",
    "    else:\n",
    "        y_processed = y.copy()\n",
    "\n",
    "    # Build and fit pipeline\n",
    "    preprocessor = build_preprocessing_pipeline(X_processed, classification=False)\n",
    "    X_scaled = preprocessor.fit_transform(X_processed)\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_processed, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9b68706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X, y, classification=True, test_size=0.2, random_state=42, stratify=True):\n",
    "    \"\"\"\n",
    "    General-purpose preprocessing: applies pipeline and splits data.\n",
    "\n",
    "    Args:\n",
    "        X: Features (DataFrame)\n",
    "        y: Target (Series or array)\n",
    "        classification: True for classification, False for regression\n",
    "        test_size: Fraction for test split\n",
    "        random_state: Seed for reproducibility\n",
    "        stratify: Use stratified split for classification\n",
    "\n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Build pipeline\n",
    "    preprocessor = build_preprocessing_pipeline(X, classification=classification)\n",
    "    X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "    # Encode target for classification\n",
    "    if classification:\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le = LabelEncoder()\n",
    "        y_processed = le.fit_transform(y)\n",
    "    else:\n",
    "        y_processed = y\n",
    "\n",
    "    # Split\n",
    "    if classification and stratify:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_processed, y_processed, test_size=test_size, random_state=random_state, stratify=y_processed\n",
    "        )\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_processed, y_processed, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8979ee4b",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52652ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris Dataset\n",
    "def load_iris_data():\n",
    "    iris = sns.load_dataset(\"iris\")\n",
    "    print(iris.head())\n",
    "\n",
    "    X_iris = iris.drop(\"species\", axis=1)\n",
    "    y_iris = iris[\"species\"]\n",
    "    print(X_iris.head())\n",
    "    print(y_iris.head())\n",
    "    return X_iris, y_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e022a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stroke Prediction Dataset\n",
    "def load_stroke_data():\n",
    "    stroke_data = pd.read_csv(\"../data/healthcare-dataset-stroke-data.csv\")\n",
    "    print(stroke_data.shape)\n",
    "    X_stroke = stroke_data.drop(\"stroke\", axis=1)\n",
    "    y_stroke = stroke_data[\"stroke\"]\n",
    "    print(X_stroke.head())\n",
    "    print(y_stroke.head())\n",
    "    return X_stroke, y_stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "61848e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset\n",
    "def load_mnist_subset(n_samples=10000, random_state=42):\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    X = pd.DataFrame(mnist.data)\n",
    "    y = pd.Series(mnist.target, name=\"digit\")\n",
    "    # Sample 10k rows\n",
    "    X_mnist = X.sample(n=n_samples, random_state=random_state)\n",
    "    y_mnist = y.loc[X_mnist.index]\n",
    "    print(X_mnist.shape)\n",
    "    print(X_mnist.head())\n",
    "    print(y_mnist.head())\n",
    "    return X_mnist, y_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0d742d",
   "metadata": {},
   "source": [
    "### Function approx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1a7d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Sine Wave Dataset\n",
    "def generate_sine_wave_data(num_samples: int = 1000, noise_level: float = 0.1) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    X = np.linspace(0, 2 * np.pi, num_samples)\n",
    "    y = np.sin(X) + noise_level * np.random.randn(num_samples)\n",
    "    return X.reshape(-1, 1), y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c07ee2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wine quality dataset from UCI ML Repository\n",
    "def load_wine_quality_data():\n",
    "    wine_quality = fetch_ucirepo(id=186) \n",
    "    \n",
    "    # data (as pandas dataframes) \n",
    "    X_wine = wine_quality.data.features \n",
    "    y_wine = wine_quality.data.targets \n",
    "    print(wine_quality.metadata)\n",
    "    print(wine_quality.variables)\n",
    "    print(X_wine.head())\n",
    "    print(y_wine.head())\n",
    "    print (X_wine.shape, y_wine.shape)\n",
    "    return X_wine, y_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "0ce16cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# California Housing Dataset\n",
    "def load_california_housing_data(n_samples=10000, random_state=42):\n",
    "    california = fetch_california_housing()\n",
    "    X = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "    y = pd.Series(california.target, name=\"MedHouseVal\")\n",
    "    # Sample 10k rows\n",
    "    X_california = X.sample(n=n_samples, random_state=random_state)\n",
    "    y_california = y.loc[X_california.index]\n",
    "    print(X_california.shape)\n",
    "    print(X_california.head())\n",
    "    print(y_california.head())\n",
    "    return X_california, y_california"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11210285",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "782a2018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, activation_fn=nn.ReLU):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
    "        self.activation = activation_fn()\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd77059e",
   "metadata": {},
   "source": [
    "### Training Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ffd22c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sgd(model, X_train, y_train, X_test, y_test, epochs=50, lr=0.01, batch_size=32, classification=True):\n",
    "    criterion = nn.CrossEntropyLoss() if classification else nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    # Fix: Use float32 for regression targets\n",
    "    if classification:\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    else:\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "        train_loss = running_loss / len(loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_test_tensor)\n",
    "            loss = criterion(outputs, y_test_tensor)\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b7052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Losses: [0.2641366285085678, 0.18634218513965606, 0.1823194345831871, 0.18047621548175813, 0.1780942678451538, 0.17647044152021407, 0.17483941376209258, 0.1724669712781906, 0.1709962460398674, 0.16852614551782608, 0.1671838653087616, 0.1653331932425499, 0.16404469221830367, 0.16185179799795152, 0.16005868524312972, 0.15840239375829696, 0.15699963361024857, 0.1552102318406105, 0.1534101501107216, 0.15173113882541656, 0.15004020422697067, 0.14871203601360322, 0.1467205587029457, 0.14426895141601562, 0.14394105732440948, 0.14217049181461333, 0.14105167210102082, 0.13878014475107192, 0.13741614788770676, 0.13608859330415726, 0.13445573180913925, 0.1326293858885765, 0.13131160914897919, 0.1296436882019043, 0.12849194914102555, 0.12695381611585618, 0.12564386934041977, 0.12439741939306259, 0.12267718315124512, 0.1213734084367752, 0.11990109443664551, 0.11816814631223678, 0.11703598380088806, 0.11588238775730134, 0.11457890421152114, 0.11346655637025833, 0.11196885854005814, 0.11062962248921394, 0.10985833883285523, 0.10826268672943115]\n",
      "Test Losses: [0.20377613604068756, 0.19290386140346527, 0.18884789943695068, 0.18825413286685944, 0.184227854013443, 0.182896688580513, 0.182071253657341, 0.180193692445755, 0.17626331746578217, 0.17402906715869904, 0.17445416748523712, 0.16976934671401978, 0.16841921210289001, 0.16805101931095123, 0.16497190296649933, 0.16211815178394318, 0.16167519986629486, 0.15883669257164001, 0.1591225564479828, 0.1558530032634735, 0.1536223441362381, 0.15321385860443115, 0.1502159833908081, 0.14721350371837616, 0.14562919735908508, 0.14629866182804108, 0.14481481909751892, 0.1434796303510666, 0.13963809609413147, 0.13905508816242218, 0.13870559632778168, 0.13572591543197632, 0.13563835620880127, 0.13289108872413635, 0.1302967518568039, 0.13095876574516296, 0.12877973914146423, 0.1253824681043625, 0.12639999389648438, 0.12404874712228775, 0.12358421087265015, 0.11993656307458878, 0.11854462325572968, 0.11839120090007782, 0.11622888594865799, 0.11531539261341095, 0.11322589963674545, 0.11135488748550415, 0.1100684180855751, 0.10963238030672073]\n"
     ]
    }
   ],
   "source": [
    "# # Example usage of SGD - function approximation\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(X_sine, y_sine, classification=False)\n",
    "# model = FeedforwardNN(input_dim=1, hidden_dim=16, output_dim=1)\n",
    "# train_losses, test_losses = train_sgd(model, X_train_scaled, y_train.astype(np.float32), X_test_scaled, y_test.astype(np.float32), classification=False)\n",
    "# print(\"Train Losses:\", train_losses)\n",
    "# print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ded0309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Losses: [1.1347683350245157, 1.1181134939193726, 1.1024710416793824, 1.087291669845581, 1.072637923558553, 1.0583802620569864, 1.0443703571955363, 1.0308705568313599, 1.0174931327501933, 1.004461932182312, 0.9920138756434123, 0.9796104272206624, 0.9673454403877259, 0.9554144382476807, 0.9436902324358623, 0.9318963646888733, 0.9203847408294678, 0.9090999245643616, 0.8978511293729147, 0.8869513034820556, 0.8754758238792419, 0.8644565780957539, 0.8537913044293721, 0.8428460081418355, 0.832503100236257, 0.8221273938814799, 0.812004538377126, 0.801518440246582, 0.7915170073509217, 0.7815437197685242, 0.7717524011929829, 0.7620238184928894, 0.752192231019338, 0.7431997259457906, 0.7337142109870911, 0.7247867425282796, 0.7159688472747803, 0.7070787469546, 0.6985208590825399, 0.6902126948038737, 0.6821887731552124, 0.6737266699473063, 0.6658616582552592, 0.658047862847646, 0.6503567099571228, 0.6428790926933289, 0.6355992595354716, 0.628311284383138, 0.6214141488075257, 0.6144300977389018]\n",
      "Test Losses: [1.1426018476486206, 1.1276479959487915, 1.113135576248169, 1.0990220308303833, 1.085580825805664, 1.072354793548584, 1.0593817234039307, 1.0467437505722046, 1.0343314409255981, 1.0221562385559082, 1.0104469060897827, 0.9986880421638489, 0.987212061882019, 0.9759376049041748, 0.9645947813987732, 0.9535633325576782, 0.9426538944244385, 0.9319896697998047, 0.9213494658470154, 0.9105715155601501, 0.9000673890113831, 0.889665961265564, 0.8794950246810913, 0.8694239258766174, 0.8594240546226501, 0.8496378660202026, 0.8398959636688232, 0.8302934169769287, 0.8207689523696899, 0.8114143013954163, 0.8021090030670166, 0.7929076552391052, 0.783945620059967, 0.7751211524009705, 0.7664585113525391, 0.7578757405281067, 0.7494944930076599, 0.7412846684455872, 0.7331740856170654, 0.7252634167671204, 0.7174291610717773, 0.7097606062889099, 0.7022260427474976, 0.694897472858429, 0.6876818537712097, 0.680575430393219, 0.6735715270042419, 0.6667985916137695, 0.6601513028144836, 0.6536538004875183]\n"
     ]
    }
   ],
   "source": [
    "# # Example usage of SGD - Classification\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(X_iris, y_iris, classification=True)\n",
    "# model = FeedforwardNN(input_dim=4, hidden_dim=16, output_dim=3)\n",
    "# train_losses, test_losses = train_sgd(model, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "# print(\"Train Losses:\", train_losses)\n",
    "# print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "48aa3547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCG Training Function\n",
    "def train_scg(\n",
    "    model: nn.Module,\n",
    "    X_train: torch.Tensor,\n",
    "    y_train: torch.Tensor,\n",
    "    X_test: torch.Tensor,\n",
    "    y_test: torch.Tensor,\n",
    "    max_epochs: int = 1000,\n",
    "    tolerance: float = 1e-6,\n",
    "    sigma: float = 5e-5,\n",
    "    lambda_init: float = 5e-7,\n",
    "    verbose: bool = True,\n",
    "    eval_freq: int = 10\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train a PyTorch neural network using Scaled Conjugate Gradient algorithm.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch neural network model\n",
    "        X_train: Training input data\n",
    "        y_train: Training target data\n",
    "        X_test: Test input data  \n",
    "        y_test: Test target data\n",
    "        max_epochs: Maximum number of training epochs\n",
    "        tolerance: Convergence tolerance for gradient norm\n",
    "        sigma: Parameter for Hessian approximation\n",
    "        lambda_init: Initial regularization parameter\n",
    "        verbose: Whether to print progress\n",
    "        eval_freq: Frequency of evaluation and printing\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_losses, test_losses)\n",
    "    \"\"\"\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    \n",
    "    # Determine loss function based on model output\n",
    "    if y_train.dtype == torch.long or (y_train.ndim == 1 and len(torch.unique(y_train)) <= 10):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        task_type = 'classification'\n",
    "    else:\n",
    "        criterion = nn.MSELoss()\n",
    "        task_type = 'regression'\n",
    "    \n",
    "    # Get total number of parameters\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Helper functions\n",
    "    def get_weights():\n",
    "        \"\"\"Extract all model parameters as a single vector\"\"\"\n",
    "        return torch.cat([p.view(-1) for p in model.parameters()])\n",
    "    \n",
    "    def set_weights(weights):\n",
    "        \"\"\"Set model parameters from a single vector\"\"\"\n",
    "        idx = 0\n",
    "        for p in model.parameters():\n",
    "            param_length = p.numel()\n",
    "            p.data = weights[idx:idx + param_length].view(p.shape)\n",
    "            idx += param_length\n",
    "    \n",
    "    def compute_loss_and_gradient(weights):\n",
    "        \"\"\"Compute loss and gradient for given weights\"\"\"\n",
    "        set_weights(weights)\n",
    "        model.zero_grad()\n",
    "        \n",
    "        outputs = model(X_train)\n",
    "        if task_type == 'classification':\n",
    "            loss = criterion(outputs, y_train)\n",
    "        else:\n",
    "            loss = criterion(outputs, y_train)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Extract gradients\n",
    "        grad = torch.cat([p.grad.view(-1) for p in model.parameters()])\n",
    "        \n",
    "        return loss.item(), grad\n",
    "    \n",
    "    def evaluate_model():\n",
    "        \"\"\"Evaluate model on train and test sets\"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Training loss\n",
    "            train_outputs = model(X_train)\n",
    "            if task_type == 'classification':\n",
    "                train_loss = criterion(train_outputs, y_train).item()\n",
    "            else:\n",
    "                train_loss = criterion(train_outputs, y_train).item()\n",
    "            \n",
    "            # Test loss\n",
    "            test_outputs = model(X_test)\n",
    "            if task_type == 'classification':\n",
    "                test_loss = criterion(test_outputs, y_test).item()\n",
    "            else:\n",
    "                test_loss = criterion(test_outputs, y_test).item()\n",
    "        \n",
    "        model.train()\n",
    "        return train_loss, test_loss\n",
    "    \n",
    "    # Initialize SCG variables\n",
    "    w_k = get_weights()\n",
    "    f_k, g_k = compute_loss_and_gradient(w_k)\n",
    "    r_k = g_k.clone()\n",
    "    r_k_prev = None  # Will store previous gradient for beta calculation\n",
    "    p_k = -r_k.clone()\n",
    "    \n",
    "    lambda_k = lambda_init\n",
    "    lambda_bar = 0.0\n",
    "    success = True\n",
    "    k = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    # Initial evaluation\n",
    "    train_loss, test_loss = evaluate_model()\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Initial - Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}\")\n",
    "    \n",
    "    # Main SCG loop\n",
    "    for epoch in range(max_epochs):\n",
    "        # Step 1: Calculate scaling parameter if successful step\n",
    "        if success:\n",
    "            sigma_k = sigma / torch.sqrt(torch.dot(p_k, p_k))\n",
    "        \n",
    "        # Step 2: Approximate Hessian-vector product\n",
    "        w_temp = w_k + sigma_k * p_k\n",
    "        _, g_temp = compute_loss_and_gradient(w_temp)\n",
    "        s_k = (g_temp - g_k) / sigma_k\n",
    "        \n",
    "        # Step 3 & 4: Scale the search direction\n",
    "        delta_k = torch.dot(p_k, s_k)\n",
    "        \n",
    "        if delta_k <= 0:\n",
    "            s_k = s_k + (lambda_k - delta_k) * p_k\n",
    "            delta_k = lambda_k * torch.dot(p_k, p_k)\n",
    "            lambda_k = 2 * lambda_k\n",
    "        \n",
    "        # Step 5: Calculate step length\n",
    "        mu_k = torch.dot(p_k, r_k)\n",
    "        alpha_k = -mu_k / delta_k  # Note the negative sign for correct step direction\n",
    "        \n",
    "        # Step 6: Calculate comparison parameter  \n",
    "        Delta_k = alpha_k * mu_k  # Expected decrease (should be negative)\n",
    "        \n",
    "        # Step 7: Update weights and evaluate\n",
    "        w_new = w_k + alpha_k * p_k\n",
    "        f_new, _ = compute_loss_and_gradient(w_new)\n",
    "        Delta_f = f_new - f_k\n",
    "        \n",
    "        # Step 8: Test for successful reduction\n",
    "        # In SCG: Delta_k < 0 (predicted decrease), Delta_f should be actual change\n",
    "        # Accept if actual decrease is at least 25% of predicted decrease\n",
    "        if Delta_f < 0.25 * Delta_k:\n",
    "            success = True\n",
    "            lambda_bar = 0\n",
    "            \n",
    "            if Delta_f >= 0.75 * Delta_k:\n",
    "                lambda_k = lambda_k / 4\n",
    "            \n",
    "            # Accept the step\n",
    "            w_k = w_new\n",
    "            f_k = f_new\n",
    "            r_k_prev = r_k.clone()  # Store previous gradient\n",
    "            _, g_k = compute_loss_and_gradient(w_k)\n",
    "            r_k = g_k.clone()\n",
    "            lambda_bar = lambda_bar + lambda_k\n",
    "            lambda_k = lambda_bar\n",
    "            \n",
    "        else:\n",
    "            success = False\n",
    "            lambda_bar = lambda_bar + lambda_k\n",
    "            lambda_k = lambda_bar\n",
    "        \n",
    "        # Step 9: Update search direction (only if successful)\n",
    "        if success:\n",
    "            # Check for restart condition\n",
    "            if k % n_params == 0 or r_k_prev is None:\n",
    "                p_k = -r_k.clone()  # Restart with steepest descent\n",
    "            else:\n",
    "                # Polak-Ribiere formula with numerical stability\n",
    "                beta_k = torch.dot(r_k, r_k - r_k_prev) / (torch.dot(r_k_prev, r_k_prev) + 1e-10)\n",
    "                p_k = -r_k + beta_k * p_k\n",
    "            \n",
    "            k += 1\n",
    "            \n",
    "            # Check convergence\n",
    "            grad_norm = torch.norm(r_k).item()\n",
    "            if grad_norm < tolerance:\n",
    "                if verbose:\n",
    "                    print(f\"Converged at epoch {epoch}, gradient norm: {grad_norm:.2e}\")\n",
    "                break\n",
    "            \n",
    "            # Evaluate and store losses\n",
    "            if epoch % eval_freq == 0 or epoch == max_epochs - 1:\n",
    "                train_loss, test_loss = evaluate_model()\n",
    "                train_losses.append(train_loss)\n",
    "                test_losses.append(test_loss)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch:4d} - Train Loss: {train_loss:.6f}, \"\n",
    "                          f\"Test Loss: {test_loss:.6f}, Grad Norm: {grad_norm:.2e}\")\n",
    "    \n",
    "    # Final evaluation if not already done\n",
    "    if (max_epochs - 1) % eval_freq != 0:\n",
    "        train_loss, test_loss = evaluate_model()\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Training completed. Final - Train Loss: {train_losses[-1]:.6f}, \"\n",
    "              f\"Test Loss: {test_losses[-1]:.6f}\")\n",
    "    \n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a675d",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "*For classification tasks, the output layer is linear and we use `CrossEntropyLoss`, which applies the required softmax internally. For regression tasks, the output layer is also linear, and we use `MSELoss`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de23cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage of SCG - classification\n",
    "# model = FeedforwardNN(input_dim=4, hidden_dim=16, output_dim=3)\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(X, y, classification=True)\n",
    "\n",
    "# X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "# X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# train_losses, test_losses = train_scg(\n",
    "#     model=model,\n",
    "#     X_train=X_train_tensor,\n",
    "#     y_train=y_train_tensor,\n",
    "#     X_test=X_test_tensor,\n",
    "#     y_test=y_test_tensor,\n",
    "#     max_epochs=500,\n",
    "#     tolerance=1e-5,\n",
    "#     verbose=True,\n",
    "#     eval_freq=1\n",
    "# )\n",
    "# print(\"Train Losses:\", train_losses)\n",
    "# print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02672718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage of SCG - function approximation\n",
    "# model = FeedforwardNN(input_dim=1, hidden_dim=16, output_dim=1)\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(X_sine, y_sine, classification=False)\n",
    "# X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "# X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "# train_losses, test_losses = train_scg(\n",
    "#     model=model,\n",
    "#     X_train=X_train_tensor,\n",
    "#     y_train=y_train_tensor,\n",
    "#     X_test=X_test_tensor,\n",
    "#     y_test=y_test_tensor,\n",
    "#     max_epochs=500,\n",
    "#     tolerance=1e-5,\n",
    "#     verbose=True,\n",
    "#     eval_freq=1\n",
    "# )\n",
    "# print(\"Train Losses:\", train_losses)\n",
    "# print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "14362933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lfrog(\n",
    "    model: nn.Module,\n",
    "    X_train: torch.Tensor,\n",
    "    y_train: torch.Tensor,\n",
    "    X_test: torch.Tensor,\n",
    "    y_test: torch.Tensor,\n",
    "    loss_fn: Optional[Callable] = None,\n",
    "    epochs: int = 1000,\n",
    "    dt: float = 0.5,\n",
    "    max_step: float = 1.0,\n",
    "    convergence_tol: float = 1e-5,\n",
    "    max_consecutive_decreases: int = 2,\n",
    "    time_step_reduction_threshold: int = 3,\n",
    "    time_step_increase_factor: float = 0.001,\n",
    "    batch_size: Optional[int] = None,\n",
    "    device: str = 'cpu',\n",
    "    print_every: int = 100,\n",
    "    early_stopping_patience: int = 50,\n",
    "    min_improvement: float = 1e-6\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train a neural network using Snyman's LeapFrog dynamic optimization algorithm.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch neural network model\n",
    "        X_train, y_train: Training data and labels\n",
    "        X_test, y_test: Test data and labels\n",
    "        loss_fn: Loss function (default: MSE for regression, CrossEntropy for classification)\n",
    "        epochs: Maximum number of iterations\n",
    "        dt: Initial time step\n",
    "        max_step: Maximum allowable step size (δ in the paper)\n",
    "        convergence_tol: Convergence tolerance for gradient norm\n",
    "        max_consecutive_decreases: j parameter - max consecutive velocity decreases before reset\n",
    "        time_step_reduction_threshold: m parameter - consecutive steps before time step reduction\n",
    "        time_step_increase_factor: δ₁ parameter for time step increase\n",
    "        batch_size: Batch size (None for full batch)\n",
    "        device: Device to run on\n",
    "        print_every: Print progress every N steps\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_losses, test_losses) lists\n",
    "    \"\"\"\n",
    "    \n",
    "    # Move model and data to device\n",
    "    model = model.to(device)\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "    X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "    \n",
    "    # Auto-detect loss function if not provided\n",
    "    if loss_fn is None:\n",
    "        if len(y_train.shape) == 1 or y_train.shape[1] == 1:\n",
    "            if torch.all((y_train == 0) | (y_train == 1)):\n",
    "                loss_fn = nn.BCEWithLogitsLoss()\n",
    "            else:\n",
    "                loss_fn = nn.MSELoss()\n",
    "        else:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_test_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Get initial parameters as flat vector\n",
    "    params = []\n",
    "    for p in model.parameters():\n",
    "        params.append(p.view(-1))\n",
    "    x_k = torch.cat(params)\n",
    "    n_params = len(x_k)\n",
    "    \n",
    "    # Compute initial gradient and velocity\n",
    "    def compute_loss_and_grad():\n",
    "        model.zero_grad()\n",
    "        if batch_size is None:\n",
    "            outputs = model(X_train)\n",
    "            loss = loss_fn(outputs, y_train)\n",
    "        else:\n",
    "            # Mini-batch gradient\n",
    "            idx = torch.randperm(len(X_train))[:batch_size]\n",
    "            outputs = model(X_train[idx])\n",
    "            loss = loss_fn(outputs, y_train[idx])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Extract gradients as flat vector\n",
    "        grads = []\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                grads.append(p.grad.view(-1))\n",
    "            else:\n",
    "                grads.append(torch.zeros_like(p.view(-1)))\n",
    "        grad = torch.cat(grads)\n",
    "        \n",
    "        return loss.item(), grad\n",
    "    \n",
    "    def update_model_params(x):\n",
    "        \"\"\"Update model parameters from flat parameter vector\"\"\"\n",
    "        idx = 0\n",
    "        for p in model.parameters():\n",
    "            param_size = p.numel()\n",
    "            p.data = x[idx:idx + param_size].view(p.shape)\n",
    "            idx += param_size\n",
    "    \n",
    "    def evaluate_test():\n",
    "        \"\"\"Evaluate model on test set\"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test)\n",
    "            test_loss = loss_fn(test_outputs, y_test)\n",
    "        model.train()\n",
    "        return test_loss.item()\n",
    "    \n",
    "    # Initialize algorithm variables\n",
    "    train_loss, grad_k = compute_loss_and_grad()\n",
    "    v_k = -0.5 * grad_k * dt  # Initial velocity\n",
    "    \n",
    "    # Algorithm state variables\n",
    "    consecutive_decreases = 0\n",
    "    consecutive_negative_dot_products = 0\n",
    "    successful_steps = 0\n",
    "    current_dt = dt\n",
    "    \n",
    "    print(f\"Initial loss: {train_loss:.6f}, Gradient norm: {torch.norm(grad_k):.6f}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Store current state\n",
    "        x_k_old = x_k.clone()\n",
    "        v_k_old = v_k.clone()\n",
    "        grad_k_old = grad_k.clone()\n",
    "        v_k_norm_old = torch.norm(v_k)\n",
    "        \n",
    "        # Step A: Compute step size and limit if necessary\n",
    "        step_size = torch.norm(v_k) * current_dt\n",
    "        if step_size > max_step:\n",
    "            v_k = max_step * v_k / step_size\n",
    "            step_size = max_step\n",
    "        \n",
    "        # Step B: Leap-frog integration\n",
    "        # Update position\n",
    "        x_k = x_k + v_k * current_dt\n",
    "        update_model_params(x_k)\n",
    "        \n",
    "        # Compute new gradient and update velocity\n",
    "        train_loss, grad_k = compute_loss_and_grad()\n",
    "        a_k = -grad_k  # acceleration (negative gradient)\n",
    "        v_k = v_k + a_k * current_dt\n",
    "        \n",
    "        # Record losses\n",
    "        test_loss = evaluate_test()\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if test_loss < best_test_loss - min_improvement:\n",
    "            best_test_loss = test_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping at epoch {epoch}: No improvement in test loss for {early_stopping_patience} epochs\")\n",
    "                print(f\"Best test loss: {best_test_loss:.6f}\")\n",
    "                break\n",
    "        \n",
    "        # Step C: Check convergence\n",
    "        grad_norm = torch.norm(grad_k)\n",
    "        if grad_norm < convergence_tol:\n",
    "            print(f\"Converged at epoch {epoch}: gradient norm {grad_norm:.2e}\")\n",
    "            break\n",
    "        \n",
    "        # Time step control - check for gradient direction changes AND velocity-gradient alignment\n",
    "        if epoch > 0:\n",
    "            dot_product = torch.dot(grad_k, grad_k_old)\n",
    "            velocity_gradient_dot = torch.dot(v_k, grad_k)\n",
    "            \n",
    "            if dot_product <= 0:\n",
    "                consecutive_negative_dot_products += 1\n",
    "            else:\n",
    "                consecutive_negative_dot_products = 0\n",
    "                \n",
    "            # Additional safeguard: if velocity is pointing uphill, this is concerning\n",
    "            if velocity_gradient_dot > 0:\n",
    "                consecutive_negative_dot_products += 1  # Treat as problematic\n",
    "        \n",
    "        # Time step reduction\n",
    "        if consecutive_negative_dot_products >= time_step_reduction_threshold:\n",
    "            current_dt = current_dt / 2\n",
    "            current_dt = max(current_dt, 1e-6)\n",
    "            x_k = (x_k + x_k_old) / 2\n",
    "            v_k = (v_k + v_k_old) / 4\n",
    "            update_model_params(x_k)\n",
    "            consecutive_negative_dot_products = 0\n",
    "            successful_steps = 0\n",
    "            if print_every > 0 and epoch % print_every == 0:\n",
    "                print(f\"Epoch {epoch}: Reduced time step to {current_dt:.6f}\")\n",
    "        \n",
    "        # Step D: Energy monitoring (kinetic energy check)\n",
    "        v_k_norm = torch.norm(v_k)\n",
    "        if v_k_norm > v_k_norm_old:\n",
    "            # Kinetic energy increased - successful step\n",
    "            consecutive_decreases = 0\n",
    "            if step_size < max_step:\n",
    "                successful_steps += 1\n",
    "                # Time step increase - cap growth for stability\n",
    "                growth_factor = min(1.01, 1 + successful_steps * time_step_increase_factor)\n",
    "                current_dt = growth_factor * current_dt\n",
    "                current_dt = max(current_dt, 1e-6)\n",
    "\n",
    "        else:\n",
    "            # Kinetic energy decreased - intervene\n",
    "            consecutive_decreases += 1\n",
    "            successful_steps = 0\n",
    "            \n",
    "            # Restart from midpoint\n",
    "            x_k = (x_k + x_k_old) / 2\n",
    "            update_model_params(x_k)\n",
    "            \n",
    "            if consecutive_decreases <= max_consecutive_decreases:\n",
    "                # Reduce velocity\n",
    "                v_k = (v_k + v_k_old) / 4\n",
    "            else:\n",
    "                # Reset velocity to zero\n",
    "                v_k = torch.zeros_like(v_k)\n",
    "                consecutive_decreases = 0\n",
    "        \n",
    "        # Print progress\n",
    "        if print_every > 0 and epoch % print_every == 0:\n",
    "            print(f\"Epoch {epoch}: Train Loss = {train_loss:.6f}, \"\n",
    "                  f\"Test Loss = {test_loss:.6f}, Grad Norm = {grad_norm:.6f}, \"\n",
    "                  f\"dt = {current_dt:.6f}\")\n",
    "    \n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34a1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage of LFROG - classification\n",
    "# model = FeedforwardNN(input_dim=4, hidden_dim=16, output_dim=3)\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(X, y, classification=True)\n",
    "\n",
    "# X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "# X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# train_losses, test_losses = train_lfrog(\n",
    "#     model=model,\n",
    "#     X_train=X_train_tensor,\n",
    "#     y_train=y_train_tensor,\n",
    "#     X_test=X_test_tensor,\n",
    "#     y_test=y_test_tensor,\n",
    "#     epochs=500,\n",
    "#     dt=0.005,\n",
    "#     max_step=0.05,\n",
    "#     early_stopping_patience=50,  # Stop if no improvement for 50 epochs\n",
    "#     min_improvement=1e-6,        # Minimum improvement threshold\n",
    "#     print_every=25,\n",
    "#     loss_fn=nn.CrossEntropyLoss()\n",
    "# )\n",
    "# print(\"Train Losses:\", train_losses)\n",
    "# print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e5fee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage of LFROG - function approximation\n",
    "# model = FeedforwardNN(input_dim=1, hidden_dim=16, output_dim=1)\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(X_sine, y_sine, classification=False)\n",
    "# X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "# X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)    \n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "# train_losses, test_losses = train_lfrog(\n",
    "#     model=model,\n",
    "#     X_train=X_train_tensor,\n",
    "#     y_train=y_train_tensor,\n",
    "#     X_test=X_test_tensor,\n",
    "#     y_test=y_test_tensor,   \n",
    "#     epochs=500,\n",
    "#     dt=0.005,\n",
    "#     max_step=0.05,\n",
    "#     early_stopping_patience=50,  # Stop if no improvement for 50 epochs\n",
    "#     min_improvement=1e-6,        # Minimum improvement threshold\n",
    "#     print_every=25,\n",
    "#     loss_fn=nn.MSELoss()\n",
    "# )\n",
    "# print(\"Train Losses:\", train_losses)\n",
    "# print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef9d413",
   "metadata": {},
   "source": [
    "### Visualisatin and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591ce1ea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
