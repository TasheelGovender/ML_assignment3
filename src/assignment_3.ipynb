{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cab3a8a",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf4ce5",
   "metadata": {},
   "source": [
    "This project compares three feedforward neural network training algorithms: Stochastic Gradient Descent (SGD), Scaled Conjugate Gradient (SCG), and LeapFrog. Using six datasets—three for classification and three for regression—the study evaluates convergence speed, stability, and predictive accuracy. Each network has a single hidden layer, with experiments across different hidden layer sizes and hyperparameters. Performance is measured using accuracy and F1-score for classification, and MSE, RMSE, and R² for regression, alongside training time and convergence behavior. The results highlight the strengths and weaknesses of each optimizer across problems of varying complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e30bd8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d9f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "72ddc923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.datasets import load_iris\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import List, Tuple, Optional, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994246ee",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8979ee4b",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52652ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width species\n",
      "0           5.1          3.5           1.4          0.2  setosa\n",
      "1           4.9          3.0           1.4          0.2  setosa\n",
      "2           4.7          3.2           1.3          0.2  setosa\n",
      "3           4.6          3.1           1.5          0.2  setosa\n",
      "4           5.0          3.6           1.4          0.2  setosa\n"
     ]
    }
   ],
   "source": [
    "# Iris Dataset\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "print(iris.head())\n",
    "\n",
    "X = iris.drop(\"species\", axis=1)\n",
    "y = iris[\"species\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0d742d",
   "metadata": {},
   "source": [
    "### Function approx."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b257fa5",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a93ca2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X, y, classification=True, test_size=0.2, random_state=42):\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Target processing\n",
    "    if classification:\n",
    "        le = LabelEncoder()\n",
    "        y_train = le.fit_transform(y_train)\n",
    "        y_test = le.transform(y_test)\n",
    "    # For regression, y_train/y_test remain unchanged\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef3700",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(X, y, classification=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11210285",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "782a2018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, activation_fn=nn.ReLU):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
    "        self.activation = activation_fn()\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd77059e",
   "metadata": {},
   "source": [
    "### Training Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffd22c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD Training Function\n",
    "def train_sgd(model, X_train, y_train, X_test, y_test, epochs=50, lr=0.01, batch_size=32):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "        train_loss = running_loss / len(loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_test_tensor)\n",
    "            loss = criterion(outputs, y_test_tensor)\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b7052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of SGD\n",
    "model = FeedforwardNN(input_dim=4, hidden_dim=16, output_dim=3)\n",
    "train_losses, test_losses = train_sgd(model, X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "print(\"Train Losses:\", train_losses)\n",
    "print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "48aa3547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCG Training Function\n",
    "def train_scg(\n",
    "    model: nn.Module,\n",
    "    X_train: torch.Tensor,\n",
    "    y_train: torch.Tensor,\n",
    "    X_test: torch.Tensor,\n",
    "    y_test: torch.Tensor,\n",
    "    max_epochs: int = 1000,\n",
    "    tolerance: float = 1e-6,\n",
    "    sigma: float = 5e-5,\n",
    "    lambda_init: float = 5e-7,\n",
    "    verbose: bool = True,\n",
    "    eval_freq: int = 10\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train a PyTorch neural network using Scaled Conjugate Gradient algorithm.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch neural network model\n",
    "        X_train: Training input data\n",
    "        y_train: Training target data\n",
    "        X_test: Test input data  \n",
    "        y_test: Test target data\n",
    "        max_epochs: Maximum number of training epochs\n",
    "        tolerance: Convergence tolerance for gradient norm\n",
    "        sigma: Parameter for Hessian approximation\n",
    "        lambda_init: Initial regularization parameter\n",
    "        verbose: Whether to print progress\n",
    "        eval_freq: Frequency of evaluation and printing\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (train_losses, test_losses)\n",
    "    \"\"\"\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    \n",
    "    # Determine loss function based on model output\n",
    "    if y_train.dtype == torch.long or (y_train.ndim == 1 and len(torch.unique(y_train)) <= 10):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        task_type = 'classification'\n",
    "    else:\n",
    "        criterion = nn.MSELoss()\n",
    "        task_type = 'regression'\n",
    "    \n",
    "    # Get total number of parameters\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Helper functions\n",
    "    def get_weights():\n",
    "        \"\"\"Extract all model parameters as a single vector\"\"\"\n",
    "        return torch.cat([p.view(-1) for p in model.parameters()])\n",
    "    \n",
    "    def set_weights(weights):\n",
    "        \"\"\"Set model parameters from a single vector\"\"\"\n",
    "        idx = 0\n",
    "        for p in model.parameters():\n",
    "            param_length = p.numel()\n",
    "            p.data = weights[idx:idx + param_length].view(p.shape)\n",
    "            idx += param_length\n",
    "    \n",
    "    def compute_loss_and_gradient(weights):\n",
    "        \"\"\"Compute loss and gradient for given weights\"\"\"\n",
    "        set_weights(weights)\n",
    "        model.zero_grad()\n",
    "        \n",
    "        outputs = model(X_train)\n",
    "        if task_type == 'classification':\n",
    "            loss = criterion(outputs, y_train)\n",
    "        else:\n",
    "            loss = criterion(outputs, y_train)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Extract gradients\n",
    "        grad = torch.cat([p.grad.view(-1) for p in model.parameters()])\n",
    "        \n",
    "        return loss.item(), grad\n",
    "    \n",
    "    def evaluate_model():\n",
    "        \"\"\"Evaluate model on train and test sets\"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Training loss\n",
    "            train_outputs = model(X_train)\n",
    "            if task_type == 'classification':\n",
    "                train_loss = criterion(train_outputs, y_train).item()\n",
    "            else:\n",
    "                train_loss = criterion(train_outputs, y_train).item()\n",
    "            \n",
    "            # Test loss\n",
    "            test_outputs = model(X_test)\n",
    "            if task_type == 'classification':\n",
    "                test_loss = criterion(test_outputs, y_test).item()\n",
    "            else:\n",
    "                test_loss = criterion(test_outputs, y_test).item()\n",
    "        \n",
    "        model.train()\n",
    "        return train_loss, test_loss\n",
    "    \n",
    "    # Initialize SCG variables\n",
    "    w_k = get_weights()\n",
    "    f_k, g_k = compute_loss_and_gradient(w_k)\n",
    "    r_k = g_k.clone()\n",
    "    r_k_prev = None  # Will store previous gradient for beta calculation\n",
    "    p_k = -r_k.clone()\n",
    "    \n",
    "    lambda_k = lambda_init\n",
    "    lambda_bar = 0.0\n",
    "    success = True\n",
    "    k = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    # Initial evaluation\n",
    "    train_loss, test_loss = evaluate_model()\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Initial - Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}\")\n",
    "    \n",
    "    # Main SCG loop\n",
    "    for epoch in range(max_epochs):\n",
    "        # Step 1: Calculate scaling parameter if successful step\n",
    "        if success:\n",
    "            sigma_k = sigma / torch.sqrt(torch.dot(p_k, p_k))\n",
    "        \n",
    "        # Step 2: Approximate Hessian-vector product\n",
    "        w_temp = w_k + sigma_k * p_k\n",
    "        _, g_temp = compute_loss_and_gradient(w_temp)\n",
    "        s_k = (g_temp - g_k) / sigma_k\n",
    "        \n",
    "        # Step 3 & 4: Scale the search direction\n",
    "        delta_k = torch.dot(p_k, s_k)\n",
    "        \n",
    "        if delta_k <= 0:\n",
    "            s_k = s_k + (lambda_k - delta_k) * p_k\n",
    "            delta_k = lambda_k * torch.dot(p_k, p_k)\n",
    "            lambda_k = 2 * lambda_k\n",
    "        \n",
    "        # Step 5: Calculate step length\n",
    "        mu_k = torch.dot(p_k, r_k)\n",
    "        alpha_k = -mu_k / delta_k  # Note the negative sign for correct step direction\n",
    "        \n",
    "        # Step 6: Calculate comparison parameter  \n",
    "        Delta_k = alpha_k * mu_k  # Expected decrease (should be negative)\n",
    "        \n",
    "        # Step 7: Update weights and evaluate\n",
    "        w_new = w_k + alpha_k * p_k\n",
    "        f_new, _ = compute_loss_and_gradient(w_new)\n",
    "        Delta_f = f_new - f_k\n",
    "        \n",
    "        # Step 8: Test for successful reduction\n",
    "        # In SCG: Delta_k < 0 (predicted decrease), Delta_f should be actual change\n",
    "        # Accept if actual decrease is at least 25% of predicted decrease\n",
    "        if Delta_f < 0.25 * Delta_k:\n",
    "            success = True\n",
    "            lambda_bar = 0\n",
    "            \n",
    "            if Delta_f >= 0.75 * Delta_k:\n",
    "                lambda_k = lambda_k / 4\n",
    "            \n",
    "            # Accept the step\n",
    "            w_k = w_new\n",
    "            f_k = f_new\n",
    "            r_k_prev = r_k.clone()  # Store previous gradient\n",
    "            _, g_k = compute_loss_and_gradient(w_k)\n",
    "            r_k = g_k.clone()\n",
    "            lambda_bar = lambda_bar + lambda_k\n",
    "            lambda_k = lambda_bar\n",
    "            \n",
    "        else:\n",
    "            success = False\n",
    "            lambda_bar = lambda_bar + lambda_k\n",
    "            lambda_k = lambda_bar\n",
    "        \n",
    "        # Step 9: Update search direction (only if successful)\n",
    "        if success:\n",
    "            # Check for restart condition\n",
    "            if k % n_params == 0 or r_k_prev is None:\n",
    "                p_k = -r_k.clone()  # Restart with steepest descent\n",
    "            else:\n",
    "                # Polak-Ribiere formula with numerical stability\n",
    "                beta_k = torch.dot(r_k, r_k - r_k_prev) / (torch.dot(r_k_prev, r_k_prev) + 1e-10)\n",
    "                p_k = -r_k + beta_k * p_k\n",
    "            \n",
    "            k += 1\n",
    "            \n",
    "            # Check convergence\n",
    "            grad_norm = torch.norm(r_k).item()\n",
    "            if grad_norm < tolerance:\n",
    "                if verbose:\n",
    "                    print(f\"Converged at epoch {epoch}, gradient norm: {grad_norm:.2e}\")\n",
    "                break\n",
    "            \n",
    "            # Evaluate and store losses\n",
    "            if epoch % eval_freq == 0 or epoch == max_epochs - 1:\n",
    "                train_loss, test_loss = evaluate_model()\n",
    "                train_losses.append(train_loss)\n",
    "                test_losses.append(test_loss)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch:4d} - Train Loss: {train_loss:.6f}, \"\n",
    "                          f\"Test Loss: {test_loss:.6f}, Grad Norm: {grad_norm:.2e}\")\n",
    "    \n",
    "    # Final evaluation if not already done\n",
    "    if (max_epochs - 1) % eval_freq != 0:\n",
    "        train_loss, test_loss = evaluate_model()\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Training completed. Final - Train Loss: {train_losses[-1]:.6f}, \"\n",
    "              f\"Test Loss: {test_losses[-1]:.6f}\")\n",
    "    \n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a675d",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "\n",
    "*For classification tasks, the output layer is linear and we use `CrossEntropyLoss`, which applies the required softmax internally. For regression tasks, the output layer is also linear, and we use `MSELoss`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de23cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of SCG\n",
    "model = FeedforwardNN(input_dim=4, hidden_dim=16, output_dim=3)\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(X, y, classification=True)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_losses, test_losses = train_scg(\n",
    "    model=model,\n",
    "    X_train=X_train_tensor,\n",
    "    y_train=y_train_tensor,\n",
    "    X_test=X_test_tensor,\n",
    "    y_test=y_test_tensor,\n",
    "    max_epochs=500,\n",
    "    tolerance=1e-5,\n",
    "    verbose=True,\n",
    "    eval_freq=1\n",
    ")\n",
    "print(\"Train Losses:\", train_losses)\n",
    "print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "14362933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lfrog(\n",
    "    model: nn.Module,\n",
    "    X_train: torch.Tensor,\n",
    "    y_train: torch.Tensor,\n",
    "    X_test: torch.Tensor,\n",
    "    y_test: torch.Tensor,\n",
    "    loss_fn: Optional[Callable] = None,\n",
    "    epochs: int = 1000,\n",
    "    dt: float = 0.5,\n",
    "    max_step: float = 1.0,\n",
    "    convergence_tol: float = 1e-5,\n",
    "    max_consecutive_decreases: int = 2,\n",
    "    time_step_reduction_threshold: int = 3,\n",
    "    time_step_increase_factor: float = 0.001,\n",
    "    batch_size: Optional[int] = None,\n",
    "    device: str = 'cpu',\n",
    "    print_every: int = 100,\n",
    "    early_stopping_patience: int = 50,\n",
    "    min_improvement: float = 1e-6\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train a neural network using Snyman's LeapFrog dynamic optimization algorithm.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch neural network model\n",
    "        X_train, y_train: Training data and labels\n",
    "        X_test, y_test: Test data and labels\n",
    "        loss_fn: Loss function (default: MSE for regression, CrossEntropy for classification)\n",
    "        epochs: Maximum number of iterations\n",
    "        dt: Initial time step\n",
    "        max_step: Maximum allowable step size (δ in the paper)\n",
    "        convergence_tol: Convergence tolerance for gradient norm\n",
    "        max_consecutive_decreases: j parameter - max consecutive velocity decreases before reset\n",
    "        time_step_reduction_threshold: m parameter - consecutive steps before time step reduction\n",
    "        time_step_increase_factor: δ₁ parameter for time step increase\n",
    "        batch_size: Batch size (None for full batch)\n",
    "        device: Device to run on\n",
    "        print_every: Print progress every N steps\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_losses, test_losses) lists\n",
    "    \"\"\"\n",
    "    \n",
    "    # Move model and data to device\n",
    "    model = model.to(device)\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "    X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "    \n",
    "    # Auto-detect loss function if not provided\n",
    "    if loss_fn is None:\n",
    "        if len(y_train.shape) == 1 or y_train.shape[1] == 1:\n",
    "            if torch.all((y_train == 0) | (y_train == 1)):\n",
    "                loss_fn = nn.BCEWithLogitsLoss()\n",
    "            else:\n",
    "                loss_fn = nn.MSELoss()\n",
    "        else:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_test_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Get initial parameters as flat vector\n",
    "    params = []\n",
    "    for p in model.parameters():\n",
    "        params.append(p.view(-1))\n",
    "    x_k = torch.cat(params)\n",
    "    n_params = len(x_k)\n",
    "    \n",
    "    # Compute initial gradient and velocity\n",
    "    def compute_loss_and_grad():\n",
    "        model.zero_grad()\n",
    "        if batch_size is None:\n",
    "            outputs = model(X_train)\n",
    "            loss = loss_fn(outputs, y_train)\n",
    "        else:\n",
    "            # Mini-batch gradient\n",
    "            idx = torch.randperm(len(X_train))[:batch_size]\n",
    "            outputs = model(X_train[idx])\n",
    "            loss = loss_fn(outputs, y_train[idx])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Extract gradients as flat vector\n",
    "        grads = []\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                grads.append(p.grad.view(-1))\n",
    "            else:\n",
    "                grads.append(torch.zeros_like(p.view(-1)))\n",
    "        grad = torch.cat(grads)\n",
    "        \n",
    "        return loss.item(), grad\n",
    "    \n",
    "    def update_model_params(x):\n",
    "        \"\"\"Update model parameters from flat parameter vector\"\"\"\n",
    "        idx = 0\n",
    "        for p in model.parameters():\n",
    "            param_size = p.numel()\n",
    "            p.data = x[idx:idx + param_size].view(p.shape)\n",
    "            idx += param_size\n",
    "    \n",
    "    def evaluate_test():\n",
    "        \"\"\"Evaluate model on test set\"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test)\n",
    "            test_loss = loss_fn(test_outputs, y_test)\n",
    "        model.train()\n",
    "        return test_loss.item()\n",
    "    \n",
    "    # Initialize algorithm variables\n",
    "    train_loss, grad_k = compute_loss_and_grad()\n",
    "    v_k = -0.5 * grad_k * dt  # Initial velocity\n",
    "    \n",
    "    # Algorithm state variables\n",
    "    consecutive_decreases = 0\n",
    "    consecutive_negative_dot_products = 0\n",
    "    successful_steps = 0\n",
    "    current_dt = dt\n",
    "    \n",
    "    print(f\"Initial loss: {train_loss:.6f}, Gradient norm: {torch.norm(grad_k):.6f}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Store current state\n",
    "        x_k_old = x_k.clone()\n",
    "        v_k_old = v_k.clone()\n",
    "        grad_k_old = grad_k.clone()\n",
    "        v_k_norm_old = torch.norm(v_k)\n",
    "        \n",
    "        # Step A: Compute step size and limit if necessary\n",
    "        step_size = torch.norm(v_k) * current_dt\n",
    "        if step_size > max_step:\n",
    "            v_k = max_step * v_k / step_size\n",
    "            step_size = max_step\n",
    "        \n",
    "        # Step B: Leap-frog integration\n",
    "        # Update position\n",
    "        x_k = x_k + v_k * current_dt\n",
    "        update_model_params(x_k)\n",
    "        \n",
    "        # Compute new gradient and update velocity\n",
    "        train_loss, grad_k = compute_loss_and_grad()\n",
    "        a_k = -grad_k  # acceleration (negative gradient)\n",
    "        v_k = v_k + a_k * current_dt\n",
    "        \n",
    "        # Record losses\n",
    "        test_loss = evaluate_test()\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if test_loss < best_test_loss - min_improvement:\n",
    "            best_test_loss = test_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping at epoch {epoch}: No improvement in test loss for {early_stopping_patience} epochs\")\n",
    "                print(f\"Best test loss: {best_test_loss:.6f}\")\n",
    "                break\n",
    "        \n",
    "        # Step C: Check convergence\n",
    "        grad_norm = torch.norm(grad_k)\n",
    "        if grad_norm < convergence_tol:\n",
    "            print(f\"Converged at epoch {epoch}: gradient norm {grad_norm:.2e}\")\n",
    "            break\n",
    "        \n",
    "        # Time step control - check for gradient direction changes AND velocity-gradient alignment\n",
    "        if epoch > 0:\n",
    "            dot_product = torch.dot(grad_k, grad_k_old)\n",
    "            velocity_gradient_dot = torch.dot(v_k, grad_k)\n",
    "            \n",
    "            if dot_product <= 0:\n",
    "                consecutive_negative_dot_products += 1\n",
    "            else:\n",
    "                consecutive_negative_dot_products = 0\n",
    "                \n",
    "            # Additional safeguard: if velocity is pointing uphill, this is concerning\n",
    "            if velocity_gradient_dot > 0:\n",
    "                consecutive_negative_dot_products += 1  # Treat as problematic\n",
    "        \n",
    "        # Time step reduction\n",
    "        if consecutive_negative_dot_products >= time_step_reduction_threshold:\n",
    "            current_dt = current_dt / 2\n",
    "            current_dt = max(current_dt, 1e-6)\n",
    "            x_k = (x_k + x_k_old) / 2\n",
    "            v_k = (v_k + v_k_old) / 4\n",
    "            update_model_params(x_k)\n",
    "            consecutive_negative_dot_products = 0\n",
    "            successful_steps = 0\n",
    "            if print_every > 0 and epoch % print_every == 0:\n",
    "                print(f\"Epoch {epoch}: Reduced time step to {current_dt:.6f}\")\n",
    "        \n",
    "        # Step D: Energy monitoring (kinetic energy check)\n",
    "        v_k_norm = torch.norm(v_k)\n",
    "        if v_k_norm > v_k_norm_old:\n",
    "            # Kinetic energy increased - successful step\n",
    "            consecutive_decreases = 0\n",
    "            if step_size < max_step:\n",
    "                successful_steps += 1\n",
    "                # Time step increase - cap growth for stability\n",
    "                growth_factor = min(1.01, 1 + successful_steps * time_step_increase_factor)\n",
    "                current_dt = growth_factor * current_dt\n",
    "                current_dt = max(current_dt, 1e-6)\n",
    "\n",
    "        else:\n",
    "            # Kinetic energy decreased - intervene\n",
    "            consecutive_decreases += 1\n",
    "            successful_steps = 0\n",
    "            \n",
    "            # Restart from midpoint\n",
    "            x_k = (x_k + x_k_old) / 2\n",
    "            update_model_params(x_k)\n",
    "            \n",
    "            if consecutive_decreases <= max_consecutive_decreases:\n",
    "                # Reduce velocity\n",
    "                v_k = (v_k + v_k_old) / 4\n",
    "            else:\n",
    "                # Reset velocity to zero\n",
    "                v_k = torch.zeros_like(v_k)\n",
    "                consecutive_decreases = 0\n",
    "        \n",
    "        # Print progress\n",
    "        if print_every > 0 and epoch % print_every == 0:\n",
    "            print(f\"Epoch {epoch}: Train Loss = {train_loss:.6f}, \"\n",
    "                  f\"Test Loss = {test_loss:.6f}, Grad Norm = {grad_norm:.6f}, \"\n",
    "                  f\"dt = {current_dt:.6f}\")\n",
    "    \n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f34a1a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 1.233758, Gradient norm: 0.647875\n",
      "Epoch 0: Train Loss = 1.233753, Test Loss = 1.237413, Grad Norm = 0.647870, dt = 0.005005\n",
      "Epoch 25: Train Loss = 1.229558, Test Loss = 1.232980, Grad Norm = 0.644134, dt = 0.006193\n",
      "Epoch 50: Train Loss = 1.213176, Test Loss = 1.215644, Grad Norm = 0.629470, dt = 0.007942\n",
      "Epoch 75: Train Loss = 1.175905, Test Loss = 1.176072, Grad Norm = 0.596699, dt = 0.010186\n",
      "Epoch 100: Train Loss = 1.107208, Test Loss = 1.102618, Grad Norm = 0.543188, dt = 0.013062\n",
      "Epoch 125: Train Loss = 0.999380, Test Loss = 0.984256, Grad Norm = 0.480960, dt = 0.016751\n",
      "Epoch 150: Train Loss = 0.850173, Test Loss = 0.815665, Grad Norm = 0.448073, dt = 0.021483\n",
      "Epoch 175: Train Loss = 0.659971, Test Loss = 0.600434, Grad Norm = 0.402417, dt = 0.027550\n",
      "Epoch 200: Train Loss = 0.481025, Test Loss = 0.404988, Grad Norm = 0.322561, dt = 0.035331\n",
      "Epoch 225: Train Loss = 0.380186, Test Loss = 0.309734, Grad Norm = 0.353253, dt = 0.042475\n",
      "Epoch 250: Train Loss = 0.242737, Test Loss = 0.170656, Grad Norm = 0.134584, dt = 0.054472\n",
      "Epoch 275: Train Loss = 0.201310, Test Loss = 0.117302, Grad Norm = 0.244148, dt = 0.066141\n",
      "Epoch 300: Train Loss = 0.140697, Test Loss = 0.109044, Grad Norm = 0.241887, dt = 0.083234\n",
      "Epoch 325: Train Loss = 0.095698, Test Loss = 0.059988, Grad Norm = 0.118679, dt = 0.097513\n",
      "Epoch 350: Train Loss = 0.078982, Test Loss = 0.068157, Grad Norm = 0.095461, dt = 0.123820\n",
      "Epoch 375: Train Loss = 0.058299, Test Loss = 0.048577, Grad Norm = 0.023802, dt = 0.158791\n",
      "Epoch 400: Train Loss = 0.053011, Test Loss = 0.039077, Grad Norm = 0.016377, dt = 0.180927\n",
      "Epoch 425: Train Loss = 0.050050, Test Loss = 0.031902, Grad Norm = 0.016132, dt = 0.231797\n",
      "Epoch 450: Train Loss = 0.047243, Test Loss = 0.020168, Grad Norm = 0.015924, dt = 0.289388\n",
      "Epoch 475: Train Loss = 0.046719, Test Loss = 0.017841, Grad Norm = 0.006089, dt = 0.325509\n",
      "Train Losses: [1.2337530851364136, 1.2337373495101929, 1.2337110042572021, 1.2336739301681519, 1.2336262464523315, 1.2335675954818726, 1.2334976196289062, 1.233415961265564, 1.2333227396011353, 1.2332172393798828, 1.2330989837646484, 1.2329679727554321, 1.2328237295150757, 1.2326656579971313, 1.2324931621551514, 1.2323068380355835, 1.2321054935455322, 1.2318888902664185, 1.2316566705703735, 1.2314083576202393, 1.231143593788147, 1.2308621406555176, 1.2305630445480347, 1.230246663093567, 1.2299116849899292, 1.2295582294464111, 1.229185700416565, 1.2287936210632324, 1.2283815145492554, 1.2279486656188965, 1.2274949550628662, 1.2270197868347168, 1.2265225648880005, 1.2260024547576904, 1.2254595756530762, 1.2248929738998413, 1.2243022918701172, 1.2236872911453247, 1.2230467796325684, 1.2223803997039795, 1.2216877937316895, 1.220968246459961, 1.220220685005188, 1.2194453477859497, 1.2186410427093506, 1.2178072929382324, 1.2169435024261475, 1.2160491943359375, 1.2151237726211548, 1.2141660451889038, 1.2131760120391846, 1.2121527194976807, 1.2110955715179443, 1.210004210472107, 1.2088781595230103, 1.2077162265777588, 1.2065184116363525, 1.2052834033966064, 1.2040107250213623, 1.2026996612548828, 1.201349139213562, 1.1999586820602417, 1.1985275745391846, 1.1970551013946533, 1.1955406665802002, 1.1939834356307983, 1.1923828125, 1.1907380819320679, 1.189048409461975, 1.1873128414154053, 1.1855307817459106, 1.1837016344070435, 1.1818249225616455, 1.1798999309539795, 1.1779273748397827, 1.1759051084518433, 1.1738320589065552, 1.1717084646224976, 1.1695327758789062, 1.1673046350479126, 1.1650232076644897, 1.1626824140548706, 1.160285234451294, 1.1578333377838135, 1.1553254127502441, 1.1527615785598755, 1.1501445770263672, 1.1474710702896118, 1.1447365283966064, 1.1419429779052734, 1.139089822769165, 1.1361767053604126, 1.1332039833068848, 1.130170226097107, 1.1270745992660522, 1.123916745185852, 1.1206973791122437, 1.1174179315567017, 1.1140763759613037, 1.110673427581787, 1.1072077751159668, 1.1036794185638428, 1.1000884771347046, 1.0964322090148926, 1.0927112102508545, 1.0889265537261963, 1.0850765705108643, 1.0811601877212524, 1.0771781206130981, 1.0731292963027954, 1.0690135955810547, 1.0648304224014282, 1.060579538345337, 1.056264877319336, 1.0518879890441895, 1.0474445819854736, 1.042934775352478, 1.0383576154708862, 1.033715844154358, 1.0290066003799438, 1.0242325067520142, 1.0193923711776733, 1.0144846439361572, 1.009514570236206, 1.0044798851013184, 0.9993799924850464, 0.9942134618759155, 0.9889783263206482, 0.9836744666099548, 0.9783061742782593, 0.9728679656982422, 0.9673598408699036, 0.9617889523506165, 0.956147313117981, 0.9504525065422058, 0.9447039365768433, 0.938892126083374, 0.9330122470855713, 0.9270600080490112, 0.9210397005081177, 0.9149523973464966, 0.9087979197502136, 0.9025706052780151, 0.8962730169296265, 0.8899025321006775, 0.8834631443023682, 0.8769495487213135, 0.8703607320785522, 0.863696277141571, 0.8569676280021667, 0.8501725792884827, 0.8433132171630859, 0.8363809585571289, 0.8293809294700623, 0.8223026990890503, 0.8151512145996094, 0.8079231381416321, 0.8006228804588318, 0.7932493090629578, 0.7858079671859741, 0.7782965898513794, 0.7707281708717346, 0.7630938291549683, 0.7553942203521729, 0.7476414442062378, 0.7398436665534973, 0.7319920063018799, 0.724091649055481, 0.7161599397659302, 0.7081955671310425, 0.7002044320106506, 0.6921894550323486, 0.6841457486152649, 0.6760941743850708, 0.6680352687835693, 0.659971296787262, 0.6519066691398621, 0.6438542008399963, 0.6358305215835571, 0.6278356909751892, 0.6198728084564209, 0.6119609475135803, 0.6041118502616882, 0.5963238477706909, 0.5885922312736511, 0.5809324979782104, 0.5733633637428284, 0.5658918023109436, 0.5585240721702576, 0.5512698888778687, 0.5441268086433411, 0.5371055006980896, 0.5302445888519287, 0.5235523581504822, 0.5170214772224426, 0.5106438398361206, 0.5044025182723999, 0.4983151853084564, 0.4923911392688751, 0.48662126064300537, 0.48102518916130066, 0.47556594014167786, 0.47026166319847107, 0.46508315205574036, 0.46000951528549194, 0.4550224542617798, 0.4501112997531891, 0.4452608823776245, 0.4404551684856415, 0.4356956481933594, 0.4309729337692261, 0.4262729585170746, 0.4215623736381531, 0.4168848693370819, 0.4168318808078766, 0.41433185338974, 0.4117204248905182, 0.40895968675613403, 0.4060301184654236, 0.40292173624038696, 0.3996271789073944, 0.39614221453666687, 0.3924635648727417, 0.3885813057422638, 0.38448867201805115, 0.38018566370010376, 0.3756795823574066, 0.37096789479255676, 0.36605483293533325, 0.3609677255153656, 0.35568395256996155, 0.35022059082984924, 0.34460708498954773, 0.338825523853302, 0.3329315781593323, 0.32692715525627136, 0.3208230137825012, 0.31466108560562134, 0.30844852328300476, 0.3022022545337677, 0.29595857858657837, 0.2897452712059021, 0.283620148897171, 0.277615487575531, 0.2717726528644562, 0.26616939902305603, 0.2608295977115631, 0.2557736337184906, 0.25104671716690063, 0.24669066071510315, 0.24273653328418732, 0.23922006785869598, 0.23619048297405243, 0.23368029296398163, 0.23169048130512238, 0.2302376925945282, 0.22929710149765015, 0.2292657494544983, 0.22886066138744354, 0.22845469415187836, 0.22802157700061798, 0.22753450274467468, 0.2269657552242279, 0.22627930343151093, 0.22544942796230316, 0.22445546090602875, 0.22327102720737457, 0.22186978161334991, 0.2202303111553192, 0.21833637356758118, 0.21617381274700165, 0.21373558044433594, 0.21102012693881989, 0.20803356170654297, 0.2047899216413498, 0.2013104259967804, 0.19762322306632996, 0.19376346468925476, 0.1897769421339035, 0.18571104109287262, 0.1816205233335495, 0.17755867540836334, 0.17357619106769562, 0.16973184049129486, 0.16609002649784088, 0.16269813477993011, 0.15957039594650269, 0.15686100721359253, 0.1545807421207428, 0.15259228646755219, 0.15088582038879395, 0.14943726360797882, 0.1481921374797821, 0.14711402356624603, 0.14612296223640442, 0.14519697427749634, 0.1442727893590927, 0.1432761549949646, 0.14213703572750092, 0.14080141484737396, 0.14069710671901703, 0.1395740807056427, 0.13806863129138947, 0.1361825317144394, 0.1339307576417923, 0.13134264945983887, 0.12845997512340546, 0.12533804774284363, 0.12204515933990479, 0.1186518594622612, 0.11515042185783386, 0.11168723553419113, 0.10840842127799988, 0.10539375245571136, 0.10273263603448868, 0.10050364583730698, 0.09876859188079834, 0.09756674617528915, 0.09691047668457031, 0.09680218249559402, 0.09677428752183914, 0.09674659371376038, 0.09665144234895706, 0.09645368158817291, 0.09613852202892303, 0.0956982895731926, 0.0951305702328682, 0.09444095939397812, 0.09364224225282669, 0.09275378286838531, 0.09180054068565369, 0.09081149101257324, 0.08981890976428986, 0.08885558694601059, 0.08795133233070374, 0.08713023364543915, 0.08641032129526138, 0.08580291271209717, 0.08530402928590775, 0.0849020853638649, 0.08457720279693604, 0.08430246263742447, 0.08404682576656342, 0.08377259224653244, 0.08344319462776184, 0.08302538841962814, 0.0824936106801033, 0.08182678371667862, 0.08101557940244675, 0.08006247133016586, 0.07898218184709549, 0.07780123502016068, 0.07655660808086395, 0.07529298216104507, 0.07405819743871689, 0.07290033996105194, 0.07186343520879745, 0.07098057121038437, 0.07026909291744232, 0.06973102688789368, 0.06934630125761032, 0.06907061487436295, 0.0688820332288742, 0.06868920475244522, 0.06840243190526962, 0.06795597821474075, 0.06730734556913376, 0.0664430558681488, 0.06538251042366028, 0.06417685002088547, 0.06290246546268463, 0.06164686381816864, 0.06049841642379761, 0.05953088775277138, 0.05879269540309906, 0.05829913169145584, 0.05803171172738075, 0.05794166401028633, 0.05795757472515106, 0.05792947858572006, 0.05784503370523453, 0.05765526741743088, 0.05734338238835335, 0.0569160133600235, 0.05639607086777687, 0.05582088232040405, 0.055239565670490265, 0.054703231900930405, 0.05426061153411865, 0.053950097411870956, 0.05379161983728409, 0.053778715431690216, 0.05377063900232315, 0.05376272648572922, 0.05373537540435791, 0.05367494001984596, 0.053580090403556824, 0.05345527082681656, 0.05331004783511162, 0.05315740033984184, 0.053011227399110794, 0.05288374051451683, 0.05278315395116806, 0.05271192267537117, 0.052666231989860535, 0.05263630300760269, 0.052608054131269455, 0.05256601795554161, 0.05249636247754097, 0.05238983407616615, 0.052244044840335846, 0.05206448957324028, 0.05186397582292557, 0.05166076496243477, 0.05147336423397064, 0.051317598670721054, 0.05120183154940605, 0.051124002784490585, 0.05107639729976654, 0.051031846553087234, 0.05096616968512535, 0.05085950344800949, 0.0507027767598629, 0.05050146207213402, 0.05027473345398903, 0.05005006492137909, 0.04985489696264267, 0.049707792699337006, 0.04961242154240608, 0.04955602064728737, 0.04951319471001625, 0.04945378378033638, 0.049352359026670456, 0.04919696971774101, 0.04899386316537857, 0.04876673221588135, 0.04855018109083176, 0.04837798327207565, 0.0482703372836113, 0.04822433739900589, 0.04821263998746872, 0.04819260165095329, 0.048123691231012344, 0.04798594489693642, 0.04779086634516716, 0.047577857971191406, 0.04739714786410332, 0.04728712886571884, 0.04725639522075653, 0.04725280776619911, 0.047242797911167145, 0.04721749573945999, 0.04716986045241356, 0.047103460878133774, 0.04703129827976227, 0.04696999490261078, 0.04693238437175751, 0.046920936554670334, 0.04692568629980087, 0.04692166671156883, 0.04690916836261749, 0.04688187688589096, 0.04684586822986603, 0.04681187495589256, 0.04678988456726074, 0.04678385332226753, 0.0467832125723362, 0.04678190127015114, 0.04677780345082283, 0.04676918312907219, 0.04675666242837906, 0.04674280434846878, 0.046731043606996536, 0.046723656356334686, 0.046720586717128754, 0.04671930521726608, 0.04671609774231911, 0.04670821502804756, 0.04669562354683876, 0.04668121039867401, 0.04666917026042938, 0.046664413064718246, 0.04666413739323616, 0.04666350036859512, 0.04666182026267052, 0.04665827751159668, 0.046652812510728836, 0.04664623737335205, 0.04663970693945885, 0.04663601890206337, 0.046633847057819366, 0.046631719917058945, 0.046628475189208984, 0.04662365838885307, 0.04661789909005165, 0.046612538397312164, 0.04660866782069206, 0.046606313437223434, 0.04660433530807495, 0.04660132899880409]\n",
      "Test Losses: [1.237412691116333, 1.2373961210250854, 1.2373683452606201, 1.2373292446136475, 1.2372785806655884, 1.2372167110443115, 1.2371426820755005, 1.2370566129684448, 1.2369581460952759, 1.236846685409546, 1.2367217540740967, 1.2365833520889282, 1.2364308834075928, 1.2362637519836426, 1.2360819578170776, 1.2358847856521606, 1.2356719970703125, 1.2354429960250854, 1.2351977825164795, 1.2349352836608887, 1.234655737876892, 1.2343579530715942, 1.2340422868728638, 1.2337075471878052, 1.233353853225708, 1.2329802513122559, 1.23258638381958, 1.232171893119812, 1.2317360639572144, 1.2312787771224976, 1.2307989597320557, 1.2302964925765991, 1.2297706604003906, 1.229220986366272, 1.2286466360092163, 1.2280476093292236, 1.2274225950241089, 1.2267718315124512, 1.2260938882827759, 1.2253888845443726, 1.2246555089950562, 1.2238938808441162, 1.2231026887893677, 1.2222820520401, 1.2214304208755493, 1.2205479145050049, 1.2196334600448608, 1.2186863422393799, 1.2177058458328247, 1.2166924476623535, 1.2156442403793335, 1.214560627937317, 1.2134413719177246, 1.2122851610183716, 1.2110915184020996, 1.209859848022461, 1.2085891962051392, 1.207278847694397, 1.2059284448623657, 1.2045369148254395, 1.2031036615371704, 1.2016276121139526, 1.200108289718628, 1.1985447406768799, 1.1969361305236816, 1.1952818632125854, 1.1935811042785645, 1.191834807395935, 1.190040946006775, 1.1881983280181885, 1.186306118965149, 1.1843632459640503, 1.1823692321777344, 1.1803231239318848, 1.1782243251800537, 1.1760716438293457, 1.1738654375076294, 1.1716042757034302, 1.169286847114563, 1.16691255569458, 1.1644809246063232, 1.161990761756897, 1.1594414710998535, 1.1568323373794556, 1.1541622877120972, 1.1514309644699097, 1.1486374139785767, 1.1457810401916504, 1.1428608894348145, 1.1398766040802002, 1.1368271112442017, 1.1337120532989502, 1.1305301189422607, 1.1272814273834229, 1.1239651441574097, 1.1205809116363525, 1.117128610610962, 1.113606572151184, 1.1100144386291504, 1.106351613998413, 1.1026175022125244, 1.0988109111785889, 1.094930648803711, 1.0909775495529175, 1.0869507789611816, 1.0828500986099243, 1.0786751508712769, 1.0744253396987915, 1.070102334022522, 1.0657206773757935, 1.061263918876648, 1.056731104850769, 1.0520634651184082, 1.047315239906311, 1.0424883365631104, 1.037582278251648, 1.0325982570648193, 1.0275429487228394, 1.0224133729934692, 1.0172046422958374, 1.0119155645370483, 1.006545901298523, 1.0010952949523926, 0.9955635070800781, 0.98995041847229, 0.9842556118965149, 0.9784790277481079, 0.9726165533065796, 0.9666687846183777, 0.9606355428695679, 0.9545215964317322, 0.9483241438865662, 0.9420429468154907, 0.935677707195282, 0.9292457699775696, 0.9227439761161804, 0.9161586761474609, 0.9094942212104797, 0.9027472734451294, 0.8959245681762695, 0.8890408873558044, 0.8820736408233643, 0.8750258088111877, 0.867896318435669, 0.8606809377670288, 0.8533849120140076, 0.8460114002227783, 0.8385517001152039, 0.8310043811798096, 0.8233744502067566, 0.815664529800415, 0.8078612089157104, 0.7999725341796875, 0.7919993996620178, 0.7839423418045044, 0.7758018970489502, 0.7676003575325012, 0.7593414783477783, 0.7510045766830444, 0.7425978779792786, 0.7341387867927551, 0.7255851030349731, 0.7169567942619324, 0.7082625031471252, 0.6995051503181458, 0.6906865239143372, 0.6818087100982666, 0.6728807091712952, 0.663892388343811, 0.6548885107040405, 0.6458627581596375, 0.6367824077606201, 0.6276877522468567, 0.61860191822052, 0.6095137000083923, 0.600434422492981, 0.5913761258125305, 0.5823416113853455, 0.5733306407928467, 0.5643545389175415, 0.5554344654083252, 0.546580970287323, 0.537804901599884, 0.5291102528572083, 0.5205197930335999, 0.5120384097099304, 0.5036845803260803, 0.49548470973968506, 0.48745545744895935, 0.4796333909034729, 0.4719718396663666, 0.46447497606277466, 0.4571491479873657, 0.44999316334724426, 0.44301193952560425, 0.43621715903282166, 0.42961618304252625, 0.42319831252098083, 0.41695940494537354, 0.41088584065437317, 0.4049878418445587, 0.3992655873298645, 0.39370429515838623, 0.38830146193504333, 0.3830479681491852, 0.3779577612876892, 0.3731042444705963, 0.368369996547699, 0.36380043625831604, 0.3593914210796356, 0.3550510108470917, 0.3508134186267853, 0.34662044048309326, 0.34251144528388977, 0.34246543049812317, 0.34027203917503357, 0.3379475772380829, 0.335472047328949, 0.33284682035446167, 0.33006101846694946, 0.32711219787597656, 0.3240083158016205, 0.3207290470600128, 0.3172617256641388, 0.3135982155799866, 0.30973440408706665, 0.30566731095314026, 0.30139559507369995, 0.29691874980926514, 0.29223838448524475, 0.28735724091529846, 0.2822811007499695, 0.277016282081604, 0.2715688645839691, 0.26594966650009155, 0.2601722478866577, 0.2542506754398346, 0.24820102751255035, 0.24204550683498383, 0.2358068823814392, 0.2295103669166565, 0.2231837809085846, 0.21685829758644104, 0.21057429909706116, 0.2044001966714859, 0.19833959639072418, 0.19242438673973083, 0.1866721212863922, 0.18105940520763397, 0.17571178078651428, 0.1706562042236328, 0.1659233421087265, 0.16154177486896515, 0.15758568048477173, 0.1540333777666092, 0.1508900225162506, 0.14817915856838226, 0.14815863966941833, 0.14689518511295319, 0.14566785097122192, 0.14447197318077087, 0.14327766001224518, 0.14205698668956757, 0.14078916609287262, 0.1394539326429367, 0.13803227245807648, 0.13650710880756378, 0.13486355543136597, 0.13309162855148315, 0.1311856210231781, 0.1291448026895523, 0.12697404623031616, 0.12468422204256058, 0.12229246646165848, 0.11982198059558868, 0.11730194836854935, 0.11478909105062485, 0.11230107396841049, 0.10987775772809982, 0.10756254941225052, 0.105398990213871, 0.10343046486377716, 0.1017034575343132, 0.10024683177471161, 0.09911630302667618, 0.09830193221569061, 0.09781685471534729, 0.09766773879528046, 0.09783757477998734, 0.09830917418003082, 0.0990544930100441, 0.10003619641065598, 0.10120730847120285, 0.10251546651124954, 0.10389375686645508, 0.10526999831199646, 0.10656718909740448, 0.10770462453365326, 0.10860156267881393, 0.10917385667562485, 0.10904376953840256, 0.10872093588113785, 0.107888363301754, 0.10655278712511063, 0.10471682250499725, 0.10240628570318222, 0.09966253489255905, 0.09653990715742111, 0.09310315549373627, 0.08942952007055283, 0.08560585230588913, 0.08172198385000229, 0.07788405567407608, 0.07420365512371063, 0.07078060507774353, 0.0677151158452034, 0.06508224457502365, 0.06295198947191238, 0.06136976182460785, 0.06037270277738571, 0.060377735644578934, 0.06038279086351395, 0.06024056673049927, 0.060132935643196106, 0.060050494968891144, 0.05998753756284714, 0.0599423423409462, 0.05991807207465172, 0.05992233380675316, 0.05996691435575485, 0.06006671115756035, 0.060238756239414215, 0.0605001226067543, 0.06086628884077072, 0.06134861335158348, 0.061952922493219376, 0.06267783045768738, 0.06351323425769806, 0.06443829834461212, 0.06542392075061798, 0.06643319875001907, 0.06742244213819504, 0.0683426484465599, 0.06914316862821579, 0.06977412104606628, 0.07019082456827164, 0.07035746425390244, 0.07024507224559784, 0.06983881443738937, 0.06913761794567108, 0.06815709918737411, 0.06692496687173843, 0.06548310071229935, 0.06388663500547409, 0.06220214441418648, 0.060487374663352966, 0.058805130422115326, 0.05721183121204376, 0.05575225502252579, 0.05445529893040657, 0.0533321350812912, 0.0523746982216835, 0.051558349281549454, 0.05085210129618645, 0.05021534115076065, 0.04961230978369713, 0.049020808190107346, 0.048438478261232376, 0.047885484993457794, 0.04740234091877937, 0.047043006867170334, 0.046864595264196396, 0.04691547900438309, 0.047223806381225586, 0.047788847237825394, 0.04857659712433815, 0.049519993364810944, 0.05052412301301956, 0.05147493630647659, 0.05139951780438423, 0.05154050514101982, 0.05136993154883385, 0.05087408050894737, 0.05006243288516998, 0.04896719008684158, 0.04764118045568466, 0.04615406692028046, 0.04458599537611008, 0.043020449578762054, 0.041535452008247375, 0.04019581153988838, 0.039043717086315155, 0.03905866667628288, 0.03907349333167076, 0.03888663277029991, 0.03876250609755516, 0.03870043903589249, 0.03870081156492233, 0.0387643501162529, 0.03889088332653046, 0.03907744213938713, 0.03931653872132301, 0.0395946130156517, 0.03989122062921524, 0.04017879441380501, 0.040424831211566925, 0.04059451073408127, 0.040654830634593964, 0.04057860001921654, 0.04034857079386711, 0.039960142225027084, 0.039423272013664246, 0.038761671632528305, 0.03801058232784271, 0.037211980670690536, 0.03640897944569588, 0.035639867186546326, 0.03493328392505646, 0.034305475652217865, 0.03376055136322975, 0.03329375758767128, 0.03289630636572838, 0.03256047144532204, 0.03228258341550827, 0.03206287696957588, 0.03190191462635994, 0.03179485723376274, 0.03172557055950165, 0.03166358917951584, 0.03156520798802376, 0.03137996420264244, 0.031060809269547462, 0.030575919896364212, 0.02991851605474949, 0.029111888259649277, 0.028206875547766685, 0.02727154642343521, 0.026375023648142815, 0.025570018216967583, 0.02487984113395214, 0.024294976145029068, 0.02378106489777565, 0.023295462131500244, 0.022805102169513702, 0.022298429161310196, 0.02178594097495079, 0.021289590746164322, 0.02082645706832409, 0.020395664498209953, 0.020391888916492462, 0.020167982205748558, 0.01992662064731121, 0.019668476656079292, 0.019404739141464233, 0.019154194742441177, 0.01893618330359459, 0.01876237615942955, 0.0186306182295084, 0.018524177372455597, 0.018518302589654922, 0.018444230780005455, 0.01834644190967083, 0.018228944391012192, 0.01810215227305889, 0.017978493124246597, 0.01786741055548191, 0.017869649454951286, 0.017827678471803665, 0.01779589243233204, 0.017774317413568497, 0.017763616517186165, 0.017764559015631676, 0.01777690462768078, 0.01779799349606037, 0.017822179943323135, 0.01784125529229641, 0.017846491187810898, 0.01783129572868347, 0.017793506383895874, 0.017736028879880905, 0.017665568739175797, 0.017589988186955452, 0.01758977584540844, 0.01755177415907383, 0.01751517690718174, 0.017481064423918724, 0.017449745908379555, 0.01742069236934185, 0.01739215850830078, 0.017360901460051537, 0.017322301864624023, 0.017271216958761215, 0.01720336824655533, 0.017116907984018326, 0.01701323315501213, 0.01689687743782997, 0.016774293035268784, 0.016652364283800125, 0.016537073999643326, 0.016432976350188255]\n"
     ]
    }
   ],
   "source": [
    "# Example usage of LFROG\n",
    "model = FeedforwardNN(input_dim=4, hidden_dim=16, output_dim=3)\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(X, y, classification=True)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_losses, test_losses = train_lfrog(\n",
    "    model=model,\n",
    "    X_train=X_train_tensor,\n",
    "    y_train=y_train_tensor,\n",
    "    X_test=X_test_tensor,\n",
    "    y_test=y_test_tensor,\n",
    "    epochs=500,\n",
    "    dt=0.005,\n",
    "    max_step=0.05,\n",
    "    early_stopping_patience=50,  # Stop if no improvement for 50 epochs\n",
    "    min_improvement=1e-6,        # Minimum improvement threshold\n",
    "    print_every=25,\n",
    "    loss_fn=nn.CrossEntropyLoss()\n",
    ")\n",
    "print(\"Train Losses:\", train_losses)\n",
    "print(\"Test Losses:\", test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef9d413",
   "metadata": {},
   "source": [
    "### Visualisatin and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591ce1ea",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
